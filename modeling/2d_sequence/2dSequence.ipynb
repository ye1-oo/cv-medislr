{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "2D Hands Image 기반 시퀀스 분류 (MobileNetV3 + GRU + Attention)"
      ],
      "metadata": {
        "id": "CAGzNW5a5MKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_seq_cnn_from_hands_mobilenet_v2.py\n",
        "\"\"\"\n",
        "2D Hands Image 기반 시퀀스 분류 (MobileNetV3 + GRU + Attention)\n",
        "\n",
        "1) /Holistic_hands_frames/1~10 폴더의 PNG를 직접 읽어서 시퀀스 구성\n",
        "   - 파일 예시: WORD1496_..._REAL01_U_s00.png ~ s15.png\n",
        "   - base_id = '..._REAL01_U' 단위로 한 시퀀스로 그룹\n",
        "2) 각 base_id 를 하나의 시퀀스(sample)로 사용 (길이 T=SEQ_LEN)\n",
        "3) 라벨은 파일명 내 WORDxxxx 에서 추출 (xxxx 문자열)\n",
        "4) 전체 시퀀스를 라벨 기준 stratified 0.7 / 0.15 / 0.15 split\n",
        "5) MobileNetV3 Small 프레임 인코더 + GRU + Attention으로 분류 학습\n",
        "   - Input: (B, T, 3, H, W)  (2D 이미지를 그대로 사용)\n",
        "   - 프레임 인코더: MobileNetV3 Small (ImageNet pretrained) → 256차원\n",
        "   - 시퀀스 인코더: 양방향 GRU (hidden=192, num_layers=2, dropout=0.2)\n",
        "   - Attention Pooling\n",
        "   - Class weight 적용\n",
        "   - Label smoothing\n",
        "   - 프레임 레벨 Gaussian noise + Temporal dropout (train 시에만)\n",
        "   - ReduceLROnPlateau 스케줄러 + EarlyStopping\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "print(\"[INFO] train_seq_cnn_from_hands_mobilenet_v2.py loaded\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 설정\n",
        "# -------------------------------------------------------------\n",
        "ROOT = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands\"\n",
        "\n",
        "SEQ_LEN = 16       # 시퀀스 길이 (s00~s15 기준)\n",
        "IMG_SIZE = 96      # 입력 이미지 크기 (HxW)\n",
        "BATCH = 32         # Colab CPU가 부담되면 16으로 줄여도 됨\n",
        "EPOCHS = 40\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "ES_PATIENCE = 7    # Early stopping patience\n",
        "\n",
        "# 프레임 레벨 데이터 증강\n",
        "IMG_NOISE_STD = 0.01     # 0이면 비활성화\n",
        "TEMP_DROP_PROB = 0.10    # 0이면 비활성화 (프레임 드롭)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Device = {DEVICE}\")\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. 시퀀스 메타 수집 (2D PNG → 시퀀스 경로 리스트)\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# 예:  WORD1496_..._REAL01_U_s15.png  → base_id='WORD1496_..._REAL01_U', seg_idx=15\n",
        "SEQ_GROUP_RE = re.compile(r\"(.+)_s(\\d+)\\.png$\")\n",
        "\n",
        "\n",
        "def collect_sequences_from_png(root_dir: str) -> Tuple[List[List[str]], List[str]]:\n",
        "    \"\"\"\n",
        "    각 person 폴더(1~10) 아래 PNG 파일을 읽어서\n",
        "    base_id 단위 시퀀스를 구성하고, 라벨 문자열을 추출합니다.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sequences : List[List[str]]\n",
        "        각 원소는 한 시퀀스를 구성하는 frame path 리스트 (길이 가변)\n",
        "    labels_str : List[str]\n",
        "        각 시퀀스에 대응되는 라벨 문자열 (예: \"0029\")\n",
        "    \"\"\"\n",
        "    sequences: List[List[str]] = []\n",
        "    labels_str: List[str] = []\n",
        "\n",
        "    # 1,2,...,10 폴더만 사용 (seq_embeddings_hands 등은 제외)\n",
        "    person_dirs = sorted(\n",
        "        [\n",
        "            d for d in glob.glob(os.path.join(root_dir, \"*\"))\n",
        "            if os.path.isdir(d) and os.path.basename(d).isdigit()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    for p_dir in person_dirs:\n",
        "        person_id = os.path.basename(p_dir)\n",
        "        png_files = sorted(glob.glob(os.path.join(p_dir, \"*.png\")))\n",
        "        print(f\"[SCAN] Person {person_id}: {len(png_files)} frames\")\n",
        "\n",
        "        # base_id 기준 그룹핑\n",
        "        groups: Dict[str, Dict[int, str]] = {}\n",
        "\n",
        "        for img_path in png_files:\n",
        "            fname = os.path.basename(img_path)\n",
        "            m = SEQ_GROUP_RE.match(fname)\n",
        "            if not m:\n",
        "                continue\n",
        "            base_id, seg_str = m.groups()\n",
        "            seg_idx = int(seg_str)\n",
        "\n",
        "            if base_id not in groups:\n",
        "                groups[base_id] = {}\n",
        "            # 같은 seg_idx에 여러 파일이 있을 가능성은 거의 없지만, 있다면 가장 앞의 것만 사용\n",
        "            if seg_idx not in groups[base_id]:\n",
        "                groups[base_id][seg_idx] = img_path\n",
        "\n",
        "        # base_id 하나가 하나의 시퀀스가 됨\n",
        "        for base_id, seg_dict in groups.items():\n",
        "            seg_indices = sorted(seg_dict.keys())\n",
        "            frame_paths = [seg_dict[i] for i in seg_indices]\n",
        "\n",
        "            if len(frame_paths) == 0:\n",
        "                continue\n",
        "\n",
        "            # 라벨 문자열 추출: base_id 내 WORDxxxx 사용\n",
        "            label_txt = \"0000\"\n",
        "            try:\n",
        "                parts = base_id.split(\"_\")\n",
        "                word_token = next(\n",
        "                    (p for p in parts if p.startswith(\"WORD\") and p[4:].isdigit()),\n",
        "                    None,\n",
        "                )\n",
        "                if word_token is not None:\n",
        "                    label_txt = word_token.replace(\"WORD\", \"\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            sequences.append(frame_paths)\n",
        "            labels_str.append(label_txt)\n",
        "\n",
        "    print(f\"[META] Total sequence samples = {len(sequences)}\")\n",
        "    return sequences, labels_str\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. Dataset 정의\n",
        "# -------------------------------------------------------------\n",
        "class HandsSeqDataset(Dataset):\n",
        "    \"\"\"\n",
        "    sequences: List[List[str]]  (각 원소는 frame path 리스트)\n",
        "    labels   : List[int]        (encoded label)\n",
        "    indices  : Subset 인덱스\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequences: List[List[str]],\n",
        "        labels: List[int],\n",
        "        indices: List[int],\n",
        "        transform: transforms.Compose,\n",
        "    ):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def _select_frames(self, frame_paths: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        길이가 가변인 frame_paths를 SEQ_LEN 길이로 변환.\n",
        "        - 길이가 SEQ_LEN보다 길면 균등 간격으로 샘플링\n",
        "        - 짧으면 마지막 프레임을 반복하여 패딩\n",
        "        \"\"\"\n",
        "        L = len(frame_paths)\n",
        "        if L == 0:\n",
        "            raise ValueError(\"Empty frame list encountered.\")\n",
        "\n",
        "        if L == SEQ_LEN:\n",
        "            return frame_paths\n",
        "        elif L > SEQ_LEN:\n",
        "            idxs = np.linspace(0, L - 1, SEQ_LEN)\n",
        "            idxs = np.round(idxs).astype(int)\n",
        "            idxs = np.clip(idxs, 0, L - 1)\n",
        "            return [frame_paths[i] for i in idxs]\n",
        "        else:\n",
        "            pad_num = SEQ_LEN - L\n",
        "            return frame_paths + [frame_paths[-1]] * pad_num\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        real_idx = self.indices[idx]\n",
        "        frame_paths = self.sequences[real_idx]\n",
        "        frame_paths = self._select_frames(frame_paths)\n",
        "\n",
        "        imgs = []\n",
        "        for path in frame_paths:\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            img = self.transform(img)  # (3, H, W)\n",
        "            imgs.append(img)\n",
        "        # (T, 3, H, W)\n",
        "        seq_tensor = torch.stack(imgs, dim=0)\n",
        "        label = self.labels[real_idx]\n",
        "        return seq_tensor, label\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. MobileNetV3 + GRU + Attention 모델\n",
        "# -------------------------------------------------------------\n",
        "class FrameEncoderMobileNetV3(nn.Module):\n",
        "    \"\"\"\n",
        "    ImageNet pretrained MobileNetV3 Small을 프레임 인코더로 사용.\n",
        "    입력: (B, 3, H, W) → 출력: (B, out_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_dim: int = 256, pretrained: bool = True):\n",
        "        super().__init__()\n",
        "        weights = models.MobileNet_V3_Small_Weights.DEFAULT if pretrained else None\n",
        "        backbone = models.mobilenet_v3_small(weights=weights)\n",
        "\n",
        "        # feature extractor (classifier 제거)\n",
        "        self.features = backbone.features\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        in_feat = backbone.classifier[0].in_features\n",
        "        self.proj = nn.Linear(in_feat, out_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, 3, H, W)\n",
        "        f = self.features(x)              # (B, C, H', W')\n",
        "        f = self.gap(f).flatten(1)        # (B, C)\n",
        "        f = self.proj(f)                  # (B, out_dim)\n",
        "        return f\n",
        "\n",
        "\n",
        "class SeqCNN_MobileNet_GRU_Attn(nn.Module):\n",
        "    \"\"\"\n",
        "    2D 이미지 → MobileNetV3 Small → GRU + Attention → 분류\n",
        "    GRU/Attention/Head 구조는 CNN 임베딩 GRU 모델과 최대한 동일하게 구성.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        frame_out_dim: int = 256,\n",
        "        hidden_dim: int = 192,\n",
        "        num_layers: int = 2,\n",
        "        bidirectional: bool = True,\n",
        "        dropout: float = 0.2,\n",
        "        pretrained_backbone: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = FrameEncoderMobileNetV3(\n",
        "            out_dim=frame_out_dim,\n",
        "            pretrained=pretrained_backbone,\n",
        "        )\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=frame_out_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "\n",
        "        self.attn_fc = nn.Linear(out_dim, 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(out_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T, 3, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "\n",
        "        # 1) 프레임별 CNN 인코딩\n",
        "        feat = self.encoder(x)               # (B*T, F)\n",
        "        feat = feat.view(B, T, -1)           # (B, T, F)\n",
        "\n",
        "        # 2) GRU + Attention\n",
        "        out, _ = self.gru(feat)              # (B, T, H*)\n",
        "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        w = self.attn_fc(out)                # (B, T, 1)\n",
        "        w = torch.softmax(w, dim=1)          # (B, T, 1)\n",
        "        w = torch.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        feat_seq = (w * out).sum(dim=1)      # (B, H*)\n",
        "        logits = self.head(feat_seq)         # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. Train / Eval loop  (CNN 임베딩 GRU 버전과 동일 패턴)\n",
        "# -------------------------------------------------------------\n",
        "def run_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion,\n",
        "    optimizer=None,\n",
        "    device=\"cpu\",\n",
        "    phase: str = \"train\",\n",
        "):\n",
        "    train_flag = optimizer is not None\n",
        "    model.train(train_flag)\n",
        "\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        # xb: (B, T, 3, H, W)\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        # ----- frame-level augmentation (train 시에만) -----\n",
        "        if train_flag:\n",
        "            if IMG_NOISE_STD > 0:\n",
        "                noise = torch.randn_like(xb) * IMG_NOISE_STD\n",
        "                xb = xb + noise\n",
        "\n",
        "            if TEMP_DROP_PROB > 0:\n",
        "                # 일부 프레임 전체를 drop (0으로)\n",
        "                B, T, C, H, W = xb.shape\n",
        "                mask = (\n",
        "                    torch.rand(B, T, 1, 1, 1, device=device)\n",
        "                    > TEMP_DROP_PROB\n",
        "                ).float()\n",
        "                xb = xb * mask\n",
        "\n",
        "        if train_flag:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "\n",
        "        if train_flag:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=1)\n",
        "            total += yb.size(0)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            loss_sum += loss.item() * yb.size(0)\n",
        "\n",
        "    avg_loss = loss_sum / max(1, total)\n",
        "    acc = correct / max(1, total)\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def eval_with_confusion(model, loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            all_y += yb.cpu().tolist()\n",
        "            all_p += pred.cpu().tolist()\n",
        "    cm = confusion_matrix(all_y, all_p)\n",
        "    acc = (cm.diagonal().sum() / cm.sum()) if cm.sum() > 0 else 0.0\n",
        "    return acc, cm\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. main\n",
        "# -------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # 1) 시퀀스 메타 수집\n",
        "    seq_paths, labels_str = collect_sequences_from_png(ROOT)\n",
        "\n",
        "    if len(seq_paths) == 0:\n",
        "        raise ValueError(\"수집된 시퀀스가 없습니다. 폴더 구조와 파일명을 확인하세요.\")\n",
        "\n",
        "    # 라벨 인코딩 (문자열 → 정수)\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(labels_str)\n",
        "    labels = encoder.transform(labels_str).astype(np.int64)\n",
        "    num_classes = len(encoder.classes_)\n",
        "    print(f\"[META] num_classes = {num_classes}, labels = {encoder.classes_}\")\n",
        "\n",
        "    # 2) Stratified random split\n",
        "    y_all = labels\n",
        "    indices = np.arange(len(seq_paths))\n",
        "\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.3,\n",
        "        stratify=y_all,\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,\n",
        "        stratify=y_all[temp_idx],\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    print(f\"[SPLIT] train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
        "\n",
        "    # 3) class weight 계산 (임베딩 GRU와 동일 방식)\n",
        "    class_counts = np.bincount(y_all[train_idx], minlength=num_classes)\n",
        "    max_count = class_counts.max()\n",
        "    class_weights = (max_count / class_counts).astype(np.float32)\n",
        "    print(\"[INFO] class weights:\", class_weights)\n",
        "\n",
        "    # 4) Transform 정의 (ImageNet 정규화 사용)\n",
        "    imagenet_mean = [0.485, 0.456, 0.406]\n",
        "    imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "    ])\n",
        "\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "    ])\n",
        "\n",
        "    # 5) Dataset / DataLoader\n",
        "    train_set = HandsSeqDataset(seq_paths, y_all.tolist(), train_idx.tolist(), transform_train)\n",
        "    val_set   = HandsSeqDataset(seq_paths, y_all.tolist(), val_idx.tolist(), transform_eval)\n",
        "    test_set  = HandsSeqDataset(seq_paths, y_all.tolist(), test_idx.tolist(), transform_eval)\n",
        "\n",
        "    train_dl = DataLoader(train_set, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=(DEVICE.type==\"cuda\"))\n",
        "    val_dl   = DataLoader(val_set, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=(DEVICE.type==\"cuda\"))\n",
        "    test_dl  = DataLoader(test_set, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=(DEVICE.type==\"cuda\"))\n",
        "\n",
        "    # 6) 모델 정의\n",
        "    model = SeqCNN_MobileNet_GRU_Attn(\n",
        "        num_classes=num_classes,\n",
        "        frame_out_dim=256,\n",
        "        hidden_dim=192,\n",
        "        num_layers=2,\n",
        "        bidirectional=True,\n",
        "        dropout=0.2,\n",
        "        pretrained_backbone=True,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # 필요하면 backbone freeze (RAM/속도 절약용)\n",
        "    # for p in model.encoder.features.parameters():\n",
        "    #     p.requires_grad = False\n",
        "\n",
        "    weight_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weight_tensor, label_smoothing=0.05)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=LR,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"max\",\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-5,\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    save_path = os.path.join(ROOT, \"seq_cnn_mobilenet_gru_attn_hands_best.pt\")\n",
        "\n",
        "    # 7) 학습 루프\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr_loss, tr_acc = run_epoch(model, train_dl, criterion, optimizer, DEVICE, phase=\"train\")\n",
        "        val_loss, val_acc = run_epoch(model, val_dl, criterion, None, DEVICE, phase=\"val\")\n",
        "\n",
        "        print(f\"[Ep{epoch:02d}] Train acc={tr_acc:.3f}, loss={tr_loss:.3f} | Val acc={val_acc:.3f}, loss={val_loss:.3f}\")\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(best_state, save_path)\n",
        "            print(f\"  -> New best val acc={best_val_acc:.3f} (model saved)\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  -> No improvement for {epochs_no_improve} epoch(s)\")\n",
        "\n",
        "        if epochs_no_improve >= ES_PATIENCE:\n",
        "            print(f\"[EarlyStopping] {ES_PATIENCE} epoch 동안 개선이 없어 중단합니다.\")\n",
        "            break\n",
        "\n",
        "    # 8) Best 모델 로드 후 Test 평가\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "        model.to(DEVICE)\n",
        "        print(f\"[INFO] Loaded best model from {save_path}\")\n",
        "\n",
        "    test_acc, cm = eval_with_confusion(model, test_dl, DEVICE)\n",
        "    print(f\"\\n✅ Final Test Accuracy = {test_acc:.3f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWp2nyRdz3TY",
        "outputId": "9c9eea09-fa2a-4317-ead2-fa50acdc5a2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_seq_cnn_from_hands_mobilenet_v2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_seq_cnn_from_hands_mobilenet_v2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2xCtgMYz_Ao",
        "outputId": "092faab3-f9ef-48ae-8a2f-6420e133e160"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] train_seq_cnn_from_hands_mobilenet_v2.py loaded\n",
            "[INFO] Device = cuda\n",
            "[SCAN] Person 1: 1712 frames\n",
            "[SCAN] Person 10: 1760 frames\n",
            "[SCAN] Person 2: 1760 frames\n",
            "[SCAN] Person 3: 1760 frames\n",
            "[SCAN] Person 4: 1760 frames\n",
            "[SCAN] Person 5: 1760 frames\n",
            "[SCAN] Person 6: 1760 frames\n",
            "[SCAN] Person 7: 1760 frames\n",
            "[SCAN] Person 8: 1760 frames\n",
            "[SCAN] Person 9: 1760 frames\n",
            "[META] Total sequence samples = 1097\n",
            "[META] num_classes = 22, labels = ['0029' '0033' '0036' '0037' '0039' '0040' '0041' '0042' '0046' '0062'\n",
            " '0064' '0065' '0163' '0187' '0400' '0572' '0689' '0885' '1115' '1129'\n",
            " '1158' '1496']\n",
            "[SPLIT] train=767, val=165, test=165\n",
            "[INFO] class weights: [1.      1.      1.      1.      1.      1.      1.      1.      1.\n",
            " 1.      1.      1.      1.      1.      1.09375 1.      1.      1.\n",
            " 1.      1.      1.      1.     ]\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n",
            "100% 9.83M/9.83M [00:00<00:00, 194MB/s]\n",
            "[Ep01] Train acc=0.163, loss=2.928 | Val acc=0.115, loss=3.522\n",
            "  -> New best val acc=0.115 (model saved)\n",
            "[Ep02] Train acc=0.458, loss=1.928 | Val acc=0.242, loss=2.753\n",
            "  -> New best val acc=0.242 (model saved)\n",
            "[Ep03] Train acc=0.614, loss=1.473 | Val acc=0.121, loss=3.734\n",
            "  -> No improvement for 1 epoch(s)\n",
            "[Ep04] Train acc=0.728, loss=1.127 | Val acc=0.091, loss=4.392\n",
            "  -> No improvement for 2 epoch(s)\n",
            "[Ep05] Train acc=0.763, loss=0.992 | Val acc=0.255, loss=2.901\n",
            "  -> New best val acc=0.255 (model saved)\n",
            "[Ep06] Train acc=0.784, loss=0.923 | Val acc=0.515, loss=1.620\n",
            "  -> New best val acc=0.515 (model saved)\n",
            "[Ep07] Train acc=0.838, loss=0.802 | Val acc=0.612, loss=1.522\n",
            "  -> New best val acc=0.612 (model saved)\n",
            "[Ep08] Train acc=0.868, loss=0.688 | Val acc=0.582, loss=1.514\n",
            "  -> No improvement for 1 epoch(s)\n",
            "[Ep09] Train acc=0.893, loss=0.653 | Val acc=0.552, loss=1.682\n",
            "  -> No improvement for 2 epoch(s)\n",
            "[Ep10] Train acc=0.906, loss=0.611 | Val acc=0.491, loss=1.954\n",
            "  -> No improvement for 3 epoch(s)\n",
            "[Ep11] Train acc=0.913, loss=0.597 | Val acc=0.630, loss=1.248\n",
            "  -> New best val acc=0.630 (model saved)\n",
            "[Ep12] Train acc=0.930, loss=0.571 | Val acc=0.673, loss=1.384\n",
            "  -> New best val acc=0.673 (model saved)\n",
            "[Ep13] Train acc=0.928, loss=0.558 | Val acc=0.588, loss=1.660\n",
            "  -> No improvement for 1 epoch(s)\n",
            "[Ep14] Train acc=0.936, loss=0.548 | Val acc=0.697, loss=1.295\n",
            "  -> New best val acc=0.697 (model saved)\n",
            "[Ep15] Train acc=0.940, loss=0.529 | Val acc=0.521, loss=1.882\n",
            "  -> No improvement for 1 epoch(s)\n",
            "[Ep16] Train acc=0.943, loss=0.531 | Val acc=0.661, loss=1.617\n",
            "  -> No improvement for 2 epoch(s)\n",
            "[Ep17] Train acc=0.969, loss=0.482 | Val acc=0.691, loss=1.523\n",
            "  -> No improvement for 3 epoch(s)\n",
            "[Ep18] Train acc=0.962, loss=0.475 | Val acc=0.630, loss=1.566\n",
            "  -> No improvement for 4 epoch(s)\n",
            "[Ep19] Train acc=0.979, loss=0.445 | Val acc=0.733, loss=1.123\n",
            "  -> New best val acc=0.733 (model saved)\n",
            "[Ep20] Train acc=0.973, loss=0.442 | Val acc=0.758, loss=1.078\n",
            "  -> New best val acc=0.758 (model saved)\n",
            "[Ep21] Train acc=0.980, loss=0.424 | Val acc=0.939, loss=0.541\n",
            "  -> New best val acc=0.939 (model saved)\n",
            "[Ep22] Train acc=0.979, loss=0.424 | Val acc=0.952, loss=0.569\n",
            "  -> New best val acc=0.952 (model saved)\n",
            "[Ep23] Train acc=0.975, loss=0.420 | Val acc=0.909, loss=0.620\n",
            "  -> No improvement for 1 epoch(s)\n",
            "[Ep24] Train acc=0.987, loss=0.404 | Val acc=0.909, loss=0.647\n",
            "  -> No improvement for 2 epoch(s)\n",
            "[Ep25] Train acc=0.984, loss=0.402 | Val acc=0.903, loss=0.671\n",
            "  -> No improvement for 3 epoch(s)\n",
            "[Ep26] Train acc=0.987, loss=0.402 | Val acc=0.867, loss=0.694\n",
            "  -> No improvement for 4 epoch(s)\n",
            "[Ep27] Train acc=0.990, loss=0.396 | Val acc=0.873, loss=0.700\n",
            "  -> No improvement for 5 epoch(s)\n",
            "[Ep28] Train acc=0.980, loss=0.406 | Val acc=0.903, loss=0.640\n",
            "  -> No improvement for 6 epoch(s)\n",
            "[Ep29] Train acc=0.988, loss=0.394 | Val acc=0.945, loss=0.585\n",
            "  -> No improvement for 7 epoch(s)\n",
            "[EarlyStopping] 7 epoch 동안 개선이 없어 중단합니다.\n",
            "[INFO] Loaded best model from /content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands/seq_cnn_mobilenet_gru_attn_hands_best.pt\n",
            "\n",
            "✅ Final Test Accuracy = 0.903\n",
            "Confusion Matrix:\n",
            "[[4 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [2 3 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 7 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 6 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 6 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0]\n",
            " [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2D Hands Image 기반 시퀀스 분류 (MobileNetV3 + TCN + Attention)"
      ],
      "metadata": {
        "id": "6LKRX41S5JpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_seq_cnn_from_hands_mobilenet_tcn.py\n",
        "\"\"\"\n",
        "2D Hands Image 기반 시퀀스 분류 (MobileNetV3 + TCN + Attention)\n",
        "\n",
        "1) /Holistic_hands_frames/1~10 폴더의 PNG를 직접 읽어서 시퀀스 구성\n",
        "   - 파일 예시: WORD1496_..._REAL01_U_s00.png ~ s15.png\n",
        "   - base_id = '..._REAL01_U' 단위로 한 시퀀스로 그룹\n",
        "2) 각 base_id 를 하나의 시퀀스(sample)로 사용 (길이 T=SEQ_LEN)\n",
        "3) 라벨은 파일명 내 WORDxxxx 에서 추출 (xxxx 문자열)\n",
        "4) 전체 시퀀스를 라벨 기준 stratified 0.7 / 0.15 / 0.15 split\n",
        "5) MobileNetV3 Small 프레임 인코더 + TCN + Attention으로 분류 학습\n",
        "   - Input: (B, T, 3, H, W)  (2D 이미지를 그대로 사용)\n",
        "   - 프레임 인코더: MobileNetV3 Small (ImageNet pretrained) → 256차원\n",
        "   - 시퀀스 인코더: TCN (hidden_channels=256, dilations=1,2,4)\n",
        "   - Attention Pooling\n",
        "   - Class weight 적용\n",
        "   - Label smoothing\n",
        "   - 프레임 레벨 Gaussian noise + Temporal dropout (train 시에만)\n",
        "   - ReduceLROnPlateau 스케줄러 + EarlyStopping\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset  # Subset은 사용하지 않아도 무방\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "print(\"[INFO] train_seq_cnn_from_hands_mobilenet_tcn.py loaded\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 설정\n",
        "# -------------------------------------------------------------\n",
        "ROOT = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands\"\n",
        "\n",
        "SEQ_LEN = 16       # 시퀀스 길이 (s00~s15 기준)\n",
        "IMG_SIZE = 96      # 입력 이미지 크기 (HxW)\n",
        "BATCH = 32         # Colab CPU가 부담되면 16으로 줄여도 됨\n",
        "EPOCHS = 40\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "ES_PATIENCE = 7    # Early stopping patience\n",
        "\n",
        "# 프레임 레벨 데이터 증강\n",
        "IMG_NOISE_STD = 0.01     # 0이면 비활성화\n",
        "TEMP_DROP_PROB = 0.10    # 0이면 비활성화 (프레임 드롭)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Device = {DEVICE}\")\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. 시퀀스 메타 수집 (2D PNG → 시퀀스 경로 리스트)\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# 예:  WORD1496_..._REAL01_U_s15.png  → base_id='WORD1496_..._REAL01_U', seg_idx=15\n",
        "SEQ_GROUP_RE = re.compile(r\"(.+)_s(\\d+)\\.png$\")\n",
        "\n",
        "\n",
        "def collect_sequences_from_png(root_dir: str) -> Tuple[List[List[str]], List[str]]:\n",
        "    \"\"\"\n",
        "    각 person 폴더(1~10) 아래 PNG 파일을 읽어서\n",
        "    base_id 단위 시퀀스를 구성하고, 라벨 문자열을 추출합니다.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sequences : List[List[str]]\n",
        "        각 원소는 한 시퀀스를 구성하는 frame path 리스트 (길이 가변)\n",
        "    labels_str : List[str]\n",
        "        각 시퀀스에 대응되는 라벨 문자열 (예: \"0029\")\n",
        "    \"\"\"\n",
        "    sequences: List[List[str]] = []\n",
        "    labels_str: List[str] = []\n",
        "\n",
        "    # 1,2,...,10 폴더만 사용 (seq_embeddings_hands 등은 제외)\n",
        "    person_dirs = sorted(\n",
        "        [\n",
        "            d for d in glob.glob(os.path.join(root_dir, \"*\"))\n",
        "            if os.path.isdir(d) and os.path.basename(d).isdigit()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    for p_dir in person_dirs:\n",
        "        person_id = os.path.basename(p_dir)\n",
        "        png_files = sorted(glob.glob(os.path.join(p_dir, \"*.png\")))\n",
        "        print(f\"[SCAN] Person {person_id}: {len(png_files)} frames\")\n",
        "\n",
        "        # base_id 기준 그룹핑\n",
        "        groups: Dict[str, Dict[int, str]] = {}\n",
        "\n",
        "        for img_path in png_files:\n",
        "            fname = os.path.basename(img_path)\n",
        "            m = SEQ_GROUP_RE.match(fname)\n",
        "            if not m:\n",
        "                continue\n",
        "            base_id, seg_str = m.groups()\n",
        "            seg_idx = int(seg_str)\n",
        "\n",
        "            if base_id not in groups:\n",
        "                groups[base_id] = {}\n",
        "            # 같은 seg_idx에 여러 파일이 있을 가능성은 거의 없지만, 있다면 가장 앞의 것만 사용\n",
        "            if seg_idx not in groups[base_id]:\n",
        "                groups[base_id][seg_idx] = img_path\n",
        "\n",
        "        # base_id 하나가 하나의 시퀀스가 됨\n",
        "        for base_id, seg_dict in groups.items():\n",
        "            seg_indices = sorted(seg_dict.keys())\n",
        "            frame_paths = [seg_dict[i] for i in seg_indices]\n",
        "\n",
        "            if len(frame_paths) == 0:\n",
        "                continue\n",
        "\n",
        "            # 라벨 문자열 추출: base_id 내 WORDxxxx 사용\n",
        "            label_txt = \"0000\"\n",
        "            try:\n",
        "                parts = base_id.split(\"_\")\n",
        "                word_token = next(\n",
        "                    (p for p in parts if p.startswith(\"WORD\") and p[4:].isdigit()),\n",
        "                    None,\n",
        "                )\n",
        "                if word_token is not None:\n",
        "                    label_txt = word_token.replace(\"WORD\", \"\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            sequences.append(frame_paths)\n",
        "            labels_str.append(label_txt)\n",
        "\n",
        "    print(f\"[META] Total sequence samples = {len(sequences)}\")\n",
        "    return sequences, labels_str\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. Dataset 정의\n",
        "# -------------------------------------------------------------\n",
        "class HandsSeqDataset(Dataset):\n",
        "    \"\"\"\n",
        "    sequences: List[List[str]]  (각 원소는 frame path 리스트)\n",
        "    labels   : List[int]        (encoded label)\n",
        "    indices  : Subset 인덱스\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequences: List[List[str]],\n",
        "        labels: List[int],\n",
        "        indices: List[int],\n",
        "        transform: transforms.Compose,\n",
        "    ):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def _select_frames(self, frame_paths: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        길이가 가변인 frame_paths를 SEQ_LEN 길이로 변환.\n",
        "        - 길이가 SEQ_LEN보다 길면 균등 간격으로 샘플링\n",
        "        - 짧으면 마지막 프레임을 반복하여 패딩\n",
        "        \"\"\"\n",
        "        L = len(frame_paths)\n",
        "        if L == 0:\n",
        "            raise ValueError(\"Empty frame list encountered.\")\n",
        "\n",
        "        if L == SEQ_LEN:\n",
        "            return frame_paths\n",
        "        elif L > SEQ_LEN:\n",
        "            idxs = np.linspace(0, L - 1, SEQ_LEN)\n",
        "            idxs = np.round(idxs).astype(int)\n",
        "            idxs = np.clip(idxs, 0, L - 1)\n",
        "            return [frame_paths[i] for i in idxs]\n",
        "        else:\n",
        "            pad_num = SEQ_LEN - L\n",
        "            return frame_paths + [frame_paths[-1]] * pad_num\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        real_idx = self.indices[idx]\n",
        "        frame_paths = self.sequences[real_idx]\n",
        "        frame_paths = self._select_frames(frame_paths)\n",
        "\n",
        "        imgs = []\n",
        "        for path in frame_paths:\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            img = self.transform(img)  # (3, H, W)\n",
        "            imgs.append(img)\n",
        "        # (T, 3, H, W)\n",
        "        seq_tensor = torch.stack(imgs, dim=0)\n",
        "        label = self.labels[real_idx]\n",
        "        return seq_tensor, label\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. MobileNetV3 프레임 인코더\n",
        "# -------------------------------------------------------------\n",
        "class FrameEncoderMobileNetV3(nn.Module):\n",
        "    \"\"\"\n",
        "    ImageNet pretrained MobileNetV3 Small을 프레임 인코더로 사용.\n",
        "    입력: (B, 3, H, W) → 출력: (B, out_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_dim: int = 256, pretrained: bool = True):\n",
        "        super().__init__()\n",
        "        weights = models.MobileNet_V3_Small_Weights.DEFAULT if pretrained else None\n",
        "        backbone = models.mobilenet_v3_small(weights=weights)\n",
        "\n",
        "        # feature extractor (classifier 제거)\n",
        "        self.features = backbone.features\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        in_feat = backbone.classifier[0].in_features\n",
        "        self.proj = nn.Linear(in_feat, out_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, 3, H, W)\n",
        "        f = self.features(x)              # (B, C, H', W')\n",
        "        f = self.gap(f).flatten(1)        # (B, C)\n",
        "        f = self.proj(f)                  # (B, out_dim)\n",
        "        return f\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. TCN 정의\n",
        "# -------------------------------------------------------------\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int = 3,\n",
        "        dilation: int = 1,\n",
        "        dropout: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        padding = (kernel_size - 1) * dilation // 2  # 길이 유지용\n",
        "\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            padding=padding,\n",
        "            dilation=dilation,\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            padding=padding,\n",
        "            dilation=dilation,\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.downsample = (\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "            if in_channels != out_channels\n",
        "            else None\n",
        "        )\n",
        "        self.final_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, C, T)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        out = out + res\n",
        "        out = self.final_relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels: int,\n",
        "        hidden_channels: int = 256,\n",
        "        num_layers: int = 3,\n",
        "        kernel_size: int = 3,\n",
        "        dropout: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_ch = input_channels\n",
        "        for i in range(num_layers):\n",
        "            dilation = 2**i  # 1, 2, 4, ...\n",
        "            layers.append(\n",
        "                TemporalBlock(\n",
        "                    in_channels=in_ch,\n",
        "                    out_channels=hidden_channels,\n",
        "                    kernel_size=kernel_size,\n",
        "                    dilation=dilation,\n",
        "                    dropout=dropout,\n",
        "                )\n",
        "            )\n",
        "            in_ch = hidden_channels\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self.out_channels = hidden_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, C, T)\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. MobileNet + TCN + Attention 모델\n",
        "# -------------------------------------------------------------\n",
        "class SeqCNN_MobileNet_TCN_Attn(nn.Module):\n",
        "    \"\"\"\n",
        "    2D 이미지 → MobileNetV3 Small → TCN → Attention → 분류\n",
        "    CNN 쪽은 GRU 버전과 동일하게 두고, 시퀀스 인코더만 TCN으로 교체.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        frame_out_dim: int = 256,\n",
        "        tcn_hidden: int = 256,\n",
        "        tcn_layers: int = 3,\n",
        "        dropout: float = 0.2,\n",
        "        pretrained_backbone: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = FrameEncoderMobileNetV3(\n",
        "            out_dim=frame_out_dim,\n",
        "            pretrained=pretrained_backbone,\n",
        "        )\n",
        "\n",
        "        # TCN: 입력 채널 = frame_out_dim, 출력 채널 = tcn_hidden\n",
        "        self.tcn = TemporalConvNet(\n",
        "            input_channels=frame_out_dim,\n",
        "            hidden_channels=tcn_hidden,\n",
        "            num_layers=tcn_layers,\n",
        "            kernel_size=3,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        out_dim = self.tcn.out_channels\n",
        "\n",
        "        self.attn_fc = nn.Linear(out_dim, 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(out_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T, 3, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "\n",
        "        # 1) 프레임별 CNN 인코딩\n",
        "        feat = self.encoder(x)               # (B*T, F)\n",
        "        feat = feat.view(B, T, -1)           # (B, T, F)\n",
        "\n",
        "        # 2) TCN: (B, F, T) → (B, H, T)\n",
        "        feat_t = feat.permute(0, 2, 1)       # (B, F, T)\n",
        "        out = self.tcn(feat_t)               # (B, H, T)\n",
        "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        # 3) Attention over time\n",
        "        out_seq = out.permute(0, 2, 1)       # (B, T, H)\n",
        "        w = self.attn_fc(out_seq)            # (B, T, 1)\n",
        "        w = torch.softmax(w, dim=1)          # (B, T, 1)\n",
        "        w = torch.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        feat_seq = (w * out_seq).sum(dim=1)  # (B, H)\n",
        "        logits = self.head(feat_seq)         # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6. Train / Eval loop  (GRU 버전과 동일 패턴)\n",
        "# -------------------------------------------------------------\n",
        "def run_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion,\n",
        "    optimizer=None,\n",
        "    device=\"cpu\",\n",
        "    phase: str = \"train\",\n",
        "):\n",
        "    train_flag = optimizer is not None\n",
        "    model.train(train_flag)\n",
        "\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        # xb: (B, T, 3, H, W)\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        # ----- frame-level augmentation (train 시에만)-----\n",
        "        if train_flag:\n",
        "            if IMG_NOISE_STD > 0:\n",
        "                noise = torch.randn_like(xb) * IMG_NOISE_STD\n",
        "                xb = xb + noise\n",
        "\n",
        "            if TEMP_DROP_PROB > 0:\n",
        "                # 일부 프레임 전체를 drop (0으로)\n",
        "                B_, T_, C_, H_, W_ = xb.shape\n",
        "                mask = (\n",
        "                    torch.rand(B_, T_, 1, 1, 1, device=device)\n",
        "                    > TEMP_DROP_PROB\n",
        "                ).float()\n",
        "                xb = xb * mask\n",
        "\n",
        "        if train_flag:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "\n",
        "        if train_flag:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=1)\n",
        "            total += yb.size(0)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            loss_sum += loss.item() * yb.size(0)\n",
        "\n",
        "    avg_loss = loss_sum / max(1, total)\n",
        "    acc = correct / max(1, total)\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def eval_with_confusion(model, loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    all_y, all_p = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            all_y += yb.cpu().tolist()\n",
        "            all_p += pred.cpu().tolist()\n",
        "    cm = confusion_matrix(all_y, all_p)\n",
        "    acc = (cm.diagonal().sum() / cm.sum()) if cm.sum() > 0 else 0.0\n",
        "    return acc, cm\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 7. main\n",
        "# -------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # 1) 시퀀스 메타 수집\n",
        "    seq_paths, labels_str = collect_sequences_from_png(ROOT)\n",
        "\n",
        "    if len(seq_paths) == 0:\n",
        "        raise ValueError(\"수집된 시퀀스가 없습니다. 폴더 구조와 파일명을 확인하세요.\")\n",
        "\n",
        "    # 라벨 인코딩 (문자열 → 정수)\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(labels_str)\n",
        "    labels = encoder.transform(labels_str).astype(np.int64)\n",
        "    num_classes = len(encoder.classes_)\n",
        "    print(f\"[META] num_classes = {num_classes}, labels = {encoder.classes_}\")\n",
        "\n",
        "    # 2) Stratified random split\n",
        "    y_all = labels\n",
        "    indices = np.arange(len(seq_paths))\n",
        "\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.3,\n",
        "        stratify=y_all,\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,\n",
        "        stratify=y_all[temp_idx],\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    print(f\"[SPLIT] train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
        "\n",
        "    # 3) class weight 계산\n",
        "    class_counts = np.bincount(y_all[train_idx], minlength=num_classes)\n",
        "    max_count = class_counts.max()\n",
        "    class_weights = (max_count / class_counts).astype(np.float32)\n",
        "    print(\"[INFO] class weights:\", class_weights)\n",
        "\n",
        "    # 4) Transform 정의 (ImageNet 정규화 사용)\n",
        "    imagenet_mean = [0.485, 0.456, 0.406]\n",
        "    imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "    ])\n",
        "\n",
        "    transform_eval = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "    ])\n",
        "\n",
        "    # 5) Dataset / DataLoader\n",
        "    train_set = HandsSeqDataset(seq_paths, y_all.tolist(), train_idx.tolist(), transform_train)\n",
        "    val_set   = HandsSeqDataset(seq_paths, y_all.tolist(), val_idx.tolist(), transform_eval)\n",
        "    test_set  = HandsSeqDataset(seq_paths, y_all.tolist(), test_idx.tolist(), transform_eval)\n",
        "\n",
        "    pin = DEVICE.type == \"cuda\"\n",
        "    train_dl = DataLoader(train_set, batch_size=BATCH, shuffle=True,  num_workers=2, pin_memory=pin)\n",
        "    val_dl   = DataLoader(val_set,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=pin)\n",
        "    test_dl  = DataLoader(test_set,  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=pin)\n",
        "\n",
        "    # 6) 모델 정의 (MobileNet backbone 동일, GRU 대신 TCN)\n",
        "    model = SeqCNN_MobileNet_TCN_Attn(\n",
        "        num_classes=num_classes,\n",
        "        frame_out_dim=256,\n",
        "        tcn_hidden=256,\n",
        "        tcn_layers=3,\n",
        "        dropout=0.2,\n",
        "        pretrained_backbone=True,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # 필요하면 backbone freeze (GRU 모델과 동일하게 맞추고 싶다면 둘 다 같은 설정 사용)\n",
        "    # for p in model.encoder.features.parameters():\n",
        "    #     p.requires_grad = False\n",
        "\n",
        "    weight_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weight_tensor, label_smoothing=0.05)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=LR,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode=\"max\",\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-5,\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    save_path = os.path.join(ROOT, \"seq_cnn_mobilenet_tcn_attn_hands_best.pt\")\n",
        "\n",
        "    # 7) 학습 루프\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr_loss, tr_acc = run_epoch(model, train_dl, criterion, optimizer, DEVICE, phase=\"train\")\n",
        "        val_loss, val_acc = run_epoch(model, val_dl, criterion, None, DEVICE, phase=\"val\")\n",
        "\n",
        "        print(f\"[Ep{epoch:02d}] Train acc={tr_acc:.3f}, loss={tr_loss:.3f} | \"\n",
        "              f\"Val acc={val_acc:.3f}, loss={val_loss:.3f}\")\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(best_state, save_path)\n",
        "            print(f\"  -> New best val acc={best_val_acc:.3f} (model saved)\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  -> No improvement for {epochs_no_improve} epoch(s)\")\n",
        "\n",
        "        if epochs_no_improve >= ES_PATIENCE:\n",
        "            print(f\"[EarlyStopping] {ES_PATIENCE} epoch 동안 개선이 없어 중단합니다.\")\n",
        "            break\n",
        "\n",
        "    # 8) Best 모델 로드 후 Test 평가\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "        model.to(DEVICE)\n",
        "        print(f\"[INFO] Loaded best model from {save_path}\")\n",
        "\n",
        "    test_acc, cm = eval_with_confusion(model, test_dl, DEVICE)\n",
        "    print(f\"\\n✅ Final Test Accuracy = {test_acc:.3f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n"
      ],
      "metadata": {
        "id": "US7ixYMbnbaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fb2871-470b-42fe-ac17-0dbb949b03ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_seq_cnn_from_hands_mobilenet_tcn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_seq_cnn_from_hands_mobilenet_tcn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PwxTuovzZtT",
        "outputId": "cc485fd0-b6fc-46fc-8224-1621dd3aa930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] train_seq_cnn_from_hands_mobilenet_tcn.py loaded\n",
            "[INFO] Device = cpu\n",
            "[SCAN] Person 1: 1712 frames\n",
            "[SCAN] Person 10: 1760 frames\n",
            "[SCAN] Person 2: 1760 frames\n",
            "[SCAN] Person 3: 1760 frames\n",
            "[SCAN] Person 4: 1760 frames\n",
            "[SCAN] Person 5: 1760 frames\n",
            "[SCAN] Person 6: 1760 frames\n",
            "[SCAN] Person 7: 1760 frames\n",
            "[SCAN] Person 8: 1760 frames\n",
            "[SCAN] Person 9: 1760 frames\n",
            "[META] Total sequence samples = 1097\n",
            "[META] num_classes = 22, labels = ['0029' '0033' '0036' '0037' '0039' '0040' '0041' '0042' '0046' '0062'\n",
            " '0064' '0065' '0163' '0187' '0400' '0572' '0689' '0885' '1115' '1129'\n",
            " '1158' '1496']\n",
            "[SPLIT] train=767, val=165, test=165\n",
            "[INFO] class weights: [1.      1.      1.      1.      1.      1.      1.      1.      1.\n",
            " 1.      1.      1.      1.      1.      1.09375 1.      1.      1.\n",
            " 1.      1.      1.      1.     ]\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n",
            "100% 9.83M/9.83M [00:00<00:00, 99.4MB/s]\n",
            "[Ep01] Train acc=0.216, loss=2.645 | Val acc=0.194, loss=2.827\n",
            "  -> New best val acc=0.194 (model saved)\n",
            "[Ep02] Train acc=0.482, loss=1.831 | Val acc=0.121, loss=4.069\n",
            "  -> No improvement for 1 epoch(s)\n",
            "[Ep03] Train acc=0.610, loss=1.420 | Val acc=0.158, loss=3.472\n",
            "  -> No improvement for 2 epoch(s)\n",
            "[Ep04] Train acc=0.703, loss=1.165 | Val acc=0.109, loss=3.328\n",
            "  -> No improvement for 3 epoch(s)\n",
            "[Ep05] Train acc=0.757, loss=0.974 | Val acc=0.103, loss=3.613\n",
            "  -> No improvement for 4 epoch(s)\n",
            "[Ep06] Train acc=0.849, loss=0.778 | Val acc=0.218, loss=3.110\n",
            "  -> New best val acc=0.218 (model saved)\n",
            "[Ep07] Train acc=0.854, loss=0.730 | Val acc=0.558, loss=1.764\n",
            "  -> New best val acc=0.558 (model saved)\n",
            "[Ep08] Train acc=0.911, loss=0.638 | Val acc=0.424, loss=2.137\n",
            "  -> No improvement for 1 epoch(s)\n",
            "[Ep09] Train acc=0.918, loss=0.615 | Val acc=0.491, loss=1.826\n",
            "  -> No improvement for 2 epoch(s)\n",
            "[Ep10] Train acc=0.927, loss=0.595 | Val acc=0.467, loss=1.984\n",
            "  -> No improvement for 3 epoch(s)\n",
            "[Ep11] Train acc=0.924, loss=0.574 | Val acc=0.485, loss=1.764\n",
            "  -> No improvement for 4 epoch(s)\n",
            "[Ep12] Train acc=0.948, loss=0.526 | Val acc=0.436, loss=2.224\n",
            "  -> No improvement for 5 epoch(s)\n",
            "[Ep13] Train acc=0.954, loss=0.493 | Val acc=0.533, loss=1.646\n",
            "  -> No improvement for 6 epoch(s)\n",
            "[Ep14] Train acc=0.945, loss=0.518 | Val acc=0.521, loss=1.851\n",
            "  -> No improvement for 7 epoch(s)\n",
            "[EarlyStopping] 7 epoch 동안 개선이 없어 중단합니다.\n",
            "[INFO] Loaded best model from /content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands/seq_cnn_mobilenet_tcn_attn_hands_best.pt\n",
            "\n",
            "✅ Final Test Accuracy = 0.527\n",
            "Confusion Matrix:\n",
            "[[2 0 0 2 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 4 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 2 1 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [1 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 2 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0]\n",
            " [0 0 0 0 0 6 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 1 0 0 0 1 4 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 2 3 0 0 0 0]\n",
            " [0 0 0 0 1 2 1 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 5 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 3 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 2 0 2 0 0 0 0 0 0 0 2 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0]\n",
            " [0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 6 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0]\n",
            " [0 0 0 0 0 0 7 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전처리 파일 저장"
      ],
      "metadata": {
        "id": "bYv4MmHzEOX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess_testset_to_pt_only_tcn.py\n",
        "\"\"\"\n",
        "TEST SET 전처리 (TCN 실험용)\n",
        "- 모델 생성 없음\n",
        "- 학습 없음\n",
        "- PNG → 동일 transform → tensor → .pt 저장\n",
        "- GRU 버전과 split / 전처리 100% 동일\n",
        "- 저장 경로: data/samples/2D Sequence/tcn\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 경로 설정 (✅ 최종 확정)\n",
        "# -------------------------------------------------------------\n",
        "# 전처리된 hand frame PNG들이 있는 위치\n",
        "ROOT = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands\"\n",
        "\n",
        "# 샘플 저장 루트 (요구된 경로)\n",
        "SAVE_ROOT = \"/content/drive/MyDrive/cv-medislr/data/samples/2D Sequence\"\n",
        "\n",
        "# TCN 실험용 tensor 저장 경로\n",
        "SAVE_TENSOR_DIR = os.path.join(SAVE_ROOT, \"tcn\")\n",
        "\n",
        "# metadata 저장 경로\n",
        "SAVE_META_DIR = os.path.join(SAVE_ROOT, \"metadata\")\n",
        "\n",
        "os.makedirs(SAVE_TENSOR_DIR, exist_ok=True)\n",
        "os.makedirs(SAVE_META_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 공통 설정\n",
        "# -------------------------------------------------------------\n",
        "SEQ_LEN = 16\n",
        "IMG_SIZE = 96\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# transform (eval 기준, 학습 코드와 동일)\n",
        "# -------------------------------------------------------------\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std =[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 시퀀스 수집\n",
        "# -------------------------------------------------------------\n",
        "SEQ_GROUP_RE = re.compile(r\"(.+)_s(\\d+)\\.png$\")\n",
        "\n",
        "\n",
        "def collect_sequences(root_dir: str):\n",
        "    sequences, labels_str, base_ids = [], [], []\n",
        "\n",
        "    person_dirs = sorted(\n",
        "        d for d in glob.glob(os.path.join(root_dir, \"*\"))\n",
        "        if os.path.isdir(d) and os.path.basename(d).isdigit()\n",
        "    )\n",
        "\n",
        "    for p_dir in person_dirs:\n",
        "        png_files = sorted(glob.glob(os.path.join(p_dir, \"*.png\")))\n",
        "        groups: Dict[str, Dict[int, str]] = {}\n",
        "\n",
        "        for img_path in png_files:\n",
        "            fname = os.path.basename(img_path)\n",
        "            m = SEQ_GROUP_RE.match(fname)\n",
        "            if not m:\n",
        "                continue\n",
        "\n",
        "            base_id, seg = m.groups()\n",
        "            groups.setdefault(base_id, {})[int(seg)] = img_path\n",
        "\n",
        "        for base_id, seg_dict in groups.items():\n",
        "            frames = [seg_dict[i] for i in sorted(seg_dict)]\n",
        "            if not frames:\n",
        "                continue\n",
        "\n",
        "            # WORDxxxx 라벨 추출\n",
        "            label_txt = \"0000\"\n",
        "            for part in base_id.split(\"_\"):\n",
        "                if part.startswith(\"WORD\") and part[4:].isdigit():\n",
        "                    label_txt = part.replace(\"WORD\", \"\")\n",
        "                    break\n",
        "\n",
        "            sequences.append(frames)\n",
        "            labels_str.append(label_txt)\n",
        "            base_ids.append(base_id)\n",
        "\n",
        "    print(f\"[META] Total sequences = {len(sequences)}\")\n",
        "    return sequences, labels_str, base_ids\n",
        "\n",
        "\n",
        "def select_frames(frames: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    학습 코드와 완전히 동일한 frame selection 로직\n",
        "    \"\"\"\n",
        "    L = len(frames)\n",
        "    if L >= SEQ_LEN:\n",
        "        idxs = np.linspace(0, L - 1, SEQ_LEN).round().astype(int)\n",
        "        return [frames[i] for i in idxs]\n",
        "    else:\n",
        "        return frames + [frames[-1]] * (SEQ_LEN - L)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# main\n",
        "# -------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"[INFO] Collecting sequences (TCN)...\")\n",
        "    sequences, labels_str, base_ids = collect_sequences(ROOT)\n",
        "\n",
        "    # label encoding (split 재현용)\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(labels_str)\n",
        "\n",
        "    indices = np.arange(len(sequences))\n",
        "\n",
        "    # 🔑 이전 TCN 실험과 동일한 split 재현\n",
        "    _, temp_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.3,\n",
        "        stratify=labels,\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "    _, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,\n",
        "        stratify=labels[temp_idx],\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    print(f\"[SPLIT] TCN test samples = {len(test_idx)}\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # TEST SET 전처리 → .pt 저장\n",
        "    # ---------------------------------------------------------\n",
        "    meta_rows = []\n",
        "\n",
        "    for i, idx in enumerate(test_idx):\n",
        "        frames = select_frames(sequences[idx])\n",
        "\n",
        "        imgs = []\n",
        "        for p in frames:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            imgs.append(transform_eval(img))\n",
        "\n",
        "        tensor = torch.stack(imgs)  # (T, 3, H, W)\n",
        "\n",
        "        save_name = f\"seq_{i:05d}.pt\"\n",
        "        save_path = os.path.join(SAVE_TENSOR_DIR, save_name)\n",
        "\n",
        "        torch.save({\n",
        "            \"x\": tensor,\n",
        "            \"y\": int(labels[idx]),\n",
        "            \"base_id\": base_ids[idx],\n",
        "            \"split\": \"test\",\n",
        "            \"model\": \"TCN\",\n",
        "        }, save_path)\n",
        "\n",
        "        meta_rows.append({\n",
        "            \"seq_file\": save_name,\n",
        "            \"base_id\": base_ids[idx],\n",
        "            \"label\": int(labels[idx]),\n",
        "            \"split\": \"test\",\n",
        "            \"model\": \"TCN\",\n",
        "        })\n",
        "\n",
        "    # metadata CSV 저장\n",
        "    meta_csv = os.path.join(SAVE_META_DIR, \"tsn_hands_test_samples_tcn.csv\")\n",
        "    pd.DataFrame(meta_rows).to_csv(meta_csv, index=False)\n",
        "\n",
        "    print(f\"[DONE] Saved {len(test_idx)} TCN test tensors.\")\n",
        "    print(f\"[DONE] Metadata saved to {meta_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oePLNpG7EPs2",
        "outputId": "3193ca48-3b3b-4b16-bc42-2487b46e6a79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocess_testset_to_pt_only_tcn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python preprocess_testset_to_pt_only_tcn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgnrjFK8E16m",
        "outputId": "2fe5e0de-baa3-43ef-b6be-6cef53f50874"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Collecting sequences (TCN)...\n",
            "[META] Total sequences = 1097\n",
            "[SPLIT] TCN test samples = 165\n",
            "[DONE] Saved 165 TCN test tensors.\n",
            "[DONE] Metadata saved to /content/drive/MyDrive/cv-medislr/data/samples/2D Sequence/metadata/tsn_hands_test_samples_tcn.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess_testset_to_pt_only_gru.py\n",
        "\"\"\"\n",
        "TEST SET 전처리 (GRU 실험용)\n",
        "- 모델 생성 없음\n",
        "- 학습 없음\n",
        "- PNG → 동일 transform → tensor → .pt 저장\n",
        "- TCN 버전과 split / 전처리 100% 동일\n",
        "\"\"\"\n",
        "\n",
        "import os, glob, re, random\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 경로 설정 (요청한 형식 그대로)\n",
        "# -------------------------------------------------------------\n",
        "ROOT = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands\"\n",
        "\n",
        "SAVE_ROOT = \"/content/drive/MyDrive/cv-medislr/data/samples/2D Sequence\"\n",
        "SAVE_TENSOR_DIR = f\"{SAVE_ROOT}/gru\"\n",
        "SAVE_META_DIR   = f\"{SAVE_ROOT}/metadata\"\n",
        "\n",
        "os.makedirs(SAVE_TENSOR_DIR, exist_ok=True)\n",
        "os.makedirs(SAVE_META_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 공통 설정\n",
        "# -------------------------------------------------------------\n",
        "SEQ_LEN = 16\n",
        "IMG_SIZE = 96\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# transform (eval 기준, 학습 코드와 동일)\n",
        "# -------------------------------------------------------------\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std =[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 시퀀스 수집\n",
        "# -------------------------------------------------------------\n",
        "SEQ_GROUP_RE = re.compile(r\"(.+)_s(\\d+)\\.png$\")\n",
        "\n",
        "def collect_sequences(root_dir):\n",
        "    sequences, labels_str, base_ids = [], [], []\n",
        "\n",
        "    person_dirs = sorted(\n",
        "        d for d in glob.glob(os.path.join(root_dir, \"*\"))\n",
        "        if os.path.isdir(d) and os.path.basename(d).isdigit()\n",
        "    )\n",
        "\n",
        "    for p_dir in person_dirs:\n",
        "        pngs = sorted(glob.glob(os.path.join(p_dir, \"*.png\")))\n",
        "        groups: Dict[str, Dict[int, str]] = {}\n",
        "\n",
        "        for p in pngs:\n",
        "            m = SEQ_GROUP_RE.match(os.path.basename(p))\n",
        "            if not m:\n",
        "                continue\n",
        "            base_id, seg = m.groups()\n",
        "            groups.setdefault(base_id, {})[int(seg)] = p\n",
        "\n",
        "        for base_id, seg_dict in groups.items():\n",
        "            frames = [seg_dict[i] for i in sorted(seg_dict)]\n",
        "            if not frames:\n",
        "                continue\n",
        "\n",
        "            # WORDxxxx 라벨 추출\n",
        "            label_txt = \"0000\"\n",
        "            for part in base_id.split(\"_\"):\n",
        "                if part.startswith(\"WORD\") and part[4:].isdigit():\n",
        "                    label_txt = part.replace(\"WORD\", \"\")\n",
        "                    break\n",
        "\n",
        "            sequences.append(frames)\n",
        "            labels_str.append(label_txt)\n",
        "            base_ids.append(base_id)\n",
        "\n",
        "    print(f\"[META] Total sequences = {len(sequences)}\")\n",
        "    return sequences, labels_str, base_ids\n",
        "\n",
        "\n",
        "def select_frames(frames: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    학습 코드와 완전히 동일한 frame selection\n",
        "    \"\"\"\n",
        "    L = len(frames)\n",
        "    if L >= SEQ_LEN:\n",
        "        idxs = np.linspace(0, L - 1, SEQ_LEN).round().astype(int)\n",
        "        return [frames[i] for i in idxs]\n",
        "    return frames + [frames[-1]] * (SEQ_LEN - L)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# main\n",
        "# -------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"[INFO] Collecting sequences (GRU)...\")\n",
        "    sequences, labels_str, base_ids = collect_sequences(ROOT)\n",
        "\n",
        "    # label encoding (split 재현용)\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(labels_str)\n",
        "\n",
        "    indices = np.arange(len(sequences))\n",
        "\n",
        "    # 🔑 TCN / GRU 동일한 test split 재현\n",
        "    _, temp_idx = train_test_split(\n",
        "        indices,\n",
        "        test_size=0.3,\n",
        "        stratify=labels,\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "    _, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,\n",
        "        stratify=labels[temp_idx],\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    print(f\"[SPLIT] GRU test samples = {len(test_idx)}\")\n",
        "\n",
        "    meta_rows = []\n",
        "\n",
        "    for i, idx in enumerate(test_idx):\n",
        "        frames = select_frames(sequences[idx])\n",
        "\n",
        "        imgs = []\n",
        "        for p in frames:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            imgs.append(transform_eval(img))\n",
        "\n",
        "        tensor = torch.stack(imgs)  # (T, 3, H, W)\n",
        "\n",
        "        fname = f\"seq_{i:05d}.pt\"\n",
        "        torch.save(\n",
        "            {\n",
        "                \"x\": tensor,\n",
        "                \"y\": int(labels[idx]),\n",
        "                \"base_id\": base_ids[idx],\n",
        "                \"split\": \"test\",\n",
        "                \"model\": \"GRU\",\n",
        "            },\n",
        "            f\"{SAVE_TENSOR_DIR}/{fname}\",\n",
        "        )\n",
        "\n",
        "        meta_rows.append({\n",
        "            \"seq_file\": fname,\n",
        "            \"base_id\": base_ids[idx],\n",
        "            \"label\": int(labels[idx]),\n",
        "            \"split\": \"test\",\n",
        "            \"model\": \"GRU\",\n",
        "        })\n",
        "\n",
        "    meta_csv = f\"{SAVE_META_DIR}/tsn_hands_test_samples_gru.csv\"\n",
        "    pd.DataFrame(meta_rows).to_csv(meta_csv, index=False)\n",
        "\n",
        "    print(f\"[DONE] Saved {len(test_idx)} GRU test tensors.\")\n",
        "    print(f\"[DONE] Metadata saved to {meta_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzePjYlVFqbT",
        "outputId": "e092e546-a6db-4212-d221-d3ca795bb851"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocess_testset_to_pt_only_gru.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python preprocess_testset_to_pt_only_gru.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyMsDiTPHihO",
        "outputId": "f6d5946a-4fd3-40d8-e1d3-4ec1c1d1ce15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Collecting sequences (GRU)...\n",
            "[META] Total sequences = 1097\n",
            "[SPLIT] GRU test samples = 165\n",
            "[DONE] Saved 165 GRU test tensors.\n",
            "[DONE] Metadata saved to /content/drive/MyDrive/cv-medislr/data/samples/2D Sequence/metadata/tsn_hands_test_samples_gru.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "랜덤 20개 추출"
      ],
      "metadata": {
        "id": "ApLoWIzCPayW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile make_demo_samples_from_test.py\n",
        "\"\"\"\n",
        "TEST pt 중에서 랜덤 20개를 추출해 demo/sample 폴더 생성\n",
        "- 전처리 없음\n",
        "- split 변경 없음\n",
        "- test 정의 유지\n",
        "- seed 고정 (재현 가능)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 설정\n",
        "# -------------------------------------------------------------\n",
        "RANDOM_SEED = 42\n",
        "NUM_SAMPLES = 20\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "BASE_ROOT = \"/content/drive/MyDrive/cv-medislr/data/samples/2D Sequence\"\n",
        "\n",
        "GRU_TEST_DIR = f\"{BASE_ROOT}/gru\"\n",
        "TCN_TEST_DIR = f\"{BASE_ROOT}/tcn\"\n",
        "\n",
        "DEMO_ROOT = f\"{BASE_ROOT}/demo\"\n",
        "GRU_DEMO_DIR = f\"{DEMO_ROOT}/gru_2dsequence\"\n",
        "TCN_DEMO_DIR = f\"{DEMO_ROOT}/tcn_2dsequence\"\n",
        "META_DIR = f\"{DEMO_ROOT}/metadata\"\n",
        "\n",
        "os.makedirs(GRU_DEMO_DIR, exist_ok=True)\n",
        "os.makedirs(TCN_DEMO_DIR, exist_ok=True)\n",
        "os.makedirs(META_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 파일 목록\n",
        "# -------------------------------------------------------------\n",
        "gru_files = sorted(f for f in os.listdir(GRU_TEST_DIR) if f.endswith(\".pt\"))\n",
        "tcn_files = sorted(f for f in os.listdir(TCN_TEST_DIR) if f.endswith(\".pt\"))\n",
        "\n",
        "assert len(gru_files) >= NUM_SAMPLES, \"GRU test pt 개수 부족\"\n",
        "assert len(tcn_files) >= NUM_SAMPLES, \"TCN test pt 개수 부족\"\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 랜덤 샘플링\n",
        "# -------------------------------------------------------------\n",
        "gru_selected = random.sample(gru_files, NUM_SAMPLES)\n",
        "tcn_selected = random.sample(tcn_files, NUM_SAMPLES)\n",
        "\n",
        "rows = []\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 파일 복사\n",
        "# -------------------------------------------------------------\n",
        "for f in gru_selected:\n",
        "    shutil.copy(\n",
        "        os.path.join(GRU_TEST_DIR, f),\n",
        "        os.path.join(GRU_DEMO_DIR, f),\n",
        "    )\n",
        "    rows.append({\n",
        "        \"seq_file\": f,\n",
        "        \"model\": \"GRU\",\n",
        "        \"source\": \"test\",\n",
        "    })\n",
        "\n",
        "for f in tcn_selected:\n",
        "    shutil.copy(\n",
        "        os.path.join(TCN_TEST_DIR, f),\n",
        "        os.path.join(TCN_DEMO_DIR, f),\n",
        "    )\n",
        "    rows.append({\n",
        "        \"seq_file\": f,\n",
        "        \"model\": \"TCN\",\n",
        "        \"source\": \"test\",\n",
        "    })\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# metadata 저장\n",
        "# -------------------------------------------------------------\n",
        "meta_path = f\"{META_DIR}/demo_samples_20.csv\"\n",
        "pd.DataFrame(rows).to_csv(meta_path, index=False)\n",
        "\n",
        "print(f\"[DONE] GRU demo samples: {len(gru_selected)}\")\n",
        "print(f\"[DONE] TCN demo samples: {len(tcn_selected)}\")\n",
        "print(f\"[DONE] Metadata saved to {meta_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdcwGhQ7Nekr",
        "outputId": "ca553299-6dff-428d-9cda-f0bf0db35452"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting make_demo_samples_from_test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python make_demo_samples_from_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0lOI_tjNf7V",
        "outputId": "a658919c-f654-4c83-a4b2-2cac4c49dec9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] GRU demo samples: 20\n",
            "[DONE] TCN demo samples: 20\n",
            "[DONE] Metadata saved to /content/drive/MyDrive/cv-medislr/data/samples/2D Sequence/demo/metadata/demo_samples_20.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YjG0vNhXHiNr"
      }
    }
  ]
}