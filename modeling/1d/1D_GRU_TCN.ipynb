{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NIzFx6DgyANF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4324577-0930-4371-b408-f4a325dc69f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1D GRU 처리"
      ],
      "metadata": {
        "id": "qCa2_Cxx2Srf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# 0. 기본 설정\n",
        "# =========================\n",
        "BASE_DIR = \"/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json\"\n",
        "TARGET_LEN = 16         # T (시퀀스 길이)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 3e-4\n",
        "WEIGHT_DECAY = 1e-2\n",
        "LABEL_SMOOTHING = 0.1\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.15\n",
        "TEST_RATIO = 0.15\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1. 유틸 함수들\n",
        "# =========================\n",
        "\n",
        "EXPECTED_POSE = 33\n",
        "EXPECTED_HAND = 21  # left/right 둘 다\n",
        "\n",
        "\n",
        "def load_json(path: str) -> Dict[str, Any]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def get_landmark_array(lst: List[Dict[str, float]], expected_len: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    lst: [{\"x\":..., \"y\":..., \"z\":..., \"v\":...}, ...]\n",
        "    expected_len 길이로 (expected_len, 4) 배열 생성 (부족하면 0 패딩)\n",
        "    + NaN/Inf -> 0으로 치환\n",
        "    \"\"\"\n",
        "    arr = np.zeros((expected_len, 4), dtype=np.float32)\n",
        "    for i, lm in enumerate(lst[:expected_len]):\n",
        "        arr[i, 0] = lm.get(\"x\", 0.0)\n",
        "        arr[i, 1] = lm.get(\"y\", 0.0)\n",
        "        arr[i, 2] = lm.get(\"z\", 0.0)\n",
        "        arr[i, 3] = lm.get(\"v\", 0.0)\n",
        "\n",
        "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def normalize_skeleton(frame_kpts: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    frame_kpts: (75, 4) = pose(33) + left_hand(21) + right_hand(21)\n",
        "    - 어깨 중심 기준 translation\n",
        "    - 어깨폭으로 scale\n",
        "    - 어깨 라인이 수평이 되도록 (x, y) 회전\n",
        "    \"\"\"\n",
        "    frame_kpts = np.nan_to_num(frame_kpts, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    pose = frame_kpts[:EXPECTED_POSE]  # (33, 4)\n",
        "\n",
        "    # Mediapipe 기준: 11 = left_shoulder, 12 = right_shoulder\n",
        "    ls_idx, rs_idx = 11, 12\n",
        "    ls = pose[ls_idx]  # (x,y,z,v)\n",
        "    rs = pose[rs_idx]\n",
        "\n",
        "    use_shoulders = (ls[3] > 0.3 and rs[3] > 0.3)\n",
        "\n",
        "    if use_shoulders:\n",
        "        origin = (ls[:3] + rs[:3]) / 2.0  # (x,y,z)\n",
        "        shoulder_vec_xy = rs[:2] - ls[:2]\n",
        "        shoulder_dist = np.linalg.norm(shoulder_vec_xy)\n",
        "        if not np.isfinite(shoulder_dist) or shoulder_dist < 1e-4:\n",
        "            shoulder_dist = 1.0\n",
        "        scale = shoulder_dist\n",
        "        theta = math.atan2(float(shoulder_vec_xy[1]), float(shoulder_vec_xy[0]))\n",
        "    else:\n",
        "        origin = frame_kpts[:, :3].mean(axis=0)\n",
        "        origin = np.nan_to_num(origin, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        scale = 1.0\n",
        "        theta = 0.0\n",
        "\n",
        "    kpts = frame_kpts.copy()\n",
        "    kpts[:, :3] = (kpts[:, :3] - origin[None, :]) / (scale + 1e-6)\n",
        "\n",
        "    if abs(theta) > 1e-3:\n",
        "        cos_t = math.cos(theta)\n",
        "        sin_t = math.sin(theta)\n",
        "        x = kpts[:, 0].copy()\n",
        "        y = kpts[:, 1].copy()\n",
        "        kpts[:, 0] = x * cos_t + y * sin_t\n",
        "        kpts[:, 1] = -x * sin_t + y * cos_t\n",
        "\n",
        "    kpts = np.nan_to_num(kpts, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return kpts\n",
        "\n",
        "\n",
        "def resample_sequence(seq: np.ndarray, target_len: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    seq: (L, D)\n",
        "    target_len: T\n",
        "    \"\"\"\n",
        "    seq = np.nan_to_num(seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    L = seq.shape[0]\n",
        "    if L == target_len:\n",
        "        return seq\n",
        "    if L <= 1:\n",
        "        return np.repeat(seq, target_len, axis=0)\n",
        "\n",
        "    idxs = np.linspace(0, L - 1, target_len)\n",
        "    idxs = np.round(idxs).astype(np.int32)\n",
        "    idxs = np.clip(idxs, 0, L - 1)\n",
        "    out = seq[idxs]\n",
        "    out = np.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return out\n",
        "\n",
        "\n",
        "def add_velocity_feature(seq: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    seq: (T, D)\n",
        "    -> [position, velocity] concat (T, 2D)\n",
        "    \"\"\"\n",
        "    seq = np.nan_to_num(seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    vel = np.diff(seq, axis=0, prepend=seq[0:1])  # (T, D)\n",
        "    vel = np.nan_to_num(vel, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    out = np.concatenate([seq, vel], axis=-1)     # (T, 2D)\n",
        "    out = np.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return out\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2. 데이터 로딩 & 전처리\n",
        "# =========================\n",
        "\n",
        "SEQ_GROUP_RE = re.compile(r\"(.*)_s(\\d+)_hands$\")  # base_id, segment index 추출용\n",
        "\n",
        "\n",
        "def collect_sequences(base_dir: str):\n",
        "    \"\"\"\n",
        "    base_dir 아래:\n",
        "        person(1~10) / WORD..._s00_hands.json\n",
        "    \"\"\"\n",
        "    all_seq_arrays = []\n",
        "    all_labels = []\n",
        "\n",
        "    person_dirs = sorted(\n",
        "        [d for d in glob.glob(os.path.join(base_dir, \"*\")) if os.path.isdir(d)]\n",
        "    )\n",
        "    print(f\"Found person dirs: {person_dirs}\")\n",
        "\n",
        "    for p_dir in person_dirs:\n",
        "        person_name = os.path.basename(p_dir)\n",
        "\n",
        "        json_files = glob.glob(os.path.join(p_dir, \"*_hands.json\"))\n",
        "\n",
        "        groups = {}  # base_id -> {seg_idx:int -> [json_path,...]}\n",
        "        for jf in json_files:\n",
        "            fname = os.path.basename(jf)\n",
        "            stem, _ = os.path.splitext(fname)\n",
        "            m = SEQ_GROUP_RE.match(stem)\n",
        "            if not m:\n",
        "                continue\n",
        "            base_id, seg_str = m.groups()\n",
        "            seg_idx = int(seg_str)\n",
        "            if base_id not in groups:\n",
        "                groups[base_id] = {}\n",
        "            if seg_idx not in groups[base_id]:\n",
        "                groups[base_id][seg_idx] = []\n",
        "            groups[base_id][seg_idx].append(jf)\n",
        "\n",
        "        print(f\"[Person {person_name}] #base sequences: {len(groups)}\")\n",
        "\n",
        "        for base_id, seg_dict in groups.items():\n",
        "            seg_indices = sorted(seg_dict.keys())\n",
        "\n",
        "            segment_features = []\n",
        "            label_word = None\n",
        "\n",
        "            for seg_idx in seg_indices:\n",
        "                seg_jsons = sorted(seg_dict[seg_idx])\n",
        "                if not seg_jsons:\n",
        "                    continue\n",
        "\n",
        "                frame_kpts_list = []\n",
        "                for jf in seg_jsons:\n",
        "                    data = load_json(jf)\n",
        "\n",
        "                    if label_word is None:\n",
        "                        label_word = data.get(\"word_folder\", \"UNKNOWN\")\n",
        "\n",
        "                    pose = get_landmark_array(data.get(\"pose\", []), EXPECTED_POSE)\n",
        "                    lh = get_landmark_array(data.get(\"left_hand\", []), EXPECTED_HAND)\n",
        "                    rh = get_landmark_array(data.get(\"right_hand\", []), EXPECTED_HAND)\n",
        "                    frame_kpts = np.concatenate([pose, lh, rh], axis=0)  # (75, 4)\n",
        "                    frame_kpts = normalize_skeleton(frame_kpts)\n",
        "                    frame_kpts_list.append(frame_kpts)\n",
        "\n",
        "                if not frame_kpts_list:\n",
        "                    continue\n",
        "\n",
        "                seg_arr = np.stack(frame_kpts_list, axis=0).mean(axis=0)  # (75,4)\n",
        "                seg_arr = np.nan_to_num(seg_arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                segment_features.append(seg_arr)\n",
        "\n",
        "            if not segment_features:\n",
        "                continue\n",
        "\n",
        "            seq_arr = np.stack(segment_features, axis=0)  # (L, 75, 4)\n",
        "            seq_arr = np.nan_to_num(seq_arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            all_seq_arrays.append(seq_arr)\n",
        "            all_labels.append(label_word if label_word is not None else \"UNKNOWN\")\n",
        "\n",
        "    print(f\"Total sequences (base_id level): {len(all_seq_arrays)}\")\n",
        "    return all_seq_arrays, all_labels\n",
        "\n",
        "\n",
        "def build_label_mapping(labels: List[str]) -> Dict[str, int]:\n",
        "    uniq = sorted(list(set(labels)))\n",
        "    label2idx = {lab: i for i, lab in enumerate(uniq)}\n",
        "    print(\"Label mapping:\")\n",
        "    for k, v in label2idx.items():\n",
        "        print(f\"  {k} -> {v}\")\n",
        "    return label2idx\n",
        "\n",
        "\n",
        "def prepare_dataset(\n",
        "    base_dir: str, target_len: int\n",
        ") -> Tuple[List[np.ndarray], List[int], int]:\n",
        "    \"\"\"\n",
        "    (L,75,4) -> (T, 300) -> velocity concat (T, 600)\n",
        "    \"\"\"\n",
        "    raw_seqs, raw_labels = collect_sequences(base_dir)\n",
        "    label2idx = build_label_mapping(raw_labels)\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    for seq_arr, lab in zip(raw_seqs, raw_labels):\n",
        "        seq_arr = np.nan_to_num(seq_arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        L, J, C = seq_arr.shape  # (L, 75, 4)\n",
        "        seq_flat = seq_arr.reshape(L, J * C)  # (L, 300)\n",
        "        seq_flat = np.nan_to_num(seq_flat, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        seq_resampled = resample_sequence(seq_flat, target_len)  # (T, 300)\n",
        "        seq_with_vel = add_velocity_feature(seq_resampled)       # (T, 600)\n",
        "        seq_with_vel = np.nan_to_num(seq_with_vel, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        X_list.append(seq_with_vel.astype(np.float32))\n",
        "        y_list.append(label2idx[lab])\n",
        "\n",
        "    X_arr = np.stack(X_list, axis=0)\n",
        "    print(\"== After prepare_dataset (with nan_to_num) ==\")\n",
        "    print(\"Has NaN in X:\", np.isnan(X_arr).any())\n",
        "    print(\"Has Inf in X:\", np.isinf(X_arr).any())\n",
        "    print(\"Max abs value:\", float(np.max(np.abs(X_arr))))\n",
        "\n",
        "    input_dim = X_list[0].shape[1]\n",
        "    num_classes = len(label2idx)\n",
        "    print(f\"Final input dim D' = {input_dim}\")\n",
        "    print(f\"num_classes = {num_classes}\")\n",
        "\n",
        "    return X_list, y_list, num_classes\n",
        "\n",
        "\n",
        "def compute_global_stats(\n",
        "    X_list: List[np.ndarray], indices: List[int]\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    train split에서만 mean/std 계산\n",
        "    \"\"\"\n",
        "    if len(indices) == 0:\n",
        "        raise ValueError(\"No indices provided for computing global stats.\")\n",
        "    all_train = np.concatenate([X_list[i] for i in indices], axis=0)\n",
        "    all_train = np.nan_to_num(all_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    mean = all_train.mean(axis=0)\n",
        "    std = all_train.std(axis=0) + 1e-6\n",
        "    mean = np.nan_to_num(mean, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    std = np.nan_to_num(std, nan=1.0, posinf=1.0, neginf=1.0)\n",
        "    return mean.astype(np.float32), std.astype(np.float32)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3. Dataset & Augmentation\n",
        "# =========================\n",
        "\n",
        "def augment_seq_tensor(seq: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    seq: (T, D)\n",
        "    \"\"\"\n",
        "    T, D = seq.shape\n",
        "\n",
        "    # global scale\n",
        "    if random.random() < 0.5:\n",
        "        scale = random.uniform(0.95, 1.05)\n",
        "        seq = seq * scale\n",
        "\n",
        "    # time jitter\n",
        "    if random.random() < 0.5 and T > 8:\n",
        "        cut = random.randint(0, 2)\n",
        "        if cut > 0:\n",
        "            seq_short = seq[cut:]\n",
        "            pad = seq_short[-1:].repeat(cut, 1)\n",
        "            seq = torch.cat([seq_short, pad], dim=0)\n",
        "\n",
        "    return seq\n",
        "\n",
        "\n",
        "class KeypointSequenceDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        X_list: List[np.ndarray],\n",
        "        y_list: List[int],\n",
        "        indices: List[int],\n",
        "        mean: np.ndarray = None,\n",
        "        std: np.ndarray = None,\n",
        "        augment: bool = False,\n",
        "    ):\n",
        "        self.X_list = X_list\n",
        "        self.y_list = y_list\n",
        "        self.indices = indices\n",
        "        self.augment = augment\n",
        "\n",
        "        if mean is not None and std is not None:\n",
        "            self.mean = torch.from_numpy(mean).float()\n",
        "            self.std = torch.from_numpy(std).float()\n",
        "        else:\n",
        "            self.mean = None\n",
        "            self.std = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]\n",
        "        seq = self.X_list[real_idx]  # (T, D)\n",
        "        label = self.y_list[real_idx]\n",
        "\n",
        "        seq = torch.from_numpy(seq).float()  # (T, D)\n",
        "\n",
        "        if self.mean is not None and self.std is not None:\n",
        "            seq = (seq - self.mean) / self.std\n",
        "\n",
        "        if self.augment:\n",
        "            if random.random() < 0.7:\n",
        "                noise_std = 0.01\n",
        "                seq = seq + torch.randn_like(seq) * noise_std\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                max_shift = 3\n",
        "                shift = random.randint(-max_shift, max_shift)\n",
        "                if shift != 0:\n",
        "                    seq = torch.roll(seq, shifts=shift, dims=0)\n",
        "\n",
        "            seq = augment_seq_tensor(seq)\n",
        "\n",
        "        return seq, label\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4. GRU + Attention Pooling\n",
        "# =========================\n",
        "\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        num_classes: int,\n",
        "        hidden_dim: int = 256,\n",
        "        num_layers: int = 2,\n",
        "        bidirectional: bool = True,\n",
        "        dropout: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,   # 600\n",
        "            hidden_size=hidden_dim, # 256\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "\n",
        "        self.attn_fc = nn.Linear(out_dim, 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(out_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, D)\n",
        "        out, _ = self.gru(x)            # (B, T, H*)\n",
        "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        w = self.attn_fc(out)           # (B, T, 1)\n",
        "        w = torch.softmax(w, dim=1)\n",
        "        w = torch.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        feat = (w * out).sum(dim=1)     # (B, H*)\n",
        "        logits = self.head(feat)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5. 학습/평가 루프\n",
        "# =========================\n",
        "\n",
        "def run_epoch(model, loader, optimizer=None, criterion=None):\n",
        "    if optimizer is None:\n",
        "        model.eval()\n",
        "    else:\n",
        "        model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for seq, label in loader:\n",
        "        seq = seq.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model(seq)\n",
        "        loss = criterion(logits, label)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * seq.size(0)\n",
        "        _, pred = torch.max(logits, dim=1)\n",
        "        total_correct += (pred == label).sum().item()\n",
        "        total_count += seq.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_count\n",
        "    avg_acc = total_correct / total_count\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6. 랜덤 train/val/test split\n",
        "# =========================\n",
        "\n",
        "def random_split_indices(\n",
        "    num_samples: int,\n",
        "    train_ratio: float,\n",
        "    val_ratio: float,\n",
        "    test_ratio: float,\n",
        "    seed: int = 42,\n",
        ") -> Tuple[List[int], List[int], List[int]]:\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"비율 합이 1이 되게 설정해줘.\"\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "    indices = list(range(num_samples))\n",
        "    rng.shuffle(indices)\n",
        "\n",
        "    n_train = int(num_samples * train_ratio)\n",
        "    n_val = int(num_samples * val_ratio)\n",
        "    # 나머지는 test\n",
        "    n_test = num_samples - n_train - n_val\n",
        "\n",
        "    train_idx = indices[:n_train]\n",
        "    val_idx = indices[n_train:n_train + n_val]\n",
        "    test_idx = indices[n_train + n_val:]\n",
        "\n",
        "    print(f\"[Random Split] N={num_samples}, #train={len(train_idx)}, #val={len(val_idx)}, #test={len(test_idx)}\")\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 7. 메인\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    X_list, y_list, num_classes = prepare_dataset(BASE_DIR, TARGET_LEN)\n",
        "    input_dim = X_list[0].shape[1]\n",
        "    print(f\"Input dim for model: {input_dim}\")\n",
        "\n",
        "    num_samples = len(X_list)\n",
        "    train_idx, val_idx, test_idx = random_split_indices(\n",
        "        num_samples,\n",
        "        TRAIN_RATIO,\n",
        "        VAL_RATIO,\n",
        "        TEST_RATIO,\n",
        "        seed=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    # train 기준 mean/std\n",
        "    train_mean, train_std = compute_global_stats(X_list, train_idx)\n",
        "    print(\"[Stats] mean/std computed from train set.\")\n",
        "\n",
        "    train_dataset = KeypointSequenceDataset(\n",
        "        X_list, y_list, train_idx,\n",
        "        mean=train_mean, std=train_std,\n",
        "        augment=True\n",
        "    )\n",
        "    val_dataset = KeypointSequenceDataset(\n",
        "        X_list, y_list, val_idx,\n",
        "        mean=train_mean, std=train_std,\n",
        "        augment=False\n",
        "    )\n",
        "    test_dataset = KeypointSequenceDataset(\n",
        "        X_list, y_list, test_idx,\n",
        "        mean=train_mean, std=train_std,\n",
        "        augment=False\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
        "    )\n",
        "\n",
        "    model = GRUClassifier( input_dim=input_dim, num_classes=num_classes, hidden_dim=256,\n",
        "                            num_layers=2, bidirectional=True, dropout=0.2,).to(DEVICE)\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        train_loss, train_acc = run_epoch(model, train_loader, optimizer, criterion)\n",
        "        val_loss, val_acc = run_epoch(model, val_loader, optimizer=None, criterion=criterion)\n",
        "\n",
        "        print(\n",
        "            f\"[Epoch {epoch:02d}] \"\n",
        "            f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, \"\n",
        "            f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\"\n",
        "        )\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    test_loss, test_acc = run_epoch(model, test_loader, optimizer=None, criterion=criterion)\n",
        "    print(\n",
        "        f\"[Final] Best val_acc={best_val_acc:.3f}, \"\n",
        "        f\"Test loss={test_loss:.4f}, Test acc={test_acc:.3f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "WJD2ljPa2tTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c7cc55-0a30-4b2a-d730-264a988dcf5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Found person dirs: ['/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/1', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/10', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/2', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/3', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/4', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/5', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/6', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/7', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/8', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/9']\n",
            "[Person 1] #base sequences: 107\n",
            "[Person 10] #base sequences: 110\n",
            "[Person 2] #base sequences: 110\n",
            "[Person 3] #base sequences: 110\n",
            "[Person 4] #base sequences: 110\n",
            "[Person 5] #base sequences: 110\n",
            "[Person 6] #base sequences: 110\n",
            "[Person 7] #base sequences: 110\n",
            "[Person 8] #base sequences: 110\n",
            "[Person 9] #base sequences: 110\n",
            "Total sequences (base_id level): 1097\n",
            "Label mapping:\n",
            "  WORD0029_검사 -> 0\n",
            "  WORD0033_당뇨병 -> 1\n",
            "  WORD0036_면역 -> 2\n",
            "  WORD0037_감기 -> 3\n",
            "  WORD0039_변비 -> 4\n",
            "  WORD0040_병명 -> 5\n",
            "  WORD0041_보건소 -> 6\n",
            "  WORD0042_불면증 -> 7\n",
            "  WORD0046_설사 -> 8\n",
            "  WORD0062_진단서 -> 9\n",
            "  WORD0064_치료 -> 10\n",
            "  WORD0065_치료법 -> 11\n",
            "  WORD0163_의사 -> 12\n",
            "  WORD0187_간호사 -> 13\n",
            "  WORD0400_정밀검사 -> 14\n",
            "  WORD0572_환자실 -> 15\n",
            "  WORD0689_통증 -> 16\n",
            "  WORD0885_치료제 -> 17\n",
            "  WORD1115_건강 -> 18\n",
            "  WORD1129_검사 -> 19\n",
            "  WORD1158_피곤하다 -> 20\n",
            "  WORD1496_병원 -> 21\n",
            "== After prepare_dataset (with nan_to_num) ==\n",
            "Has NaN in X: False\n",
            "Has Inf in X: False\n",
            "Max abs value: 1.0\n",
            "Final input dim D' = 600\n",
            "num_classes = 22\n",
            "Input dim for model: 600\n",
            "[Random Split] N=1097, #train=767, #val=164, #test=166\n",
            "[Stats] mean/std computed from train set.\n",
            "[Epoch 01] train_loss=2.3977, train_acc=0.378, val_loss=1.8145, val_acc=0.567\n",
            "[Epoch 02] train_loss=1.6361, train_acc=0.652, val_loss=1.4859, val_acc=0.695\n",
            "[Epoch 03] train_loss=1.3522, train_acc=0.759, val_loss=1.3414, val_acc=0.738\n",
            "[Epoch 04] train_loss=1.1892, train_acc=0.841, val_loss=1.1821, val_acc=0.841\n",
            "[Epoch 05] train_loss=1.0686, train_acc=0.862, val_loss=1.1634, val_acc=0.823\n",
            "[Epoch 06] train_loss=0.9816, train_acc=0.904, val_loss=1.0922, val_acc=0.854\n",
            "[Epoch 07] train_loss=0.9176, train_acc=0.947, val_loss=1.0587, val_acc=0.854\n",
            "[Epoch 08] train_loss=0.8887, train_acc=0.944, val_loss=1.0264, val_acc=0.860\n",
            "[Epoch 09] train_loss=0.8386, train_acc=0.953, val_loss=0.9838, val_acc=0.896\n",
            "[Epoch 10] train_loss=0.8102, train_acc=0.960, val_loss=0.9971, val_acc=0.890\n",
            "[Epoch 11] train_loss=0.7826, train_acc=0.975, val_loss=0.9805, val_acc=0.884\n",
            "[Epoch 12] train_loss=0.7607, train_acc=0.979, val_loss=0.9916, val_acc=0.872\n",
            "[Epoch 13] train_loss=0.7520, train_acc=0.982, val_loss=0.9634, val_acc=0.902\n",
            "[Epoch 14] train_loss=0.7316, train_acc=0.992, val_loss=0.9917, val_acc=0.878\n",
            "[Epoch 15] train_loss=0.7278, train_acc=0.987, val_loss=0.9539, val_acc=0.909\n",
            "[Epoch 16] train_loss=0.7225, train_acc=0.990, val_loss=0.9200, val_acc=0.915\n",
            "[Epoch 17] train_loss=0.7165, train_acc=0.993, val_loss=0.9316, val_acc=0.902\n",
            "[Epoch 18] train_loss=0.7178, train_acc=0.990, val_loss=0.9196, val_acc=0.902\n",
            "[Epoch 19] train_loss=0.7135, train_acc=0.991, val_loss=0.9459, val_acc=0.896\n",
            "[Epoch 20] train_loss=0.7040, train_acc=0.992, val_loss=0.9092, val_acc=0.921\n",
            "[Epoch 21] train_loss=0.6978, train_acc=0.992, val_loss=0.9562, val_acc=0.896\n",
            "[Epoch 22] train_loss=0.6939, train_acc=0.990, val_loss=0.8904, val_acc=0.927\n",
            "[Epoch 23] train_loss=0.6942, train_acc=0.991, val_loss=0.9625, val_acc=0.890\n",
            "[Epoch 24] train_loss=0.6857, train_acc=0.996, val_loss=0.9084, val_acc=0.909\n",
            "[Epoch 25] train_loss=0.6756, train_acc=0.995, val_loss=0.8929, val_acc=0.921\n",
            "[Epoch 26] train_loss=0.6761, train_acc=0.993, val_loss=0.8966, val_acc=0.921\n",
            "[Epoch 27] train_loss=0.6620, train_acc=1.000, val_loss=0.9031, val_acc=0.915\n",
            "[Epoch 28] train_loss=0.6592, train_acc=0.997, val_loss=0.8905, val_acc=0.921\n",
            "[Epoch 29] train_loss=0.6580, train_acc=0.997, val_loss=0.8815, val_acc=0.915\n",
            "[Epoch 30] train_loss=0.6577, train_acc=0.997, val_loss=0.8853, val_acc=0.921\n",
            "[Epoch 31] train_loss=0.6533, train_acc=1.000, val_loss=0.8910, val_acc=0.921\n",
            "[Epoch 32] train_loss=0.6534, train_acc=0.999, val_loss=0.8694, val_acc=0.933\n",
            "[Epoch 33] train_loss=0.6518, train_acc=1.000, val_loss=0.8723, val_acc=0.921\n",
            "[Epoch 34] train_loss=0.6510, train_acc=1.000, val_loss=0.8648, val_acc=0.933\n",
            "[Epoch 35] train_loss=0.6497, train_acc=1.000, val_loss=0.8681, val_acc=0.927\n",
            "[Epoch 36] train_loss=0.6496, train_acc=0.999, val_loss=0.8606, val_acc=0.933\n",
            "[Epoch 37] train_loss=0.6495, train_acc=1.000, val_loss=0.8684, val_acc=0.927\n",
            "[Epoch 38] train_loss=0.6481, train_acc=1.000, val_loss=0.8720, val_acc=0.927\n",
            "[Epoch 39] train_loss=0.6538, train_acc=0.996, val_loss=0.8666, val_acc=0.927\n",
            "[Epoch 40] train_loss=0.6532, train_acc=0.999, val_loss=0.8660, val_acc=0.927\n",
            "[Final] Best val_acc=0.933, Test loss=0.8974, Test acc=0.916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1D TCN 처리"
      ],
      "metadata": {
        "id": "2Q6jCZ31BXtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# 0. 기본 설정\n",
        "# =========================\n",
        "BASE_DIR = \"/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json\"\n",
        "TARGET_LEN = 16         # T (시퀀스 길이)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 3e-4\n",
        "WEIGHT_DECAY = 1e-2\n",
        "LABEL_SMOOTHING = 0.1\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.15\n",
        "TEST_RATIO = 0.15\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 1. 유틸 함수들\n",
        "# =========================\n",
        "\n",
        "EXPECTED_POSE = 33\n",
        "EXPECTED_HAND = 21  # left/right 둘 다\n",
        "\n",
        "\n",
        "def load_json(path: str) -> Dict[str, Any]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def get_landmark_array(lst: List[Dict[str, float]], expected_len: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    lst: [{\"x\":..., \"y\":..., \"z\":..., \"v\":...}, ...]\n",
        "    expected_len 길이로 (expected_len, 4) 배열 생성 (부족하면 0 패딩)\n",
        "    + NaN/Inf -> 0으로 치환\n",
        "    \"\"\"\n",
        "    arr = np.zeros((expected_len, 4), dtype=np.float32)\n",
        "    for i, lm in enumerate(lst[:expected_len]):\n",
        "        arr[i, 0] = lm.get(\"x\", 0.0)\n",
        "        arr[i, 1] = lm.get(\"y\", 0.0)\n",
        "        arr[i, 2] = lm.get(\"z\", 0.0)\n",
        "        arr[i, 3] = lm.get(\"v\", 0.0)\n",
        "\n",
        "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def normalize_skeleton(frame_kpts: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    frame_kpts: (75, 4) = pose(33) + left_hand(21) + right_hand(21)\n",
        "    - 어깨 중심 기준 translation\n",
        "    - 어깨폭으로 scale\n",
        "    - 어깨 라인이 수평이 되도록 (x, y) 회전\n",
        "    \"\"\"\n",
        "    frame_kpts = np.nan_to_num(frame_kpts, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    pose = frame_kpts[:EXPECTED_POSE]  # (33, 4)\n",
        "\n",
        "    # Mediapipe 기준: 11 = left_shoulder, 12 = right_shoulder\n",
        "    ls_idx, rs_idx = 11, 12\n",
        "    ls = pose[ls_idx]  # (x,y,z,v)\n",
        "    rs = pose[rs_idx]\n",
        "\n",
        "    use_shoulders = (ls[3] > 0.3 and rs[3] > 0.3)\n",
        "\n",
        "    if use_shoulders:\n",
        "        origin = (ls[:3] + rs[:3]) / 2.0  # (x,y,z)\n",
        "        shoulder_vec_xy = rs[:2] - ls[:2]\n",
        "        shoulder_dist = np.linalg.norm(shoulder_vec_xy)\n",
        "        if not np.isfinite(shoulder_dist) or shoulder_dist < 1e-4:\n",
        "            shoulder_dist = 1.0\n",
        "        scale = shoulder_dist\n",
        "        theta = math.atan2(float(shoulder_vec_xy[1]), float(shoulder_vec_xy[0]))\n",
        "    else:\n",
        "        origin = frame_kpts[:, :3].mean(axis=0)\n",
        "        origin = np.nan_to_num(origin, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        scale = 1.0\n",
        "        theta = 0.0\n",
        "\n",
        "    kpts = frame_kpts.copy()\n",
        "    kpts[:, :3] = (kpts[:, :3] - origin[None, :]) / (scale + 1e-6)\n",
        "\n",
        "    if abs(theta) > 1e-3:\n",
        "        cos_t = math.cos(theta)\n",
        "        sin_t = math.sin(theta)\n",
        "        x = kpts[:, 0].copy()\n",
        "        y = kpts[:, 1].copy()\n",
        "        kpts[:, 0] = x * cos_t + y * sin_t\n",
        "        kpts[:, 1] = -x * sin_t + y * cos_t\n",
        "\n",
        "    kpts = np.nan_to_num(kpts, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return kpts\n",
        "\n",
        "\n",
        "def resample_sequence(seq: np.ndarray, target_len: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    seq: (L, D)\n",
        "    target_len: T\n",
        "    \"\"\"\n",
        "    seq = np.nan_to_num(seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    L = seq.shape[0]\n",
        "    if L == target_len:\n",
        "        return seq\n",
        "    if L <= 1:\n",
        "        return np.repeat(seq, target_len, axis=0)\n",
        "\n",
        "    idxs = np.linspace(0, L - 1, target_len)\n",
        "    idxs = np.round(idxs).astype(np.int32)\n",
        "    idxs = np.clip(idxs, 0, L - 1)\n",
        "    out = seq[idxs]\n",
        "    out = np.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return out\n",
        "\n",
        "\n",
        "def add_velocity_feature(seq: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    seq: (T, D)\n",
        "    -> [position, velocity] concat (T, 2D)\n",
        "    \"\"\"\n",
        "    seq = np.nan_to_num(seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    vel = np.diff(seq, axis=0, prepend=seq[0:1])  # (T, D)\n",
        "    vel = np.nan_to_num(vel, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    out = np.concatenate([seq, vel], axis=-1)     # (T, 2D)\n",
        "    out = np.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return out\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 2. 데이터 로딩 & 전처리\n",
        "# =========================\n",
        "\n",
        "SEQ_GROUP_RE = re.compile(r\"(.*)_s(\\d+)_hands$\")  # base_id, segment index 추출용\n",
        "\n",
        "\n",
        "def collect_sequences(base_dir: str):\n",
        "    \"\"\"\n",
        "    base_dir 아래:\n",
        "        person(1~10) / WORD..._s00_hands.json\n",
        "    \"\"\"\n",
        "    all_seq_arrays = []\n",
        "    all_labels = []\n",
        "\n",
        "    person_dirs = sorted(\n",
        "        [d for d in glob.glob(os.path.join(base_dir, \"*\")) if os.path.isdir(d)]\n",
        "    )\n",
        "    print(f\"Found person dirs: {person_dirs}\")\n",
        "\n",
        "    for p_dir in person_dirs:\n",
        "        person_name = os.path.basename(p_dir)\n",
        "\n",
        "        json_files = glob.glob(os.path.join(p_dir, \"*_hands.json\"))\n",
        "\n",
        "        groups = {}  # base_id -> {seg_idx:int -> [json_path,...]}\n",
        "        for jf in json_files:\n",
        "            fname = os.path.basename(jf)\n",
        "            stem, _ = os.path.splitext(fname)\n",
        "            m = SEQ_GROUP_RE.match(stem)\n",
        "            if not m:\n",
        "                continue\n",
        "            base_id, seg_str = m.groups()\n",
        "            seg_idx = int(seg_str)\n",
        "            if base_id not in groups:\n",
        "                groups[base_id] = {}\n",
        "            if seg_idx not in groups[base_id]:\n",
        "                groups[base_id][seg_idx] = []\n",
        "            groups[base_id][seg_idx].append(jf)\n",
        "\n",
        "        print(f\"[Person {person_name}] #base sequences: {len(groups)}\")\n",
        "\n",
        "        for base_id, seg_dict in groups.items():\n",
        "            seg_indices = sorted(seg_dict.keys())\n",
        "\n",
        "            segment_features = []\n",
        "            label_word = None\n",
        "\n",
        "            for seg_idx in seg_indices:\n",
        "                seg_jsons = sorted(seg_dict[seg_idx])\n",
        "                if not seg_jsons:\n",
        "                    continue\n",
        "\n",
        "                frame_kpts_list = []\n",
        "                for jf in seg_jsons:\n",
        "                    data = load_json(jf)\n",
        "\n",
        "                    if label_word is None:\n",
        "                        label_word = data.get(\"word_folder\", \"UNKNOWN\")\n",
        "\n",
        "                    pose = get_landmark_array(data.get(\"pose\", []), EXPECTED_POSE)\n",
        "                    lh = get_landmark_array(data.get(\"left_hand\", []), EXPECTED_HAND)\n",
        "                    rh = get_landmark_array(data.get(\"right_hand\", []), EXPECTED_HAND)\n",
        "                    frame_kpts = np.concatenate([pose, lh, rh], axis=0)  # (75, 4)\n",
        "                    frame_kpts = normalize_skeleton(frame_kpts)\n",
        "                    frame_kpts_list.append(frame_kpts)\n",
        "\n",
        "                if not frame_kpts_list:\n",
        "                    continue\n",
        "\n",
        "                seg_arr = np.stack(frame_kpts_list, axis=0).mean(axis=0)  # (75,4)\n",
        "                seg_arr = np.nan_to_num(seg_arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                segment_features.append(seg_arr)\n",
        "\n",
        "            if not segment_features:\n",
        "                continue\n",
        "\n",
        "            seq_arr = np.stack(segment_features, axis=0)  # (L, 75, 4)\n",
        "            seq_arr = np.nan_to_num(seq_arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            all_seq_arrays.append(seq_arr)\n",
        "            all_labels.append(label_word if label_word is not None else \"UNKNOWN\")\n",
        "\n",
        "    print(f\"Total sequences (base_id level): {len(all_seq_arrays)}\")\n",
        "    return all_seq_arrays, all_labels\n",
        "\n",
        "\n",
        "def build_label_mapping(labels: List[str]) -> Dict[str, int]:\n",
        "    uniq = sorted(list(set(labels)))\n",
        "    label2idx = {lab: i for i, lab in enumerate(uniq)}\n",
        "    print(\"Label mapping:\")\n",
        "    for k, v in label2idx.items():\n",
        "        print(f\"  {k} -> {v}\")\n",
        "    return label2idx\n",
        "\n",
        "\n",
        "def prepare_dataset(\n",
        "    base_dir: str, target_len: int\n",
        ") -> Tuple[List[np.ndarray], List[int], int]:\n",
        "    \"\"\"\n",
        "    (L,75,4) -> (T, 300) -> velocity concat (T, 600)\n",
        "    \"\"\"\n",
        "    raw_seqs, raw_labels = collect_sequences(base_dir)\n",
        "    label2idx = build_label_mapping(raw_labels)\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    for seq_arr, lab in zip(raw_seqs, raw_labels):\n",
        "        seq_arr = np.nan_to_num(seq_arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        L, J, C = seq_arr.shape  # (L, 75, 4)\n",
        "        seq_flat = seq_arr.reshape(L, J * C)  # (L, 300)\n",
        "        seq_flat = np.nan_to_num(seq_flat, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        seq_resampled = resample_sequence(seq_flat, target_len)  # (T, 300)\n",
        "        seq_with_vel = add_velocity_feature(seq_resampled)       # (T, 600)\n",
        "        seq_with_vel = np.nan_to_num(seq_with_vel, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        X_list.append(seq_with_vel.astype(np.float32))\n",
        "        y_list.append(label2idx[lab])\n",
        "\n",
        "    X_arr = np.stack(X_list, axis=0)\n",
        "    print(\"== After prepare_dataset (with nan_to_num) ==\")\n",
        "    print(\"Has NaN in X:\", np.isnan(X_arr).any())\n",
        "    print(\"Has Inf in X:\", np.isinf(X_arr).any())\n",
        "    print(\"Max abs value:\", float(np.max(np.abs(X_arr))))\n",
        "\n",
        "    input_dim = X_list[0].shape[1]\n",
        "    num_classes = len(label2idx)\n",
        "    print(f\"Final input dim D' = {input_dim}\")\n",
        "    print(f\"num_classes = {num_classes}\")\n",
        "\n",
        "    return X_list, y_list, num_classes\n",
        "\n",
        "\n",
        "def compute_global_stats(\n",
        "    X_list: List[np.ndarray], indices: List[int]\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    train split에서만 mean/std 계산\n",
        "    \"\"\"\n",
        "    if len(indices) == 0:\n",
        "        raise ValueError(\"No indices provided for computing global stats.\")\n",
        "    all_train = np.concatenate([X_list[i] for i in indices], axis=0)\n",
        "    all_train = np.nan_to_num(all_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    mean = all_train.mean(axis=0)\n",
        "    std = all_train.std(axis=0) + 1e-6\n",
        "    mean = np.nan_to_num(mean, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    std = np.nan_to_num(std, nan=1.0, posinf=1.0, neginf=1.0)\n",
        "    return mean.astype(np.float32), std.astype(np.float32)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 3. Dataset & Augmentation\n",
        "# =========================\n",
        "\n",
        "def augment_seq_tensor(seq: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    seq: (T, D)\n",
        "    \"\"\"\n",
        "    T, D = seq.shape\n",
        "\n",
        "    # global scale\n",
        "    if random.random() < 0.5:\n",
        "        scale = random.uniform(0.95, 1.05)\n",
        "        seq = seq * scale\n",
        "\n",
        "    # time jitter\n",
        "    if random.random() < 0.5 and T > 8:\n",
        "        cut = random.randint(0, 2)\n",
        "        if cut > 0:\n",
        "            seq_short = seq[cut:]\n",
        "            pad = seq_short[-1:].repeat(cut, 1)\n",
        "            seq = torch.cat([seq_short, pad], dim=0)\n",
        "\n",
        "    return seq\n",
        "\n",
        "\n",
        "class KeypointSequenceDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        X_list: List[np.ndarray],\n",
        "        y_list: List[int],\n",
        "        indices: List[int],\n",
        "        mean: np.ndarray = None,\n",
        "        std: np.ndarray = None,\n",
        "        augment: bool = False,\n",
        "    ):\n",
        "        self.X_list = X_list\n",
        "        self.y_list = y_list\n",
        "        self.indices = indices\n",
        "        self.augment = augment\n",
        "\n",
        "        if mean is not None and std is not None:\n",
        "            self.mean = torch.from_numpy(mean).float()\n",
        "            self.std = torch.from_numpy(std).float()\n",
        "        else:\n",
        "            self.mean = None\n",
        "            self.std = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]\n",
        "        seq = self.X_list[real_idx]  # (T, D)\n",
        "        label = self.y_list[real_idx]\n",
        "\n",
        "        seq = torch.from_numpy(seq).float()  # (T, D)\n",
        "\n",
        "        if self.mean is not None and self.std is not None:\n",
        "            seq = (seq - self.mean) / self.std\n",
        "\n",
        "        if self.augment:\n",
        "            if random.random() < 0.7:\n",
        "                noise_std = 0.01\n",
        "                seq = seq + torch.randn_like(seq) * noise_std\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                max_shift = 3\n",
        "                shift = random.randint(-max_shift, max_shift)\n",
        "                if shift != 0:\n",
        "                    seq = torch.roll(seq, shifts=shift, dims=0)\n",
        "\n",
        "            seq = augment_seq_tensor(seq)\n",
        "\n",
        "        return seq, label\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4. TCN + Attention Pooling\n",
        "# =========================\n",
        "\n",
        "class TemporalConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "        padding = ((kernel_size - 1) * dilation) // 2\n",
        "\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            in_channels, out_channels, kernel_size,\n",
        "            padding=padding, dilation=dilation\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(\n",
        "            out_channels, out_channels, kernel_size,\n",
        "            padding=padding, dilation=dilation\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.downsample = None\n",
        "        if in_channels != out_channels:\n",
        "            self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=\"relu\")\n",
        "        if self.downsample is not None:\n",
        "            nn.init.kaiming_normal_(self.downsample.weight, nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C_in, T)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "\n",
        "        out = out + x\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class AttnPool1d(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(in_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        x_perm = x.transpose(1, 2)          # (B, T, C)\n",
        "        scores = self.attn(x_perm).squeeze(-1)  # (B, T)\n",
        "        weights = torch.softmax(scores, dim=-1)  # (B, T)\n",
        "        pooled = torch.bmm(weights.unsqueeze(1), x_perm)  # (B, 1, C)\n",
        "        return pooled.squeeze(1)  # (B, C)\n",
        "\n",
        "\n",
        "class TCNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim: int, num_classes: int, hidden_channels: int = 256):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_channels)\n",
        "\n",
        "        self.tcn = nn.Sequential(\n",
        "            TemporalConvBlock(hidden_channels, hidden_channels, kernel_size=3, dilation=1),\n",
        "            TemporalConvBlock(hidden_channels, hidden_channels, kernel_size=3, dilation=2),\n",
        "            TemporalConvBlock(hidden_channels, hidden_channels, kernel_size=3, dilation=4),\n",
        "        )\n",
        "\n",
        "        self.global_pool = AttnPool1d(hidden_channels)\n",
        "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, D)\n",
        "        x = self.input_proj(x)   # (B, T, C)\n",
        "        x = x.transpose(1, 2)    # (B, C, T)\n",
        "        x = self.tcn(x)          # (B, C, T)\n",
        "        x = self.global_pool(x)  # (B, C)\n",
        "        logits = self.fc(x)      # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 5. 학습/평가 루프\n",
        "# =========================\n",
        "\n",
        "def run_epoch(model, loader, optimizer=None, criterion=None):\n",
        "    if optimizer is None:\n",
        "        model.eval()\n",
        "    else:\n",
        "        model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for seq, label in loader:\n",
        "        seq = seq.to(DEVICE)\n",
        "        label = label.to(DEVICE)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model(seq)\n",
        "        loss = criterion(logits, label)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * seq.size(0)\n",
        "        _, pred = torch.max(logits, dim=1)\n",
        "        total_correct += (pred == label).sum().item()\n",
        "        total_count += seq.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_count\n",
        "    avg_acc = total_correct / total_count\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 6. 랜덤 train/val/test split\n",
        "# =========================\n",
        "\n",
        "def random_split_indices(\n",
        "    num_samples: int,\n",
        "    train_ratio: float,\n",
        "    val_ratio: float,\n",
        "    test_ratio: float,\n",
        "    seed: int = 42,\n",
        ") -> Tuple[List[int], List[int], List[int]]:\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"비율 합이 1이 되게 설정해줘.\"\n",
        "\n",
        "    rng = random.Random(seed)\n",
        "    indices = list(range(num_samples))\n",
        "    rng.shuffle(indices)\n",
        "\n",
        "    n_train = int(num_samples * train_ratio)\n",
        "    n_val = int(num_samples * val_ratio)\n",
        "    # 나머지는 test\n",
        "    n_test = num_samples - n_train - n_val\n",
        "\n",
        "    train_idx = indices[:n_train]\n",
        "    val_idx = indices[n_train:n_train + n_val]\n",
        "    test_idx = indices[n_train + n_val:]\n",
        "\n",
        "    print(f\"[Random Split] N={num_samples}, #train={len(train_idx)}, #val={len(val_idx)}, #test={len(test_idx)}\")\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 7. 메인\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    X_list, y_list, num_classes = prepare_dataset(BASE_DIR, TARGET_LEN)\n",
        "    input_dim = X_list[0].shape[1]\n",
        "    print(f\"Input dim for model: {input_dim}\")\n",
        "\n",
        "    num_samples = len(X_list)\n",
        "    train_idx, val_idx, test_idx = random_split_indices(\n",
        "        num_samples,\n",
        "        TRAIN_RATIO,\n",
        "        VAL_RATIO,\n",
        "        TEST_RATIO,\n",
        "        seed=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    # train 기준 mean/std\n",
        "    train_mean, train_std = compute_global_stats(X_list, train_idx)\n",
        "    print(\"[Stats] mean/std computed from train set.\")\n",
        "\n",
        "    train_dataset = KeypointSequenceDataset(\n",
        "        X_list, y_list, train_idx,\n",
        "        mean=train_mean, std=train_std,\n",
        "        augment=True\n",
        "    )\n",
        "    val_dataset = KeypointSequenceDataset(\n",
        "        X_list, y_list, val_idx,\n",
        "        mean=train_mean, std=train_std,\n",
        "        augment=False\n",
        "    )\n",
        "    test_dataset = KeypointSequenceDataset(\n",
        "        X_list, y_list, test_idx,\n",
        "        mean=train_mean, std=train_std,\n",
        "        augment=False\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
        "    )\n",
        "\n",
        "    model = TCNClassifier(input_dim=input_dim, num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        train_loss, train_acc = run_epoch(model, train_loader, optimizer, criterion)\n",
        "        val_loss, val_acc = run_epoch(model, val_loader, optimizer=None, criterion=criterion)\n",
        "\n",
        "        print(\n",
        "            f\"[Epoch {epoch:02d}] \"\n",
        "            f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, \"\n",
        "            f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\"\n",
        "        )\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = model.state_dict()\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    test_loss, test_acc = run_epoch(model, test_loader, optimizer=None, criterion=criterion)\n",
        "    print(\n",
        "        f\"[Final] Best val_acc={best_val_acc:.3f}, \"\n",
        "        f\"Test loss={test_loss:.4f}, Test acc={test_acc:.3f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPNpKwSsBSC4",
        "outputId": "8790a66d-2811-4eee-e76b-c8bfffe011c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Found person dirs: ['/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/1', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/10', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/2', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/3', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/4', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/5', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/6', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/7', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/8', '/content/drive/MyDrive/2025CVproject/preprocessing/Keypoints_json/9']\n",
            "[Person 1] #base sequences: 107\n",
            "[Person 10] #base sequences: 110\n",
            "[Person 2] #base sequences: 110\n",
            "[Person 3] #base sequences: 110\n",
            "[Person 4] #base sequences: 110\n",
            "[Person 5] #base sequences: 110\n",
            "[Person 6] #base sequences: 110\n",
            "[Person 7] #base sequences: 110\n",
            "[Person 8] #base sequences: 110\n",
            "[Person 9] #base sequences: 110\n",
            "Total sequences (base_id level): 1097\n",
            "Label mapping:\n",
            "  WORD0029_검사 -> 0\n",
            "  WORD0033_당뇨병 -> 1\n",
            "  WORD0036_면역 -> 2\n",
            "  WORD0037_감기 -> 3\n",
            "  WORD0039_변비 -> 4\n",
            "  WORD0040_병명 -> 5\n",
            "  WORD0041_보건소 -> 6\n",
            "  WORD0042_불면증 -> 7\n",
            "  WORD0046_설사 -> 8\n",
            "  WORD0062_진단서 -> 9\n",
            "  WORD0064_치료 -> 10\n",
            "  WORD0065_치료법 -> 11\n",
            "  WORD0163_의사 -> 12\n",
            "  WORD0187_간호사 -> 13\n",
            "  WORD0400_정밀검사 -> 14\n",
            "  WORD0572_환자실 -> 15\n",
            "  WORD0689_통증 -> 16\n",
            "  WORD0885_치료제 -> 17\n",
            "  WORD1115_건강 -> 18\n",
            "  WORD1129_검사 -> 19\n",
            "  WORD1158_피곤하다 -> 20\n",
            "  WORD1496_병원 -> 21\n",
            "== After prepare_dataset (with nan_to_num) ==\n",
            "Has NaN in X: False\n",
            "Has Inf in X: False\n",
            "Max abs value: 1.0\n",
            "Final input dim D' = 600\n",
            "num_classes = 22\n",
            "Input dim for model: 600\n",
            "[Random Split] N=1097, #train=767, #val=164, #test=166\n",
            "[Stats] mean/std computed from train set.\n",
            "[Epoch 01] train_loss=2.9198, train_acc=0.151, val_loss=2.4751, val_acc=0.378\n",
            "[Epoch 02] train_loss=2.2361, train_acc=0.467, val_loss=1.8682, val_acc=0.561\n",
            "[Epoch 03] train_loss=1.7371, train_acc=0.648, val_loss=1.5180, val_acc=0.671\n",
            "[Epoch 04] train_loss=1.4553, train_acc=0.726, val_loss=1.4202, val_acc=0.701\n",
            "[Epoch 05] train_loss=1.2660, train_acc=0.789, val_loss=1.2135, val_acc=0.774\n",
            "[Epoch 06] train_loss=1.1450, train_acc=0.846, val_loss=1.0844, val_acc=0.835\n",
            "[Epoch 07] train_loss=1.0525, train_acc=0.874, val_loss=1.1292, val_acc=0.848\n",
            "[Epoch 08] train_loss=1.0173, train_acc=0.889, val_loss=1.0237, val_acc=0.860\n",
            "[Epoch 09] train_loss=0.9635, train_acc=0.911, val_loss=1.0808, val_acc=0.866\n",
            "[Epoch 10] train_loss=0.9123, train_acc=0.930, val_loss=0.9907, val_acc=0.884\n",
            "[Epoch 11] train_loss=0.8834, train_acc=0.937, val_loss=0.9544, val_acc=0.896\n",
            "[Epoch 12] train_loss=0.8251, train_acc=0.970, val_loss=0.9621, val_acc=0.921\n",
            "[Epoch 13] train_loss=0.8176, train_acc=0.963, val_loss=0.9414, val_acc=0.909\n",
            "[Epoch 14] train_loss=0.8270, train_acc=0.957, val_loss=0.9276, val_acc=0.927\n",
            "[Epoch 15] train_loss=0.7819, train_acc=0.977, val_loss=0.9028, val_acc=0.927\n",
            "[Epoch 16] train_loss=0.7791, train_acc=0.971, val_loss=0.9066, val_acc=0.921\n",
            "[Epoch 17] train_loss=0.7843, train_acc=0.962, val_loss=0.8969, val_acc=0.915\n",
            "[Epoch 18] train_loss=0.7595, train_acc=0.979, val_loss=0.8644, val_acc=0.939\n",
            "[Epoch 19] train_loss=0.7603, train_acc=0.971, val_loss=0.8452, val_acc=0.951\n",
            "[Epoch 20] train_loss=0.7488, train_acc=0.973, val_loss=0.9007, val_acc=0.915\n",
            "[Epoch 21] train_loss=0.7352, train_acc=0.979, val_loss=0.8565, val_acc=0.939\n",
            "[Epoch 22] train_loss=0.7347, train_acc=0.988, val_loss=0.8462, val_acc=0.945\n",
            "[Epoch 23] train_loss=0.7351, train_acc=0.983, val_loss=0.8447, val_acc=0.939\n",
            "[Epoch 24] train_loss=0.7233, train_acc=0.984, val_loss=0.8670, val_acc=0.933\n",
            "[Epoch 25] train_loss=0.7066, train_acc=0.991, val_loss=0.8158, val_acc=0.957\n",
            "[Epoch 26] train_loss=0.6995, train_acc=0.988, val_loss=0.8139, val_acc=0.957\n",
            "[Epoch 27] train_loss=0.6994, train_acc=0.991, val_loss=0.8255, val_acc=0.951\n",
            "[Epoch 28] train_loss=0.6963, train_acc=0.992, val_loss=0.8238, val_acc=0.957\n",
            "[Epoch 29] train_loss=0.6888, train_acc=0.995, val_loss=0.8326, val_acc=0.939\n",
            "[Epoch 30] train_loss=0.6962, train_acc=0.990, val_loss=0.8223, val_acc=0.957\n",
            "[Epoch 31] train_loss=0.6904, train_acc=0.992, val_loss=0.8154, val_acc=0.957\n",
            "[Epoch 32] train_loss=0.6870, train_acc=0.993, val_loss=0.8090, val_acc=0.957\n",
            "[Epoch 33] train_loss=0.6861, train_acc=0.995, val_loss=0.8088, val_acc=0.957\n",
            "[Epoch 34] train_loss=0.6824, train_acc=0.999, val_loss=0.8045, val_acc=0.957\n",
            "[Epoch 35] train_loss=0.6775, train_acc=0.999, val_loss=0.8047, val_acc=0.957\n",
            "[Epoch 36] train_loss=0.6818, train_acc=0.993, val_loss=0.8081, val_acc=0.957\n",
            "[Epoch 37] train_loss=0.6789, train_acc=0.995, val_loss=0.8026, val_acc=0.957\n",
            "[Epoch 38] train_loss=0.6808, train_acc=0.996, val_loss=0.8041, val_acc=0.957\n",
            "[Epoch 39] train_loss=0.6798, train_acc=0.995, val_loss=0.8065, val_acc=0.957\n",
            "[Epoch 40] train_loss=0.6780, train_acc=0.996, val_loss=0.8062, val_acc=0.957\n",
            "[Final] Best val_acc=0.957, Test loss=0.8048, Test acc=0.946\n"
          ]
        }
      ]
    }
  ]
}