{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjFvKmo4l5EF",
        "outputId": "55a253fd-4568-40c1-9cf0-00139a1c2903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "BASE_DIR ì¡´ìž¬: True\n",
            "í•˜ìœ„ í´ë”: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'seq_embeddings_hands']\n",
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# 1) êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2) skeleton ì´ë¯¸ì§€ê°€ ë“¤ì–´ ìžˆëŠ” ìµœìƒìœ„ í´ë” ê²½ë¡œ ì§€ì •\n",
        "#    ðŸ‘‰ ì—¬ê¸°ë§Œ ë„¤ í™˜ê²½ì— ë§žê²Œ ìˆ˜ì •í•´ì¤˜\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/CV/Holistic_hands_frames\")\n",
        "\n",
        "print(\"BASE_DIR ì¡´ìž¬:\", BASE_DIR.exists())\n",
        "print(\"í•˜ìœ„ í´ë”:\", [p.name for p in BASE_DIR.iterdir() if p.is_dir()])\n",
        "\n",
        "# 3) ë””ë°”ì´ìŠ¤ + ì‹œë“œ ê³ ì •\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "rows = []\n",
        "\n",
        "for person_dir in BASE_DIR.iterdir():\n",
        "    if not person_dir.is_dir():\n",
        "        continue\n",
        "    person = person_dir.name  # '1', '2', ..., '10'\n",
        "\n",
        "    for f in sorted(person_dir.iterdir()):\n",
        "        if not f.is_file():\n",
        "            continue\n",
        "        if not f.name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            continue\n",
        "\n",
        "        stem = f.stem  # í™•ìž¥ìž ëº€ íŒŒì¼ëª…\n",
        "\n",
        "        # 1) ë‹¨ì–´: WORD0029_ê²€ì‚¬ ê¹Œì§€ë§Œ ë½‘ê¸°\n",
        "        m_word = re.match(r\"(WORD\\d+_[^_]+)\", stem)\n",
        "        if m_word:\n",
        "            word = m_word.group(1)   # ì˜ˆ: WORD0029_ê²€ì‚¬\n",
        "        else:\n",
        "            # í˜•ì‹ ì•ˆ ë§žëŠ” ê±´ ìŠ¤í‚µ\n",
        "            # print(\"WARN: word íŒŒì‹± ì‹¤íŒ¨:\", stem)\n",
        "            continue\n",
        "\n",
        "        # 2) ê°ë„ + í”„ë ˆìž„ index: ..._REAL01_D_s00 ì²˜ëŸ¼ ë˜ì–´ ìžˆìŒ\n",
        "        #    â†’ angle = D, frame_idx = 0\n",
        "        m_angle = re.search(r\"_(R|D|U|F|L)_s(\\d+)$\", stem)\n",
        "        if m_angle:\n",
        "            angle = m_angle.group(1)           # 'R' / 'D' / 'U' / 'F' / 'L'\n",
        "            frame_idx = int(m_angle.group(2))  # 0~15\n",
        "        else:\n",
        "            # í˜•ì‹ ì•ˆ ë§žìœ¼ë©´ ì¼ë‹¨ angle=Xë¡œ ë¬¶ê³  frame_idx=0\n",
        "            angle = \"X\"\n",
        "            frame_idx = 0\n",
        "\n",
        "        # 3) ì‹œí€€ìŠ¤ id = ì‚¬ëžŒ / ë‹¨ì–´ / ê°ë„\n",
        "        seq_id = f\"{person}/{word}/{angle}\"\n",
        "\n",
        "        rows.append({\n",
        "            \"person\": person,\n",
        "            \"word\": word,         # ë¶„ë¥˜ labelì€ word ê¸°ì¤€ (ê°ë„ëŠ” ì¶”ê°€ ì •ë³´)\n",
        "            \"angle\": angle,\n",
        "            \"seq_id\": seq_id,\n",
        "            \"frame_idx\": frame_idx,\n",
        "            \"path\": str(f),\n",
        "        })\n",
        "\n",
        "df_frames = pd.DataFrame(rows)\n",
        "df_frames = df_frames.sort_values([\"seq_id\", \"frame_idx\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"ì´ í”„ë ˆìž„ ìˆ˜:\", len(df_frames))\n",
        "print(\"ì´ ì‹œí€€ìŠ¤ ìˆ˜:\", df_frames[\"seq_id\"].nunique())\n",
        "print(\"ë‹¨ì–´ ê°œìˆ˜:\", df_frames[\"word\"].nunique())\n",
        "print(\"ë‹¨ì–´ ì˜ˆì‹œ:\", df_frames[\"word\"].unique()[:10])\n",
        "\n",
        "print(\"\\nseq_idë³„ í”„ë ˆìž„ ìˆ˜ (ì•ž 5ê°œ)\")\n",
        "print(df_frames.groupby(\"seq_id\")[\"frame_idx\"].count().head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pt7551toE9o",
        "outputId": "9211de66-059d-493b-cac8-15a98e66af85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ í”„ë ˆìž„ ìˆ˜: 17552\n",
            "ì´ ì‹œí€€ìŠ¤ ìˆ˜: 1097\n",
            "ë‹¨ì–´ ê°œìˆ˜: 22\n",
            "ë‹¨ì–´ ì˜ˆì‹œ: ['WORD0029_á„€á…¥á†·á„‰á…¡' 'WORD0033_á„ƒá…¡á†¼á„‚á…­á„‡á…§á†¼' 'WORD0036_á„†á…§á†«á„‹á…§á†¨' 'WORD0037_á„€á…¡á†·á„€á…µ'\n",
            " 'WORD0039_á„‡á…§á†«á„‡á…µ' 'WORD0040_á„‡á…§á†¼á„†á…§á†¼' 'WORD0041_á„‡á…©á„€á…¥á†«á„‰á…©'\n",
            " 'WORD0042_á„‡á…®á†¯á„†á…§á†«á„Œá…³á†¼' 'WORD0046_á„‰á…¥á†¯á„‰á…¡' 'WORD0062_á„Œá…µá†«á„ƒá…¡á†«á„‰á…¥']\n",
            "\n",
            "seq_idë³„ í”„ë ˆìž„ ìˆ˜ (ì•ž 5ê°œ)\n",
            "seq_id\n",
            "1/WORD0029_á„€á…¥á†·á„‰á…¡/D    16\n",
            "1/WORD0029_á„€á…¥á†·á„‰á…¡/F    16\n",
            "1/WORD0029_á„€á…¥á†·á„‰á…¡/L    16\n",
            "1/WORD0029_á„€á…¥á†·á„‰á…¡/R    16\n",
            "1/WORD0029_á„€á…¥á†·á„‰á…¡/U    16\n",
            "Name: frame_idx, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) ì‹œí€€ìŠ¤ í…Œì´ë¸”\n",
        "df_seq = df_frames[[\"seq_id\", \"word\"]].drop_duplicates()\n",
        "print(\"ì´ ì‹œí€€ìŠ¤ ìˆ˜:\", len(df_seq))\n",
        "\n",
        "# 2) ì‹œí€€ìŠ¤ ì…”í”Œ í›„ 70 / 15 / 15 ë¶„í• \n",
        "seq_ids = df_seq[\"seq_id\"].values\n",
        "np.random.seed(SEED)\n",
        "np.random.shuffle(seq_ids)\n",
        "\n",
        "N_seq = len(seq_ids)\n",
        "train_end = int(N_seq * 0.7)\n",
        "val_end   = train_end + int(N_seq * 0.15)\n",
        "\n",
        "train_seq = seq_ids[:train_end]\n",
        "val_seq   = seq_ids[train_end:val_end]\n",
        "test_seq  = seq_ids[val_end:]\n",
        "\n",
        "print(\"train seq ìˆ˜:\", len(train_seq))\n",
        "print(\"val   seq ìˆ˜:\", len(val_seq))\n",
        "print(\"test  seq ìˆ˜:\", len(test_seq))\n",
        "\n",
        "# 3) split ë¼ë²¨ ë¶€ì—¬\n",
        "seq_split_map = {}\n",
        "seq_split_map.update({s: \"train\" for s in train_seq})\n",
        "seq_split_map.update({s: \"val\"   for s in val_seq})\n",
        "seq_split_map.update({s: \"test\"  for s in test_seq})\n",
        "\n",
        "df_frames[\"split\"] = df_frames[\"seq_id\"].map(seq_split_map)\n",
        "\n",
        "print(\"\\ní”„ë ˆìž„ ê¸°ì¤€ split ë¶„í¬:\")\n",
        "print(df_frames[\"split\"].value_counts(normalize=True))\n",
        "print(df_frames[\"split\"].value_counts())\n",
        "\n",
        "# 4) ë‹¨ì–´ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ (label2idx)\n",
        "classes = sorted(df_frames[\"word\"].unique())\n",
        "label2idx = {c: i for i, c in enumerate(classes)}\n",
        "idx2label = {v: k for k, v in label2idx.items()}\n",
        "\n",
        "print(\"\\ní´ëž˜ìŠ¤ ê°œìˆ˜:\", len(classes))\n",
        "print(\"ì•ž 10ê°œ í´ëž˜ìŠ¤:\", classes[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl_ucNSYoFAJ",
        "outputId": "24142811-789d-4357-a607-32ed1c91d7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ ì‹œí€€ìŠ¤ ìˆ˜: 1097\n",
            "train seq ìˆ˜: 767\n",
            "val   seq ìˆ˜: 164\n",
            "test  seq ìˆ˜: 166\n",
            "\n",
            "í”„ë ˆìž„ ê¸°ì¤€ split ë¶„í¬:\n",
            "split\n",
            "train    0.699180\n",
            "test     0.151322\n",
            "val      0.149499\n",
            "Name: proportion, dtype: float64\n",
            "split\n",
            "train    12272\n",
            "test      2656\n",
            "val       2624\n",
            "Name: count, dtype: int64\n",
            "\n",
            "í´ëž˜ìŠ¤ ê°œìˆ˜: 22\n",
            "ì•ž 10ê°œ í´ëž˜ìŠ¤: ['WORD0029_á„€á…¥á†·á„‰á…¡', 'WORD0033_á„ƒá…¡á†¼á„‚á…­á„‡á…§á†¼', 'WORD0036_á„†á…§á†«á„‹á…§á†¨', 'WORD0037_á„€á…¡á†·á„€á…µ', 'WORD0039_á„‡á…§á†«á„‡á…µ', 'WORD0040_á„‡á…§á†¼á„†á…§á†¼', 'WORD0041_á„‡á…©á„€á…¥á†«á„‰á…©', 'WORD0042_á„‡á…®á†¯á„†á…§á†«á„Œá…³á†¼', 'WORD0046_á„‰á…¥á†¯á„‰á…¡', 'WORD0062_á„Œá…µá†«á„ƒá…¡á†«á„‰á…¥']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ ì•„ì£¼ ë‹¨ìˆœí•œ transform (baseline)\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # ì „ì²´ íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ 224Ã—224ë¡œ\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std =[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "def build_tile_4x4(frame_paths):\n",
        "    \"\"\"\n",
        "    frame_paths: í•œ ì‹œí€€ìŠ¤ì— ì†í•œ í”„ë ˆìž„ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
        "    â†’ ìµœëŒ€ 16ìž¥ë§Œ ì‚¬ìš©í•´ì„œ 4Ã—4 grid íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„±\n",
        "    \"\"\"\n",
        "    grid_x, grid_y = 4, 4\n",
        "    tile_w, tile_h = 96, 96\n",
        "    N = grid_x * grid_y  # 16\n",
        "\n",
        "    # 1) frame_pathsì—ì„œ ì•žì—ì„œë¶€í„° ìµœëŒ€ 16ìž¥ë§Œ ì‚¬ìš©\n",
        "    if len(frame_paths) >= N:\n",
        "        paths = frame_paths[:N]\n",
        "    else:\n",
        "        # ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ í”„ë ˆìž„ìœ¼ë¡œ ì±„ì›Œë„£ê¸°\n",
        "        paths = frame_paths + [frame_paths[-1]] * (N - len(frame_paths))\n",
        "\n",
        "    imgs = []\n",
        "    for p in paths:\n",
        "        img = Image.open(p).convert(\"RGB\").resize((tile_w, tile_h))\n",
        "        imgs.append(img)\n",
        "\n",
        "    # 2) 4Ã—4ë¡œ ë¶™ì´ê¸°\n",
        "    canvas = Image.new(\"RGB\", (tile_w * grid_x, tile_h * grid_y), (0,0,0))\n",
        "    k = 0\n",
        "    for y in range(grid_y):\n",
        "        for x in range(grid_x):\n",
        "            canvas.paste(imgs[k], (x * tile_w, y * tile_h))\n",
        "            k += 1\n",
        "\n",
        "    return canvas\n",
        "\n",
        "\n",
        "class SkeletonTilingBaselineDataset(Dataset):\n",
        "    \"\"\"\n",
        "    ê°€ìž¥ ë‹¨ìˆœí•œ baseline:\n",
        "      - seq_id ë‹¨ìœ„ë¡œ ë¬¶ê³ \n",
        "      - ê° seqë¥¼ 4Ã—4 íƒ€ì¼ 1ìž¥ìœ¼ë¡œ ë§Œë“¤ê³ \n",
        "      - ê·¸ê±¸ CNNì— ë„£ì–´ì„œ ë‹¨ì–´ ë¶„ë¥˜\n",
        "    \"\"\"\n",
        "    def __init__(self, df_frames, split, label2idx, transform=None):\n",
        "        self.df = df_frames[df_frames[\"split\"] == split].copy()\n",
        "        self.label2idx = label2idx\n",
        "        self.transform = transform if transform is not None else base_transform\n",
        "\n",
        "        self.grouped = self.df.groupby(\"seq_id\")\n",
        "        self.seq_ids = list(self.grouped.groups.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seq_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq_id = self.seq_ids[idx]\n",
        "        seq_df = self.grouped.get_group(seq_id).sort_values(\"frame_idx\")\n",
        "\n",
        "        frame_paths = seq_df[\"path\"].values.tolist()\n",
        "        word = seq_df[\"word\"].iloc[0]\n",
        "        label = self.label2idx[word]\n",
        "\n",
        "        # 1ê°œ ì‹œí€€ìŠ¤ â†’ 4Ã—4 íƒ€ì¼ ì´ë¯¸ì§€ 1ìž¥\n",
        "        tile_img = build_tile_4x4(frame_paths)\n",
        "\n",
        "        # í…ì„œë¡œ ë³€í™˜\n",
        "        img_tensor = self.transform(tile_img)   # [3, H, W]\n",
        "        return img_tensor, torch.tensor(label, dtype=torch.long)"
      ],
      "metadata": {
        "id": "080hF-VKoFDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loaders(batch_size=8, num_workers=2):\n",
        "    train_ds = SkeletonTilingBaselineDataset(df_frames, split=\"train\", label2idx=label2idx)\n",
        "    val_ds   = SkeletonTilingBaselineDataset(df_frames, split=\"val\",   label2idx=label2idx)\n",
        "    test_ds  = SkeletonTilingBaselineDataset(df_frames, split=\"test\",  label2idx=label2idx)\n",
        "\n",
        "    print(\"train seq:\", len(train_ds), \" | val seq:\", len(val_ds), \" | test seq:\", len(test_ds))\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=(device==\"cuda\")\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=(device==\"cuda\")\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=(device==\"cuda\")\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "# ðŸ”¹ ResNet18 backbone ì‚¬ìš© (ê°€ìž¥ ë¬´ë‚œí•œ baseline)\n",
        "class TilingResNet18(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        base = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        in_features = base.fc.in_features\n",
        "        base.fc = nn.Linear(in_features, num_classes)\n",
        "        self.backbone = base\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, criterion, optimizer=None, log_interval=20):\n",
        "    if optimizer is None:\n",
        "        model.eval()\n",
        "    else:\n",
        "        model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for batch_idx, (imgs, labels) in enumerate(loader):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_count += labels.size(0)\n",
        "\n",
        "        if (optimizer is not None) and (log_interval is not None) and (batch_idx % log_interval == 0):\n",
        "            print(f\"  [batch {batch_idx:03d}] loss={loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / total_count\n",
        "    acc = total_correct / total_count if total_count > 0 else 0.0\n",
        "    return avg_loss, acc"
      ],
      "metadata": {
        "id": "V3MOMILdoFF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def train_model(num_epochs=15, lr=1e-3, weight_decay=1e-4,\n",
        "                batch_size=8, log_interval=20):\n",
        "\n",
        "    print(\"\\n===== Tiling Baseline Training ì‹œìž‘ =====\")\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_loaders(\n",
        "        batch_size=batch_size,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    model = TilingResNet18(num_classes=len(label2idx)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state = None\n",
        "    patience = 5\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\n=== Epoch {epoch:02d} ===\")\n",
        "        train_loss, train_acc = run_epoch(\n",
        "            model, train_loader, criterion,\n",
        "            optimizer=optimizer,\n",
        "            log_interval=log_interval,\n",
        "        )\n",
        "        val_loss, val_acc = run_epoch(\n",
        "            model, val_loader, criterion,\n",
        "            optimizer=None,\n",
        "            log_interval=None,\n",
        "        )\n",
        "\n",
        "        print(f\"[Epoch {epoch:02d}] \"\n",
        "              f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = deepcopy(model.state_dict())\n",
        "            no_improve = 0\n",
        "            print(f\"  -> best val acc ê°±ì‹ : {best_val_acc:.4f}\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            print(f\"  (no_improve={no_improve}/{patience})\")\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (no_improve={no_improve})\")\n",
        "            break\n",
        "\n",
        "    # best ëª¨ë¸ë¡œ test í‰ê°€\n",
        "    model.load_state_dict(best_state)\n",
        "    test_loss, test_acc = run_epoch(\n",
        "        model, test_loader, criterion,\n",
        "        optimizer=None,\n",
        "        log_interval=None,\n",
        "    )\n",
        "    print(f\"\\n[Final] best_val_acc={best_val_acc:.4f}, test_acc={test_acc:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"best_val_acc\": float(best_val_acc),\n",
        "        \"test_acc\": float(test_acc),\n",
        "    }\n",
        "\n",
        "\n",
        "# ì‹¤ì œ ì‹¤í–‰\n",
        "results = []\n",
        "\n",
        "stats = train_model(\n",
        "    num_epochs=20,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    batch_size=8,\n",
        "    log_interval=10,\n",
        ")\n",
        "results.append(stats)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n===== ìµœì¢… ê²°ê³¼ =====\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCZO5_ytobzW",
        "outputId": "ea98ca02-43ad-4948-8b99-16947eb3775e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Tiling Baseline Training ì‹œìž‘ =====\n",
            "train seq: 767  | val seq: 164  | test seq: 166\n",
            "\n",
            "=== Epoch 01 ===\n",
            "  [batch 000] loss=3.1452\n",
            "  [batch 010] loss=2.8464\n",
            "  [batch 020] loss=2.1658\n",
            "  [batch 030] loss=2.4295\n",
            "  [batch 040] loss=2.5661\n",
            "  [batch 050] loss=1.9250\n",
            "  [batch 060] loss=1.6774\n",
            "  [batch 070] loss=2.1742\n",
            "  [batch 080] loss=2.6272\n",
            "  [batch 090] loss=1.9245\n",
            "[Epoch 01] train_loss=2.3716, train_acc=0.3064 | val_loss=3.6456, val_acc=0.2744\n",
            "  -> best val acc ê°±ì‹ : 0.2744\n",
            "\n",
            "=== Epoch 02 ===\n",
            "  [batch 000] loss=1.4403\n",
            "  [batch 010] loss=0.9958\n",
            "  [batch 020] loss=1.2111\n",
            "  [batch 030] loss=2.1846\n",
            "  [batch 040] loss=1.1000\n",
            "  [batch 050] loss=1.4350\n",
            "  [batch 060] loss=1.6275\n",
            "  [batch 070] loss=2.6602\n",
            "  [batch 080] loss=2.0586\n",
            "  [batch 090] loss=1.6089\n",
            "[Epoch 02] train_loss=1.7092, train_acc=0.4785 | val_loss=1.9354, val_acc=0.4268\n",
            "  -> best val acc ê°±ì‹ : 0.4268\n",
            "\n",
            "=== Epoch 03 ===\n",
            "  [batch 000] loss=1.3749\n",
            "  [batch 010] loss=0.5502\n",
            "  [batch 020] loss=1.0784\n",
            "  [batch 030] loss=1.0279\n",
            "  [batch 040] loss=2.1792\n",
            "  [batch 050] loss=0.5448\n",
            "  [batch 060] loss=1.4266\n",
            "  [batch 070] loss=1.3098\n",
            "  [batch 080] loss=1.1948\n",
            "  [batch 090] loss=1.0123\n",
            "[Epoch 03] train_loss=1.3016, train_acc=0.5763 | val_loss=1.5137, val_acc=0.5793\n",
            "  -> best val acc ê°±ì‹ : 0.5793\n",
            "\n",
            "=== Epoch 04 ===\n",
            "  [batch 000] loss=1.3715\n",
            "  [batch 010] loss=0.3900\n",
            "  [batch 020] loss=0.7840\n",
            "  [batch 030] loss=1.1085\n",
            "  [batch 040] loss=0.7473\n",
            "  [batch 050] loss=0.6711\n",
            "  [batch 060] loss=2.0258\n",
            "  [batch 070] loss=1.0614\n",
            "  [batch 080] loss=0.9899\n",
            "  [batch 090] loss=1.8116\n",
            "[Epoch 04] train_loss=1.0448, train_acc=0.6662 | val_loss=1.0178, val_acc=0.6524\n",
            "  -> best val acc ê°±ì‹ : 0.6524\n",
            "\n",
            "=== Epoch 05 ===\n",
            "  [batch 000] loss=0.8964\n",
            "  [batch 010] loss=0.7275\n",
            "  [batch 020] loss=0.3422\n",
            "  [batch 030] loss=0.9225\n",
            "  [batch 040] loss=0.3271\n",
            "  [batch 050] loss=0.8204\n",
            "  [batch 060] loss=0.3201\n",
            "  [batch 070] loss=0.6715\n",
            "  [batch 080] loss=0.2349\n",
            "  [batch 090] loss=0.4729\n",
            "[Epoch 05] train_loss=0.7620, train_acc=0.7679 | val_loss=1.2020, val_acc=0.6402\n",
            "  (no_improve=1/5)\n",
            "\n",
            "=== Epoch 06 ===\n",
            "  [batch 000] loss=0.4172\n",
            "  [batch 010] loss=0.8382\n",
            "  [batch 020] loss=0.1689\n",
            "  [batch 030] loss=0.5427\n",
            "  [batch 040] loss=0.5811\n",
            "  [batch 050] loss=0.2378\n",
            "  [batch 060] loss=0.9709\n",
            "  [batch 070] loss=0.7438\n",
            "  [batch 080] loss=1.1148\n",
            "  [batch 090] loss=1.2888\n",
            "[Epoch 06] train_loss=0.7234, train_acc=0.7744 | val_loss=1.4191, val_acc=0.6585\n",
            "  -> best val acc ê°±ì‹ : 0.6585\n",
            "\n",
            "=== Epoch 07 ===\n",
            "  [batch 000] loss=0.4075\n",
            "  [batch 010] loss=0.1928\n",
            "  [batch 020] loss=0.8809\n",
            "  [batch 030] loss=0.3732\n",
            "  [batch 040] loss=0.4677\n",
            "  [batch 050] loss=0.4159\n",
            "  [batch 060] loss=0.2447\n",
            "  [batch 070] loss=0.1799\n",
            "  [batch 080] loss=0.2515\n",
            "  [batch 090] loss=0.4711\n",
            "[Epoch 07] train_loss=0.4859, train_acc=0.8383 | val_loss=4.2799, val_acc=0.3476\n",
            "  (no_improve=1/5)\n",
            "\n",
            "=== Epoch 08 ===\n",
            "  [batch 000] loss=0.2962\n",
            "  [batch 010] loss=0.7697\n",
            "  [batch 020] loss=0.5630\n",
            "  [batch 030] loss=0.4574\n",
            "  [batch 040] loss=1.2544\n",
            "  [batch 050] loss=0.2186\n",
            "  [batch 060] loss=0.3690\n",
            "  [batch 070] loss=0.0608\n",
            "  [batch 080] loss=0.6704\n",
            "  [batch 090] loss=0.7386\n",
            "[Epoch 08] train_loss=0.4038, train_acc=0.8879 | val_loss=1.5043, val_acc=0.6220\n",
            "  (no_improve=2/5)\n",
            "\n",
            "=== Epoch 09 ===\n",
            "  [batch 000] loss=0.6418\n",
            "  [batch 010] loss=0.1041\n",
            "  [batch 020] loss=1.0934\n",
            "  [batch 030] loss=0.1547\n",
            "  [batch 040] loss=0.1673\n",
            "  [batch 050] loss=0.0485\n",
            "  [batch 060] loss=0.0326\n",
            "  [batch 070] loss=0.9659\n",
            "  [batch 080] loss=0.1604\n",
            "  [batch 090] loss=1.1085\n",
            "[Epoch 09] train_loss=0.2908, train_acc=0.9153 | val_loss=10.7606, val_acc=0.1037\n",
            "  (no_improve=3/5)\n",
            "\n",
            "=== Epoch 10 ===\n",
            "  [batch 000] loss=0.0823\n",
            "  [batch 010] loss=0.0567\n",
            "  [batch 020] loss=0.5231\n",
            "  [batch 030] loss=0.0433\n",
            "  [batch 040] loss=0.3891\n",
            "  [batch 050] loss=0.5648\n",
            "  [batch 060] loss=0.2832\n",
            "  [batch 070] loss=0.2339\n",
            "  [batch 080] loss=0.1404\n",
            "  [batch 090] loss=0.2531\n",
            "[Epoch 10] train_loss=0.3127, train_acc=0.9035 | val_loss=3.5571, val_acc=0.4268\n",
            "  (no_improve=4/5)\n",
            "\n",
            "=== Epoch 11 ===\n",
            "  [batch 000] loss=0.0165\n",
            "  [batch 010] loss=0.2974\n",
            "  [batch 020] loss=0.4483\n",
            "  [batch 030] loss=0.1690\n",
            "  [batch 040] loss=0.7106\n",
            "  [batch 050] loss=0.3691\n",
            "  [batch 060] loss=0.1014\n",
            "  [batch 070] loss=0.5707\n",
            "  [batch 080] loss=0.2780\n",
            "  [batch 090] loss=0.4660\n",
            "[Epoch 11] train_loss=0.3839, train_acc=0.8801 | val_loss=2.5539, val_acc=0.3841\n",
            "  (no_improve=5/5)\n",
            "\n",
            "Early stopping at epoch 11 (no_improve=5)\n",
            "\n",
            "[Final] best_val_acc=0.6585, test_acc=0.6084\n",
            "\n",
            "===== ìµœì¢… ê²°ê³¼ =====\n",
            "   best_val_acc  test_acc\n",
            "0      0.658537  0.608434\n"
          ]
        }
      ]
    }
  ]
}