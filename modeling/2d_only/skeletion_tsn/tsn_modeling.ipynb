{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syl-ptijSiFQ"
      },
      "source": [
        "# 0. 공통 import & 경로"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHsywA0IS6Oy",
        "outputId": "73af4138-9e6e-4468-8af9-4f848b8528d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bNzYoCCpSG7g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6OWSTZeSmAY",
        "outputId": "66a19d7b-4b65-45cc-e21c-effaf983f34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HAND_IMG_ROOT: /content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands\n",
            "TSN_META_PATH: /content/drive/MyDrive/cv-medislr/data/preprocessed/metadata/tsn_hands_seq_meta.csv\n",
            "META exists: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/cv-medislr/data\"\n",
        "PRE_DIR  = os.path.join(BASE_DIR, \"preprocessed\")\n",
        "\n",
        "HAND_IMG_ROOT = os.path.join(PRE_DIR, \"holistic_frames\", \"hands\")\n",
        "TSN_META_PATH = os.path.join(PRE_DIR, \"metadata\", \"tsn_hands_seq_meta.csv\")\n",
        "\n",
        "print(\"HAND_IMG_ROOT:\", HAND_IMG_ROOT)\n",
        "print(\"TSN_META_PATH:\", TSN_META_PATH)\n",
        "print(\"META exists:\", os.path.exists(TSN_META_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tiONuBDee8Qd"
      },
      "outputs": [],
      "source": [
        "# (추가) frames_img 안에 박혀있는 \"옛 경로\"를 \"새 경로\"로 바꿔줄 루트\n",
        "OLD_HAND_ROOT = \"/content/drive/MyDrive/preprocessing/Holistic_hands_frames\"\n",
        "NEW_HAND_ROOT = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXiP9sQhSsGC"
      },
      "source": [
        "# 1. TSN 메타 로드 + frames_img를 리스트로 복원"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YJpkK8ZSvC4",
        "outputId": "443e6af2-2f24-4593-9554-23d86ff3df14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       seq_id  person_id word_code  label_idx  \\\n",
            "0  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_D          1  WORD0029          0   \n",
            "1  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_F          1  WORD0029          0   \n",
            "2  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_L          1  WORD0029          0   \n",
            "3  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_R          1  WORD0029          0   \n",
            "4  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_U          1  WORD0029          0   \n",
            "\n",
            "  view                                         frames_img  \n",
            "0    D  ['/content/drive/MyDrive/preprocessing/Holisti...  \n",
            "1    F  ['/content/drive/MyDrive/preprocessing/Holisti...  \n",
            "2    L  ['/content/drive/MyDrive/preprocessing/Holisti...  \n",
            "3    R  ['/content/drive/MyDrive/preprocessing/Holisti...  \n",
            "4    U  ['/content/drive/MyDrive/preprocessing/Holisti...  \n",
            "Index(['seq_id', 'person_id', 'word_code', 'label_idx', 'view', 'frames_img'], dtype='object')\n",
            "rows: 1097\n"
          ]
        }
      ],
      "source": [
        "tsn_df = pd.read_csv(TSN_META_PATH)\n",
        "print(tsn_df.head())\n",
        "print(tsn_df.columns)\n",
        "print(\"rows:\", len(tsn_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx1mQ9CaSyMv",
        "outputId": "124e5835-ff4a-486e-d9d6-8082758e58ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_path: /content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands/1/WORD0029_검사_NIA_SL_WORD0029_REAL01_D_s00.png\n",
            "exists: True\n",
            "<class 'list'> 16\n",
            "['/content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands/1/WORD0029_검사_NIA_SL_WORD0029_REAL01_D_s00.png', '/content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands/1/WORD0029_검사_NIA_SL_WORD0029_REAL01_D_s01.png', '/content/drive/MyDrive/cv-medislr/data/preprocessed/holistic_frames/hands/1/WORD0029_검사_NIA_SL_WORD0029_REAL01_D_s02.png']\n"
          ]
        }
      ],
      "source": [
        "def parse_frames_cell(val):\n",
        "    # 이미 리스트이면 그대로\n",
        "    if isinstance(val, list):\n",
        "        return val\n",
        "    # 문자열이면 리스트 리터럴일 가능성\n",
        "    if isinstance(val, str):\n",
        "        s = val.strip()\n",
        "        if not s:\n",
        "            return []\n",
        "        try:\n",
        "            return ast.literal_eval(s)\n",
        "        except Exception:\n",
        "            # 그냥 하나의 경로만 문자열인 경우\n",
        "            return [s]\n",
        "    return []\n",
        "\n",
        "# (추가) 옛 경로 -> 새 경로 치환\n",
        "def remap_frame_path(p: str) -> str:\n",
        "    p = str(p)\n",
        "    if p.startswith(OLD_HAND_ROOT):\n",
        "        p = p.replace(OLD_HAND_ROOT, NEW_HAND_ROOT, 1)\n",
        "    return p\n",
        "\n",
        "tsn_df[\"frames_img\"] = tsn_df[\"frames_img\"].apply(parse_frames_cell)\n",
        "\n",
        "# (추가) frames_img 내부 경로를 새 루트로 remap\n",
        "tsn_df[\"frames_img\"] = tsn_df[\"frames_img\"].apply(lambda lst: [remap_frame_path(x) for x in lst])\n",
        "\n",
        "import os\n",
        "sample_path = tsn_df.iloc[0][\"frames_img\"][0]\n",
        "print(\"sample_path:\", sample_path)\n",
        "print(\"exists:\", os.path.exists(sample_path))\n",
        "\n",
        "print(type(tsn_df.iloc[0][\"frames_img\"]), len(tsn_df.iloc[0][\"frames_img\"]))\n",
        "print(tsn_df.iloc[0][\"frames_img\"][:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUkS0TreTEV6"
      },
      "source": [
        "# 2. Grayscale 변환 + Transform 정의\n",
        "손 skeleton은 흰 선만 있으니 1채널(grayscale) 이 효율적."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WSesPiNITLO7"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 64\n",
        "\n",
        "train_transform = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),                       # (1, H, W)\n",
        "])\n",
        "\n",
        "eval_transform = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyFcGCBeTNSZ"
      },
      "source": [
        "# 3. 손 부분 crop 함수 + Dataset 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iHtwl4rTTL37"
      },
      "outputs": [],
      "source": [
        "def crop_to_hand(img_pil, margin=50):\n",
        "    \"\"\"\n",
        "    img_pil: PIL.Image (grayscale)\n",
        "    0이 아닌 픽셀의 bounding box를 찾아 crop\n",
        "    \"\"\"\n",
        "    arr = np.array(img_pil)\n",
        "\n",
        "    # grayscale 변환\n",
        "    if arr.ndim == 3:\n",
        "        gray = arr.mean(axis=2)\n",
        "    else:\n",
        "        gray = arr\n",
        "\n",
        "    # non-zero 픽셀 찾기\n",
        "    ys, xs = np.nonzero(gray)\n",
        "\n",
        "    # 만약 skeleton이 1픽셀도 없으면 원본 그대로 반환\n",
        "    if len(xs) == 0 or len(ys) == 0:\n",
        "        return img_pil\n",
        "\n",
        "    # bounding box\n",
        "    x_min, x_max = xs.min(), xs.max()\n",
        "    y_min, y_max = ys.min(), ys.max()\n",
        "\n",
        "    # margin 추가\n",
        "    x_min = max(0, x_min - margin)\n",
        "    y_min = max(0, y_min - margin)\n",
        "    x_max = min(gray.shape[1] - 1, x_max + margin)\n",
        "    y_max = min(gray.shape[0] - 1, y_max + margin)\n",
        "\n",
        "    # crop\n",
        "    return img_pil.crop((x_min, y_min, x_max + 1, y_max + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q7oLt52eTTme"
      },
      "outputs": [],
      "source": [
        "# # 3-2. HandTSNDataset\n",
        "\n",
        "# FRAMES_COL = \"frames_img\"\n",
        "\n",
        "# class HandTSNDataset(Dataset):\n",
        "#     def __init__(self, df, transform=None):\n",
        "#         self.df = df.reset_index(drop=True)\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.df.iloc[idx]\n",
        "#         frame_paths = row[FRAMES_COL]\n",
        "#         label = int(row[\"label_idx\"])\n",
        "\n",
        "#         frames = []\n",
        "#         for p in frame_paths:\n",
        "#             img = Image.open(p).convert(\"L\")      # 1채널로 읽기\n",
        "#             img = crop_to_hand(img, margin=50)   # ★ skeleton 주변만 crop\n",
        "\n",
        "#             if self.transform is not None:\n",
        "#                 img = self.transform(img)        # (1, H, W)\n",
        "\n",
        "#             frames.append(img)\n",
        "\n",
        "#         frames = torch.stack(frames, dim=0)      # (T, C, H, W)\n",
        "\n",
        "#         return frames, torch.tensor(label, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxwUxE_-TFHV"
      },
      "source": [
        "# (추가) 전처리용 transform 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "E1c2riWSS_VM"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "IMG_SIZE = 64  # 이미 이 크기로 쓰고 있으면 그대로\n",
        "\n",
        "preprocess_transform = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),                       # (1, H, W)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnieJzh2TBs1",
        "outputId": "17dc4e59-609a-4053-8dc7-4871e9486bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1097/1097 [1:48:39<00:00,  5.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         tensor_path  label_idx\n",
            "0  /content/drive/MyDrive/cv-medislr/data/Hand_te...          0\n",
            "1  /content/drive/MyDrive/cv-medislr/data/Hand_te...          0\n",
            "2  /content/drive/MyDrive/cv-medislr/data/Hand_te...          0\n",
            "3  /content/drive/MyDrive/cv-medislr/data/Hand_te...          0\n",
            "4  /content/drive/MyDrive/cv-medislr/data/Hand_te...          0\n",
            "텐서 메타 저장: /content/drive/MyDrive/cv-medislr/data/Hand_tensors/tsn_hands_tensor_meta.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 텐서 저장 폴더 (원하는 위치로 바꿔도 됨)\n",
        "TENSOR_ROOT = os.path.join(BASE_DIR, \"Hand_tensors\")\n",
        "os.makedirs(TENSOR_ROOT, exist_ok=True)\n",
        "\n",
        "tensor_paths = []  # 각 시퀀스의 .pt 경로를 저장할 리스트\n",
        "\n",
        "for idx, row in tqdm(tsn_df.iterrows(), total=len(tsn_df)):\n",
        "    frame_paths = row[\"frames_img\"]   # 길이 16 리스트\n",
        "    seq_tensors = []\n",
        "\n",
        "    for p in frame_paths:\n",
        "        img = Image.open(p).convert(\"L\")        # 그레이스케일\n",
        "        img = crop_to_hand(img, margin=50)      # 손 주변만 crop\n",
        "        img = preprocess_transform(img)         # (1, H, W), float32\n",
        "        seq_tensors.append(img)\n",
        "\n",
        "    seq_tensor = torch.stack(seq_tensors, dim=0)   # (T, C, H, W) = (16, 1, H, W)\n",
        "\n",
        "    # 파일명: seq_{원래 row index}.pt (원하면 seq_id 써도 됨)\n",
        "    tensor_name = f\"seq_{idx:05d}.pt\"\n",
        "    tensor_path = os.path.join(TENSOR_ROOT, tensor_name)\n",
        "\n",
        "    torch.save(seq_tensor, tensor_path)\n",
        "    tensor_paths.append(tensor_path)\n",
        "\n",
        "# tsn_df에 새 컬럼 추가\n",
        "tsn_df[\"tensor_path\"] = tensor_paths\n",
        "\n",
        "print(tsn_df[[\"tensor_path\", \"label_idx\"]].head())\n",
        "\n",
        "# 메타 저장(선택)\n",
        "TENSOR_META_PATH = os.path.join(TENSOR_ROOT, \"tsn_hands_tensor_meta.csv\")\n",
        "tsn_df.to_csv(TENSOR_META_PATH, index=False)\n",
        "print(\"텐서 메타 저장:\", TENSOR_META_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "NEW_ROOT = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/tensors/tsn_hands\"\n",
        "META_PATH = f\"{NEW_ROOT}/tsn_hands_tensor_meta.csv\"\n",
        "\n",
        "print(\"META_PATH:\", META_PATH)\n",
        "print(\"exists:\", os.path.exists(META_PATH))\n",
        "\n",
        "df = pd.read_csv(META_PATH)\n",
        "\n",
        "def remap_tensor_path(p):\n",
        "    fname = os.path.basename(p)   # seq_00001.pt\n",
        "    return os.path.join(NEW_ROOT, fname)\n",
        "\n",
        "df[\"tensor_path\"] = df[\"tensor_path\"].apply(remap_tensor_path)\n",
        "\n",
        "df.to_csv(META_PATH, index=False)\n",
        "print(\"tensor_path remap 완료\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRO2WW7oM41S",
        "outputId": "b6d5dc67-101b-4a7b-8c16-ea3b23d9932d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "META_PATH: /content/drive/MyDrive/cv-medislr/data/preprocessed/tensors/tsn_hands/tsn_hands_tensor_meta.csv\n",
            "exists: True\n",
            "tensor_path remap 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4Tc7fxA3TUps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1884653-5367-45c8-fcee-21ebcf1cba44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TENSOR_META_PATH: /content/drive/MyDrive/cv-medislr/data/preprocessed/tensors/tsn_hands/tsn_hands_tensor_meta.csv\n",
            "exists: True\n",
            "Index(['seq_id', 'person_id', 'word_code', 'label_idx', 'view', 'frames_img',\n",
            "       'tensor_path'],\n",
            "      dtype='object')\n",
            "                                         tensor_path  label_idx\n",
            "0  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "1  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "2  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "3  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "4  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "rows: 1097\n"
          ]
        }
      ],
      "source": [
        "# 텐서 메타 다시 로드 (런타임 새로 켰을 때용)\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/cv-medislr/data\"\n",
        "PRE_DIR  = os.path.join(BASE_DIR, \"preprocessed\")\n",
        "\n",
        "# ★ 여기 중요\n",
        "TENSOR_ROOT = os.path.join(PRE_DIR, \"tensors\", \"tsn_hands\")\n",
        "\n",
        "TENSOR_META_PATH = os.path.join(\n",
        "    TENSOR_ROOT,\n",
        "    \"tsn_hands_tensor_meta.csv\"\n",
        ")\n",
        "\n",
        "print(\"TENSOR_META_PATH:\", TENSOR_META_PATH)\n",
        "print(\"exists:\", os.path.exists(TENSOR_META_PATH))\n",
        "\n",
        "tensor_df = pd.read_csv(TENSOR_META_PATH)\n",
        "print(tensor_df.columns)\n",
        "print(tensor_df[[\"tensor_path\", \"label_idx\"]].head())\n",
        "print(\"rows:\", len(tensor_df))\n",
        "\n",
        "# tsn_df 대신 이제 tensor_df를 쓰게 될 거야"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivw0tZfYTpQb"
      },
      "source": [
        "# 5. MobileNetTSN (경량 TSN 모델(Small CNN)에서 교체함)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zxuvKvhHTmqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd67500-7db6-4d7e-b68e-3171ab16ddef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pknWBtygTsCS"
      },
      "outputs": [],
      "source": [
        "# class SmallHandTSN(nn.Module):\n",
        "#     def __init__(self, num_classes=22):\n",
        "#         super().__init__()\n",
        "#         self.features = nn.Sequential(\n",
        "#             nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # (B*T,32,H/2,W/2)\n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.ReLU(inplace=True),\n",
        "\n",
        "#             nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # (B*T,64,H/4,W/4)\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU(inplace=True),\n",
        "\n",
        "#             nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),# (B*T,128,H/8,W/8)\n",
        "#             nn.BatchNorm2d(128),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#         )\n",
        "#         self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "#         self.dropout = nn.Dropout(p=0.5)\n",
        "#         self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"\n",
        "#         x: (B, T, C, H, W)  여기서 C=1\n",
        "#         \"\"\"\n",
        "#         B, T, C, H, W = x.shape\n",
        "#         x = x.view(B * T, C, H, W)     # (B*T, C, H, W)\n",
        "#         feat = self.features(x)        # (B*T, 128, h, w)\n",
        "#         feat = self.global_pool(feat)  # (B*T, 128, 1, 1)\n",
        "#         feat = feat.view(B, T, 128)    # (B, T, 128)\n",
        "#         feat = feat.mean(dim=1)        # TSN: 시간 평균 (B, 128)\n",
        "#         feat = self.dropout(feat)\n",
        "#         logits = self.fc(feat)         # (B, num_classes)\n",
        "#         return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CL15jORn273X"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
        "\n",
        "class MobileNetTSN(nn.Module):\n",
        "    def __init__(self, num_classes=22, pretrained=True):\n",
        "        super().__init__()\n",
        "        # ImageNet pretrained MobileNetV2 불러오기\n",
        "        weights = MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        backbone = mobilenet_v2(weights=weights)\n",
        "\n",
        "        self.features = backbone.features           # conv 부분\n",
        "        self.last_channel = backbone.last_channel   # 1280\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(self.last_channel, num_classes)\n",
        "\n",
        "        # ImageNet 정규화용 mean/std (0~1 입력 기준)\n",
        "        self.register_buffer(\n",
        "            \"img_mean\",\n",
        "            torch.tensor([0.485, 0.456, 0.406]).view(1, 1, 3, 1, 1)\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"img_std\",\n",
        "            torch.tensor([0.229, 0.224, 0.225]).view(1, 1, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, C, H, W)  여기서 C=1 (grayscale)\n",
        "        \"\"\"\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # 1채널 → 3채널 replicate (pretrained weight 활용 위해)\n",
        "        if C == 1:\n",
        "            x = x.repeat(1, 1, 3, 1, 1)   # (B, T, 3, H, W)\n",
        "\n",
        "        # ImageNet 정규화\n",
        "        x = (x - self.img_mean) / self.img_std\n",
        "\n",
        "        # (B*T, 3, H, W)로 reshape\n",
        "        x = x.view(B * T, 3, H, W)\n",
        "\n",
        "        feat = self.features(x)          # (B*T, C2, h, w)\n",
        "        feat = self.pool(feat)           # (B*T, C2, 1, 1)\n",
        "        feat = feat.view(B, T, self.last_channel)  # (B, T, C2)\n",
        "\n",
        "        # TSN: 시간 평균 풀링\n",
        "        feat = feat.mean(dim=1)          # (B, C2)\n",
        "\n",
        "        feat = self.dropout(feat)\n",
        "        logits = self.fc(feat)           # (B, num_classes)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTSPJhDuTt-Z"
      },
      "source": [
        "# 6. 학습/검증/테스트 루프"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jltqR8F0Twi1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def run_epoch(model, loader, criterion, optimizer=None, log_interval=None):\n",
        "    \"\"\"\n",
        "    optimizer: None이면 eval, 아니면 train\n",
        "    log_interval: 정수면 batch_log 출력, None이면 출력 안 함\n",
        "    \"\"\"\n",
        "    if optimizer is None:\n",
        "        model.eval()\n",
        "        torch.set_grad_enabled(False)\n",
        "    else:\n",
        "        model.train()\n",
        "        torch.set_grad_enabled(True)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (frames, labels) in enumerate(loader):\n",
        "        # 너무 자주 안 찍히지 않게 interval로 제어\n",
        "        if (log_interval is not None) and (batch_idx % log_interval == 0):\n",
        "            print(f\"    batch {batch_idx}/{len(loader)} ...\")\n",
        "\n",
        "        frames = frames.to(device)   # (B, T, C, H, W)\n",
        "        labels = labels.to(device)   # (B,)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model(frames)       # (B, num_classes)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        if optimizer is not None:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        bs = labels.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += bs\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_acc  = total_correct / total_samples\n",
        "\n",
        "    return avg_loss, avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZLU9uAoMP3GY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05956fae-f46d-4cba-c87a-4161ce2f8aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 767 val: 165 test: 165\n",
            "Saved split: /content/drive/MyDrive/cv-medislr/data/preprocessed/tensors/tsn_hands/split_70_15_15.json\n",
            "train_df: (767, 7)\n",
            "val_df: (165, 7)\n",
            "test_df: (165, 7)\n"
          ]
        }
      ],
      "source": [
        "# 인덱스 70/15/15로 나누기\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO   = 0.15\n",
        "TEST_RATIO  = 0.15\n",
        "assert abs(TRAIN_RATIO + VAL_RATIO + TEST_RATIO - 1.0) < 1e-6\n",
        "\n",
        "indices = np.arange(len(tensor_df))\n",
        "labels  = tensor_df[\"label_idx\"].values  # 0~21\n",
        "\n",
        "# 1) train vs temp\n",
        "train_idx, temp_idx, y_train, y_temp = train_test_split(\n",
        "    indices,\n",
        "    labels,\n",
        "    test_size=VAL_RATIO + TEST_RATIO,\n",
        "    random_state=42,\n",
        "    stratify=labels,\n",
        ")\n",
        "\n",
        "# 2) temp를 val/test로\n",
        "val_ratio_rel = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n",
        "\n",
        "val_idx, test_idx = train_test_split(\n",
        "    temp_idx,\n",
        "    test_size=1.0 - val_ratio_rel,\n",
        "    random_state=42,\n",
        "    stratify=y_temp,\n",
        ")\n",
        "\n",
        "print(\"train:\", len(train_idx), \"val:\", len(val_idx), \"test:\", len(test_idx))\n",
        "\n",
        "# ===== (추가) split 인덱스 저장 =====\n",
        "SPLIT_PATH = os.path.join(TENSOR_ROOT, \"split_70_15_15.json\")\n",
        "split_dict = {\n",
        "    \"train_idx\": train_idx.tolist(),\n",
        "    \"val_idx\": val_idx.tolist(),\n",
        "    \"test_idx\": test_idx.tolist(),\n",
        "    \"seed\": 42\n",
        "}\n",
        "with open(SPLIT_PATH, \"w\") as f:\n",
        "    json.dump(split_dict, f, indent=2)\n",
        "print(\"Saved split:\", SPLIT_PATH)\n",
        "# ====================================\n",
        "\n",
        "train_df = tensor_df.iloc[train_idx].reset_index(drop=True)\n",
        "val_df   = tensor_df.iloc[val_idx].reset_index(drop=True)\n",
        "test_df  = tensor_df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"train_df:\", train_df.shape)\n",
        "print(\"val_df:\", val_df.shape)\n",
        "print(\"test_df:\", test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GSheIzp5TogU"
      },
      "outputs": [],
      "source": [
        "# .pt 텐서용 Dataset 정의\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TensorSeqDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        tensor_path = row[\"tensor_path\"]\n",
        "        label = int(row[\"label_idx\"])\n",
        "\n",
        "        seq_tensor = torch.load(tensor_path)          # (T, C, H, W)\n",
        "        seq_tensor = seq_tensor.float()               # 안전하게 float32 보장\n",
        "\n",
        "        return seq_tensor, torch.tensor(label, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4HwPn04EP9UJ"
      },
      "outputs": [],
      "source": [
        "# 새 DataLoader 헬퍼 (get_loaders)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_loaders(batch_size=8, num_workers=1):\n",
        "    train_ds = TensorSeqDataset(train_df)\n",
        "    val_ds   = TensorSeqDataset(val_df)\n",
        "    test_ds  = TensorSeqDataset(test_df)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=batch_size, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hoHbTd2oQ2cc"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import os\n",
        "\n",
        "def train_model(num_epochs=8, lr=1e-3, weight_decay=1e-4,\n",
        "                batch_size=8, log_interval=20):\n",
        "    print(\"\\n===== Training 시작 =====\")\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_loaders(\n",
        "        batch_size=batch_size,\n",
        "        num_workers=1,\n",
        "    )\n",
        "\n",
        "    model = MobileNetTSN(num_classes=22, pretrained=True).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"max\", factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_state = None\n",
        "    patience = 5\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc = run_epoch(\n",
        "            model, train_loader, criterion,\n",
        "            optimizer=optimizer,\n",
        "            log_interval=log_interval,\n",
        "        )\n",
        "        val_loss, val_acc = run_epoch(\n",
        "            model, val_loader, criterion,\n",
        "            optimizer=None,\n",
        "            log_interval=None,\n",
        "        )\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        print(f\"[Epoch {epoch:02d}] \"\n",
        "              f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = deepcopy(model.state_dict())\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch} (no_improve={no_improve})\")\n",
        "            break\n",
        "\n",
        "    # best 모델로 test 평가\n",
        "    model.load_state_dict(best_state)\n",
        "    test_loss, test_acc = run_epoch(\n",
        "        model, test_loader, criterion,\n",
        "        optimizer=None,\n",
        "        log_interval=None,\n",
        "    )\n",
        "    print(f\"[Final] best_val_acc={best_val_acc:.4f}, test_acc={test_acc:.4f}\")\n",
        "\n",
        "    # ===== best checkpoint 저장 (경로 수정됨) =====\n",
        "    BASE_DIR = \"/content/drive/MyDrive/cv-medislr/data\"\n",
        "    PRE_DIR  = os.path.join(BASE_DIR, \"preprocessed\")\n",
        "\n",
        "    CKPT_DIR = os.path.join(PRE_DIR, \"model_weights\", \"2d_only\")\n",
        "    os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "    CKPT_PATH = os.path.join(CKPT_DIR, \"mobilenet_tsn_hands_best.pt\")\n",
        "    torch.save({\n",
        "        \"model_name\": \"MobileNetTSN\",\n",
        "        \"model_group\": \"2d_only\",\n",
        "        \"num_classes\": 22,\n",
        "        \"img_size\": IMG_SIZE,\n",
        "        \"state_dict\": best_state,\n",
        "        \"best_val_acc\": float(best_val_acc),\n",
        "        \"test_acc\": float(test_acc),\n",
        "    }, CKPT_PATH)\n",
        "\n",
        "    print(\"Saved checkpoint:\", CKPT_PATH)\n",
        "    # ============================================\n",
        "\n",
        "    return {\n",
        "        \"best_val_acc\": float(best_val_acc),\n",
        "        \"test_acc\": float(test_acc),\n",
        "        \"ckpt_path\": CKPT_PATH,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bWxthCoSQ8W_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "038ba79e-05b7-4012-f90a-a428567ba19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Training 시작 =====\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 113MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 01] train_loss=2.5971, train_acc=0.2451 | val_loss=2.9760, val_acc=0.2848\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 02] train_loss=1.6956, train_acc=0.4628 | val_loss=1.4556, val_acc=0.5515\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 03] train_loss=1.5469, train_acc=0.5463 | val_loss=1.1839, val_acc=0.6121\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 04] train_loss=1.2923, train_acc=0.5893 | val_loss=1.3559, val_acc=0.5212\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 05] train_loss=1.1751, train_acc=0.6297 | val_loss=1.5913, val_acc=0.5636\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 06] train_loss=1.0068, train_acc=0.6936 | val_loss=1.1801, val_acc=0.6970\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 07] train_loss=1.0749, train_acc=0.6688 | val_loss=0.9920, val_acc=0.6667\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 08] train_loss=0.8816, train_acc=0.7262 | val_loss=0.8387, val_acc=0.7394\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 09] train_loss=0.7811, train_acc=0.7471 | val_loss=0.7651, val_acc=0.8061\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 10] train_loss=0.7992, train_acc=0.7653 | val_loss=0.7646, val_acc=0.8061\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 11] train_loss=0.7318, train_acc=0.7771 | val_loss=0.8785, val_acc=0.7455\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 12] train_loss=0.6226, train_acc=0.7927 | val_loss=0.8266, val_acc=0.7394\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 13] train_loss=0.5640, train_acc=0.8149 | val_loss=0.6361, val_acc=0.8061\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 14] train_loss=0.4832, train_acc=0.8605 | val_loss=0.4729, val_acc=0.8364\n",
            "    batch 0/96 ...\n",
            "    batch 20/96 ...\n",
            "    batch 40/96 ...\n",
            "    batch 60/96 ...\n",
            "    batch 80/96 ...\n",
            "[Epoch 15] train_loss=0.3165, train_acc=0.8931 | val_loss=0.5514, val_acc=0.8424\n",
            "[Final] best_val_acc=0.8424, test_acc=0.8242\n",
            "Saved checkpoint: /content/drive/MyDrive/cv-medislr/data/preprocessed/model_weights/2d_only/mobilenet_tsn_hands_best.pt\n",
            "\n",
            "===== 최종 결과 =====\n",
            "   best_val_acc  test_acc                                          ckpt_path\n",
            "0      0.842424  0.824242  /content/drive/MyDrive/cv-medislr/data/preproc...\n"
          ]
        }
      ],
      "source": [
        "# 최종 실행\n",
        "\n",
        "results = []\n",
        "\n",
        "stats = train_model(\n",
        "    num_epochs=15,      # 먼저 8 정도로 테스트\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    batch_size=8,\n",
        "    log_interval=20,  # 20 스텝마다 batch 로그; 귀찮으면 None\n",
        ")\n",
        "results.append(stats)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n===== 최종 결과 =====\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 용 20개 샘플 추출"
      ],
      "metadata": {
        "id": "-8x6PCvqSljw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/tensors/tsn_hands\"\n",
        "print(\"exists:\", os.path.exists(path))\n",
        "\n",
        "if os.path.exists(path):\n",
        "    print(\"files:\", os.listdir(path)[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTDsAA2xSdrH",
        "outputId": "e56f59b4-ef45-4efb-c1e9-d842bdad6f58"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exists: True\n",
            "files: ['seq_00000.pt', 'seq_00001.pt', 'seq_00002.pt', 'seq_00003.pt', 'seq_00004.pt', 'seq_00005.pt', 'seq_00006.pt', 'seq_00007.pt', 'seq_00008.pt', 'seq_00009.pt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/cv-medislr/data\"\n",
        "TENSOR_ROOT = os.path.join(BASE_DIR, \"preprocessed/tensors/tsn_hands\")\n",
        "\n",
        "META_PATH = os.path.join(TENSOR_ROOT, \"tsn_hands_tensor_meta.csv\")\n",
        "\n",
        "print(\"META exists:\", os.path.exists(META_PATH))\n",
        "\n",
        "tensor_df = pd.read_csv(META_PATH)\n",
        "print(tensor_df.columns)\n",
        "print(tensor_df[[\"tensor_path\", \"label_idx\"]].head())\n",
        "print(\"rows:\", len(tensor_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whvO8dP6Selx",
        "outputId": "8e4de145-2101-4c55-f1a3-cd4875ad5788"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "META exists: True\n",
            "Index(['seq_id', 'person_id', 'word_code', 'label_idx', 'view', 'frames_img',\n",
            "       'tensor_path'],\n",
            "      dtype='object')\n",
            "                                         tensor_path  label_idx\n",
            "0  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "1  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "2  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "3  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "4  /content/drive/MyDrive/cv-medislr/data/preproc...          0\n",
            "rows: 1097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# ===== 경로 설정 =====\n",
        "BASE_DIR = \"/content/drive/MyDrive/cv-medislr/data\"\n",
        "\n",
        "SRC_TENSOR_DIR = os.path.join(\n",
        "    BASE_DIR, \"preprocessed/tensors/tsn_hands\"\n",
        ")\n",
        "META_PATH = os.path.join(\n",
        "    SRC_TENSOR_DIR, \"tsn_hands_tensor_meta.csv\"\n",
        ")\n",
        "\n",
        "DST_SAMPLE_DIR = os.path.join(\n",
        "    BASE_DIR, \"samples/2d_only/skeleton_tsn\"\n",
        ")\n",
        "os.makedirs(DST_SAMPLE_DIR, exist_ok=True)\n",
        "\n",
        "# ===== 메타 로드 =====\n",
        "df = pd.read_csv(META_PATH)\n",
        "print(\"total tensors:\", len(df))\n",
        "\n",
        "# ===== 20개 랜덤 샘플 =====\n",
        "N_SAMPLES = 20\n",
        "sample_df = df.sample(n=N_SAMPLES, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ===== pt 파일 복사 =====\n",
        "new_tensor_paths = []\n",
        "\n",
        "for i, row in sample_df.iterrows():\n",
        "    src_path = row[\"tensor_path\"]\n",
        "    fname = os.path.basename(src_path)\n",
        "\n",
        "    dst_path = os.path.join(DST_SAMPLE_DIR, fname)\n",
        "    shutil.copy2(src_path, dst_path)\n",
        "\n",
        "    new_tensor_paths.append(dst_path)\n",
        "\n",
        "# ===== 샘플 메타 저장 =====\n",
        "sample_df[\"tensor_path\"] = new_tensor_paths\n",
        "SAMPLE_META_PATH = os.path.join(DST_SAMPLE_DIR, \"tsn_sample_meta.csv\")\n",
        "sample_df.to_csv(SAMPLE_META_PATH, index=False)\n",
        "\n",
        "print(\" 샘플 저장 완료\")\n",
        "print(\"sample dir:\", DST_SAMPLE_DIR)\n",
        "print(\"meta:\", SAMPLE_META_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Px7vXWSh8Y",
        "outputId": "dea29925-958f-47b2-c90f-f29aa9a53c66"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total tensors: 1097\n",
            " 샘플 저장 완료\n",
            "sample dir: /content/drive/MyDrive/cv-medislr/data/samples/2d_only/skeleton_tsn\n",
            "meta: /content/drive/MyDrive/cv-medislr/data/samples/2d_only/skeleton_tsn/tsn_sample_meta.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gfF_pUnES50D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}