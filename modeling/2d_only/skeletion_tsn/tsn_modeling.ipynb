{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2Lccgs7ZqwwPoVYbByHXY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 0. 공통 import & 경로"],"metadata":{"id":"syl-ptijSiFQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHsywA0IS6Oy","executionInfo":{"status":"ok","timestamp":1764271837182,"user_tz":-540,"elapsed":2515,"user":{"displayName":"노준혁","userId":"17692108505654888249"}},"outputId":"aa1eeb14-d6cd-4326-e18d-570512077d6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNzYoCCpSG7g"},"outputs":[],"source":["import os\n","import ast\n","import json\n","import numpy as np\n","import pandas as pd\n","\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","\n","from sklearn.model_selection import StratifiedKFold, train_test_split"]},{"cell_type":"code","source":["# 경로\n","BASE_DIR      = \"/content/drive/MyDrive/preprocessing\"\n","HAND_IMG_ROOT = os.path.join(BASE_DIR, \"Holistic_hands_frames\")\n","TSN_META_PATH = os.path.join(HAND_IMG_ROOT, \"tsn_hands_seq_meta.csv\")\n","\n","print(\"TSN_META_PATH:\", TSN_META_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6OWSTZeSmAY","executionInfo":{"status":"ok","timestamp":1764271837217,"user_tz":-540,"elapsed":17,"user":{"displayName":"노준혁","userId":"17692108505654888249"}},"outputId":"d118faf7-f515-4548-c5dd-cab1e34ee4db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TSN_META_PATH: /content/drive/MyDrive/preprocessing/Holistic_hands_frames/tsn_hands_seq_meta.csv\n"]}]},{"cell_type":"markdown","source":["# 1. TSN 메타 로드 + frames_img를 리스트로 복원"],"metadata":{"id":"OXiP9sQhSsGC"}},{"cell_type":"code","source":["tsn_df = pd.read_csv(TSN_META_PATH)\n","print(tsn_df.head())\n","print(tsn_df.columns)\n","print(\"rows:\", len(tsn_df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0YJpkK8ZSvC4","executionInfo":{"status":"ok","timestamp":1764271837423,"user_tz":-540,"elapsed":191,"user":{"displayName":"노준혁","userId":"17692108505654888249"}},"outputId":"c655d08b-d223-4cd3-f049-8229ec209330"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                       seq_id  person_id word_code  label_idx  \\\n","0  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_D          1  WORD0029          0   \n","1  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_F          1  WORD0029          0   \n","2  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_L          1  WORD0029          0   \n","3  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_R          1  WORD0029          0   \n","4  p1_WORD0029_검사_NIA_SL_WORD0029_REAL01_U          1  WORD0029          0   \n","\n","  view                                         frames_img  \n","0    D  ['/content/drive/MyDrive/preprocessing/Holisti...  \n","1    F  ['/content/drive/MyDrive/preprocessing/Holisti...  \n","2    L  ['/content/drive/MyDrive/preprocessing/Holisti...  \n","3    R  ['/content/drive/MyDrive/preprocessing/Holisti...  \n","4    U  ['/content/drive/MyDrive/preprocessing/Holisti...  \n","Index(['seq_id', 'person_id', 'word_code', 'label_idx', 'view', 'frames_img'], dtype='object')\n","rows: 1097\n"]}]},{"cell_type":"code","source":["def parse_frames_cell(val):\n","    # 이미 리스트이면 그대로\n","    if isinstance(val, list):\n","        return val\n","    # 문자열이면 리스트 리터럴일 가능성\n","    if isinstance(val, str):\n","        s = val.strip()\n","        if not s:\n","            return []\n","        try:\n","            return ast.literal_eval(s)\n","        except Exception:\n","            # 그냥 하나의 경로만 문자열인 경우\n","            return [s]\n","    return []\n","\n","tsn_df[\"frames_img\"] = tsn_df[\"frames_img\"].apply(parse_frames_cell)\n","\n","print(type(tsn_df.iloc[0][\"frames_img\"]), len(tsn_df.iloc[0][\"frames_img\"]))\n","print(tsn_df.iloc[0][\"frames_img\"][:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fx1mQ9CaSyMv","executionInfo":{"status":"ok","timestamp":1764271837602,"user_tz":-540,"elapsed":173,"user":{"displayName":"노준혁","userId":"17692108505654888249"}},"outputId":"fc2a393d-1faa-4c94-9ded-46d37a3248ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'> 16\n","['/content/drive/MyDrive/preprocessing/Holistic_hands_frames/1/WORD0029_검사_NIA_SL_WORD0029_REAL01_D_s00.png', '/content/drive/MyDrive/preprocessing/Holistic_hands_frames/1/WORD0029_검사_NIA_SL_WORD0029_REAL01_D_s01.png', '/content/drive/MyDrive/preprocessing/Holistic_hands_frames/1/WORD0029_검사_NIA_SL_WORD0029_REAL01_D_s02.png']\n"]}]},{"cell_type":"markdown","source":["# 2. Grayscale 변환 + Transform 정의\n","손 skeleton은 흰 선만 있으니 1채널(grayscale) 이 효율적."],"metadata":{"id":"BUkS0TreTEV6"}},{"cell_type":"code","source":["IMG_SIZE = 64\n","\n","train_transform = T.Compose([\n","    T.Resize((IMG_SIZE, IMG_SIZE)),\n","    T.ToTensor(),                       # (1, H, W)\n","])\n","\n","eval_transform = T.Compose([\n","    T.Resize((IMG_SIZE, IMG_SIZE)),\n","    T.ToTensor(),\n","])"],"metadata":{"id":"WSesPiNITLO7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. 손 부분 crop 함수 + Dataset 정의"],"metadata":{"id":"XyFcGCBeTNSZ"}},{"cell_type":"code","source":["def crop_to_hand(img_pil, margin=50):\n","    \"\"\"\n","    img_pil: PIL.Image (grayscale)\n","    0이 아닌 픽셀의 bounding box를 찾아 crop\n","    \"\"\"\n","    arr = np.array(img_pil)\n","\n","    # grayscale 변환\n","    if arr.ndim == 3:\n","        gray = arr.mean(axis=2)\n","    else:\n","        gray = arr\n","\n","    # non-zero 픽셀 찾기\n","    ys, xs = np.nonzero(gray)\n","\n","    # 만약 skeleton이 1픽셀도 없으면 원본 그대로 반환\n","    if len(xs) == 0 or len(ys) == 0:\n","        return img_pil\n","\n","    # bounding box\n","    x_min, x_max = xs.min(), xs.max()\n","    y_min, y_max = ys.min(), ys.max()\n","\n","    # margin 추가\n","    x_min = max(0, x_min - margin)\n","    y_min = max(0, y_min - margin)\n","    x_max = min(gray.shape[1] - 1, x_max + margin)\n","    y_max = min(gray.shape[0] - 1, y_max + margin)\n","\n","    # crop\n","    return img_pil.crop((x_min, y_min, x_max + 1, y_max + 1))"],"metadata":{"id":"iHtwl4rTTL37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 3-2. HandTSNDataset\n","\n","# FRAMES_COL = \"frames_img\"\n","\n","# class HandTSNDataset(Dataset):\n","#     def __init__(self, df, transform=None):\n","#         self.df = df.reset_index(drop=True)\n","#         self.transform = transform\n","\n","#     def __len__(self):\n","#         return len(self.df)\n","\n","#     def __getitem__(self, idx):\n","#         row = self.df.iloc[idx]\n","#         frame_paths = row[FRAMES_COL]\n","#         label = int(row[\"label_idx\"])\n","\n","#         frames = []\n","#         for p in frame_paths:\n","#             img = Image.open(p).convert(\"L\")      # 1채널로 읽기\n","#             img = crop_to_hand(img, margin=50)   # ★ skeleton 주변만 crop\n","\n","#             if self.transform is not None:\n","#                 img = self.transform(img)        # (1, H, W)\n","\n","#             frames.append(img)\n","\n","#         frames = torch.stack(frames, dim=0)      # (T, C, H, W)\n","\n","#         return frames, torch.tensor(label, dtype=torch.long)"],"metadata":{"id":"Q7oLt52eTTme"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# (추가) 전처리용 transform 정의"],"metadata":{"id":"KxwUxE_-TFHV"}},{"cell_type":"code","source":["import torchvision.transforms as T\n","\n","IMG_SIZE = 64  # 이미 이 크기로 쓰고 있으면 그대로\n","\n","preprocess_transform = T.Compose([\n","    T.Resize((IMG_SIZE, IMG_SIZE)),\n","    T.ToTensor(),                       # (1, H, W)\n","])"],"metadata":{"id":"E1c2riWSS_VM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import os\n","from tqdm import tqdm\n","\n","# 텐서 저장 폴더 (원하는 위치로 바꿔도 됨)\n","TENSOR_ROOT = os.path.join(BASE_DIR, \"Hand_tensors\")\n","os.makedirs(TENSOR_ROOT, exist_ok=True)\n","\n","tensor_paths = []  # 각 시퀀스의 .pt 경로를 저장할 리스트\n","\n","for idx, row in tqdm(tsn_df.iterrows(), total=len(tsn_df)):\n","    frame_paths = row[\"frames_img\"]   # 길이 16 리스트\n","    seq_tensors = []\n","\n","    for p in frame_paths:\n","        img = Image.open(p).convert(\"L\")        # 그레이스케일\n","        img = crop_to_hand(img, margin=50)      # 손 주변만 crop\n","        img = preprocess_transform(img)         # (1, H, W), float32\n","        seq_tensors.append(img)\n","\n","    seq_tensor = torch.stack(seq_tensors, dim=0)   # (T, C, H, W) = (16, 1, H, W)\n","\n","    # 파일명: seq_{원래 row index}.pt (원하면 seq_id 써도 됨)\n","    tensor_name = f\"seq_{idx:05d}.pt\"\n","    tensor_path = os.path.join(TENSOR_ROOT, tensor_name)\n","\n","    torch.save(seq_tensor, tensor_path)\n","    tensor_paths.append(tensor_path)\n","\n","# tsn_df에 새 컬럼 추가\n","tsn_df[\"tensor_path\"] = tensor_paths\n","\n","print(tsn_df[[\"tensor_path\", \"label_idx\"]].head())\n","\n","# 메타 저장(선택)\n","TENSOR_META_PATH = os.path.join(TENSOR_ROOT, \"tsn_hands_tensor_meta.csv\")\n","tsn_df.to_csv(TENSOR_META_PATH, index=False)\n","print(\"텐서 메타 저장:\", TENSOR_META_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnieJzh2TBs1","outputId":"a7e96bd7-ad46-455b-8c37-6a2ed80e8ad4","executionInfo":{"status":"ok","timestamp":1764272252409,"user_tz":-540,"elapsed":303763,"user":{"displayName":"노준혁","userId":"17692108505654888249"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1097/1097 [05:03<00:00,  3.61it/s]"]},{"output_type":"stream","name":"stdout","text":["                                         tensor_path  label_idx\n","0  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","1  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","2  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","3  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","4  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","텐서 메타 저장: /content/drive/MyDrive/preprocessing/Hand_tensors/tsn_hands_tensor_meta.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# 텐서 메타 다시 로드 (런타임 새로 켰을 때용)\n","\n","import pandas as pd\n","import torch\n","import os\n","import numpy as np\n","\n","BASE_DIR      = \"/content/drive/MyDrive/preprocessing\"\n","TENSOR_ROOT   = os.path.join(BASE_DIR, \"Hand_tensors\")\n","TENSOR_META_PATH = os.path.join(TENSOR_ROOT, \"tsn_hands_tensor_meta.csv\")\n","\n","tensor_df = pd.read_csv(TENSOR_META_PATH)\n","print(tensor_df.columns)\n","print(tensor_df[[\"tensor_path\", \"label_idx\"]].head())\n","print(\"rows:\", len(tensor_df))\n","\n","# tsn_df 대신 이제 tensor_df를 쓰게 될 거야"],"metadata":{"id":"4Tc7fxA3TUps","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764272993204,"user_tz":-540,"elapsed":97,"user":{"displayName":"노준혁","userId":"17692108505654888249"}},"outputId":"fec1035d-d7c3-418e-a1b2-a3059919a021"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['seq_id', 'person_id', 'word_code', 'label_idx', 'view', 'frames_img',\n","       'tensor_path'],\n","      dtype='object')\n","                                         tensor_path  label_idx\n","0  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","1  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","2  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","3  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","4  /content/drive/MyDrive/preprocessing/Hand_tens...          0\n","rows: 1097\n"]}]},{"cell_type":"markdown","source":["# 5. 경량 TSN 모델(Small CNN)에서 MobileNetTSN으로 교체"],"metadata":{"id":"Ivw0tZfYTpQb"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"],"metadata":{"id":"zxuvKvhHTmqb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764272993222,"user_tz":-540,"elapsed":17,"user":{"displayName":"노준혁","userId":"17692108505654888249"}},"outputId":"4f276592-cdb4-4e99-9f72-c2cd92a057cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"code","source":["# class SmallHandTSN(nn.Module):\n","#     def __init__(self, num_classes=22):\n","#         super().__init__()\n","#         self.features = nn.Sequential(\n","#             nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # (B*T,32,H/2,W/2)\n","#             nn.BatchNorm2d(32),\n","#             nn.ReLU(inplace=True),\n","\n","#             nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # (B*T,64,H/4,W/4)\n","#             nn.BatchNorm2d(64),\n","#             nn.ReLU(inplace=True),\n","\n","#             nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),# (B*T,128,H/8,W/8)\n","#             nn.BatchNorm2d(128),\n","#             nn.ReLU(inplace=True),\n","#         )\n","#         self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n","#         self.dropout = nn.Dropout(p=0.5)\n","#         self.fc = nn.Linear(128, num_classes)\n","\n","#     def forward(self, x):\n","#         \"\"\"\n","#         x: (B, T, C, H, W)  여기서 C=1\n","#         \"\"\"\n","#         B, T, C, H, W = x.shape\n","#         x = x.view(B * T, C, H, W)     # (B*T, C, H, W)\n","#         feat = self.features(x)        # (B*T, 128, h, w)\n","#         feat = self.global_pool(feat)  # (B*T, 128, 1, 1)\n","#         feat = feat.view(B, T, 128)    # (B, T, 128)\n","#         feat = feat.mean(dim=1)        # TSN: 시간 평균 (B, 128)\n","#         feat = self.dropout(feat)\n","#         logits = self.fc(feat)         # (B, num_classes)\n","#         return logits"],"metadata":{"id":"pknWBtygTsCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n","\n","class MobileNetTSN(nn.Module):\n","    def __init__(self, num_classes=22, pretrained=True):\n","        super().__init__()\n","        # ImageNet pretrained MobileNetV2 불러오기\n","        weights = MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None\n","        backbone = mobilenet_v2(weights=weights)\n","\n","        self.features = backbone.features           # conv 부분\n","        self.last_channel = backbone.last_channel   # 1280\n","        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout = nn.Dropout(p=0.5)\n","        self.fc = nn.Linear(self.last_channel, num_classes)\n","\n","        # ImageNet 정규화용 mean/std (0~1 입력 기준)\n","        self.register_buffer(\n","            \"img_mean\",\n","            torch.tensor([0.485, 0.456, 0.406]).view(1, 1, 3, 1, 1)\n","        )\n","        self.register_buffer(\n","            \"img_std\",\n","            torch.tensor([0.229, 0.224, 0.225]).view(1, 1, 3, 1, 1)\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (B, T, C, H, W)  여기서 C=1 (grayscale)\n","        \"\"\"\n","        B, T, C, H, W = x.shape\n","\n","        # 1채널 → 3채널 replicate (pretrained weight 활용 위해)\n","        if C == 1:\n","            x = x.repeat(1, 1, 3, 1, 1)   # (B, T, 3, H, W)\n","\n","        # ImageNet 정규화\n","        x = (x - self.img_mean) / self.img_std\n","\n","        # (B*T, 3, H, W)로 reshape\n","        x = x.view(B * T, 3, H, W)\n","\n","        feat = self.features(x)          # (B*T, C2, h, w)\n","        feat = self.pool(feat)           # (B*T, C2, 1, 1)\n","        feat = feat.view(B, T, self.last_channel)  # (B, T, C2)\n","\n","        # TSN: 시간 평균 풀링\n","        feat = feat.mean(dim=1)          # (B, C2)\n","\n","        feat = self.dropout(feat)\n","        logits = self.fc(feat)           # (B, num_classes)\n","        return logits"],"metadata":{"id":"CL15jORn273X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. 학습/검증/테스트 루프"],"metadata":{"id":"aTSPJhDuTt-Z"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","def run_epoch(model, loader, criterion, optimizer=None, log_interval=None):\n","    \"\"\"\n","    optimizer: None이면 eval, 아니면 train\n","    log_interval: 정수면 batch_log 출력, None이면 출력 안 함\n","    \"\"\"\n","    if optimizer is None:\n","        model.eval()\n","        torch.set_grad_enabled(False)\n","    else:\n","        model.train()\n","        torch.set_grad_enabled(True)\n","\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","\n","    for batch_idx, (frames, labels) in enumerate(loader):\n","        # 너무 자주 안 찍히지 않게 interval로 제어\n","        if (log_interval is not None) and (batch_idx % log_interval == 0):\n","            print(f\"    batch {batch_idx}/{len(loader)} ...\")\n","\n","        frames = frames.to(device)   # (B, T, C, H, W)\n","        labels = labels.to(device)   # (B,)\n","\n","        if optimizer is not None:\n","            optimizer.zero_grad()\n","\n","        logits = model(frames)       # (B, num_classes)\n","        loss = criterion(logits, labels)\n","\n","        if optimizer is not None:\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n","            optimizer.step()\n","\n","        bs = labels.size(0)\n","        total_loss += loss.item() * bs\n","        preds = logits.argmax(dim=1)\n","        total_correct += (preds == labels).sum().item()\n","        total_samples += bs\n","\n","    avg_loss = total_loss / total_samples\n","    avg_acc  = total_correct / total_samples\n","\n","    return avg_loss, avg_acc"],"metadata":{"id":"jltqR8F0Twi1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인덱스 70/15/15로 나누기\n","\n","from sklearn.model_selection import train_test_split\n","\n","TRAIN_RATIO = 0.7\n","VAL_RATIO   = 0.15\n","TEST_RATIO  = 0.15\n","assert abs(TRAIN_RATIO + VAL_RATIO + TEST_RATIO - 1.0) < 1e-6\n","\n","indices = np.arange(len(tensor_df))\n","labels  = tensor_df[\"label_idx\"].values  # 0~21\n","\n","# 1) train vs temp\n","train_idx, temp_idx, y_train, y_temp = train_test_split(\n","    indices,\n","    labels,\n","    test_size=VAL_RATIO + TEST_RATIO,\n","    random_state=42,\n","    stratify=labels,\n",")\n","\n","# 2) temp를 val/test로\n","val_ratio_rel = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\n","\n","val_idx, test_idx = train_test_split(\n","    temp_idx,\n","    test_size=1.0 - val_ratio_rel,\n","    random_state=42,\n","    stratify=y_temp,\n",")\n","\n","print(\"train:\", len(train_idx), \"val:\", len(val_idx), \"test:\", len(test_idx))\n","\n","train_df = tensor_df.iloc[train_idx].reset_index(drop=True)\n","val_df   = tensor_df.iloc[val_idx].reset_index(drop=True)\n","test_df  = tensor_df.iloc[test_idx].reset_index(drop=True)\n","\n","print(\"train_df:\", train_df.shape)\n","print(\"val_df:\", val_df.shape)\n","print(\"test_df:\", test_df.shape)"],"metadata":{"id":"ZLU9uAoMP3GY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764272993256,"user_tz":-540,"elapsed":6,"user":{"displayName":"노준혁","userId":"17692108505654888249"}},"outputId":"25c07611-c683-47cb-8097-35c056fc5a84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train: 767 val: 165 test: 165\n","train_df: (767, 7)\n","val_df: (165, 7)\n","test_df: (165, 7)\n"]}]},{"cell_type":"code","source":["# .pt 텐서용 Dataset 정의\n","\n","from torch.utils.data import Dataset\n","\n","class TensorSeqDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df.reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        tensor_path = row[\"tensor_path\"]\n","        label = int(row[\"label_idx\"])\n","\n","        seq_tensor = torch.load(tensor_path)          # (T, C, H, W)\n","        seq_tensor = seq_tensor.float()               # 안전하게 float32 보장\n","\n","        return seq_tensor, torch.tensor(label, dtype=torch.long)"],"metadata":{"id":"GSheIzp5TogU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 새 DataLoader 헬퍼 (get_loaders)\n","\n","from torch.utils.data import DataLoader\n","\n","def get_loaders(batch_size=8, num_workers=1):\n","    train_ds = TensorSeqDataset(train_df)\n","    val_ds   = TensorSeqDataset(val_df)\n","    test_ds  = TensorSeqDataset(test_df)\n","\n","    train_loader = DataLoader(\n","        train_ds, batch_size=batch_size, shuffle=True,\n","        num_workers=num_workers, pin_memory=True\n","    )\n","    val_loader = DataLoader(\n","        val_ds, batch_size=batch_size, shuffle=False,\n","        num_workers=num_workers, pin_memory=True\n","    )\n","    test_loader = DataLoader(\n","        test_ds, batch_size=batch_size, shuffle=False,\n","        num_workers=num_workers, pin_memory=True\n","    )\n","    return train_loader, val_loader, test_loader"],"metadata":{"id":"4HwPn04EP9UJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_model (단일 모델 학습/검증/테스트)\n","\n","from copy import deepcopy\n","\n","def train_model(num_epochs=8, lr=1e-3, weight_decay=1e-4,\n","                batch_size=8, log_interval=20):\n","    print(\"\\n===== Training 시작 =====\")\n","\n","    train_loader, val_loader, test_loader = get_loaders(\n","        batch_size=batch_size,\n","        num_workers=1,\n","    )\n","\n","    model = MobileNetTSN(num_classes=22, pretrained=True).to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, mode=\"max\", factor=0.5, patience=3\n","    )\n","\n","    best_val_acc = 0.0\n","    best_state = None\n","    patience = 5\n","    no_improve = 0\n","\n","    for epoch in range(1, num_epochs + 1):\n","        train_loss, train_acc = run_epoch(\n","            model, train_loader, criterion,\n","            optimizer=optimizer,\n","            log_interval=log_interval,   # batch 로그 간격\n","        )\n","        val_loss, val_acc = run_epoch(\n","            model, val_loader, criterion,\n","            optimizer=None,\n","            log_interval=None,           # val/test는 굳이 로그 X\n","        )\n","\n","        scheduler.step(val_acc)\n","\n","        print(f\"[Epoch {epoch:02d}] \"\n","              f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f} | \"\n","              f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n","\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            best_state = deepcopy(model.state_dict())\n","            no_improve = 0\n","        else:\n","            no_improve += 1\n","\n","        if no_improve >= patience:\n","            print(f\"Early stopping at epoch {epoch} (no_improve={no_improve})\")\n","            break\n","\n","    # best 모델로 test 평가\n","    model.load_state_dict(best_state)\n","    test_loss, test_acc = run_epoch(\n","        model, test_loader, criterion,\n","        optimizer=None,\n","        log_interval=None,\n","    )\n","    print(f\"[Final] best_val_acc={best_val_acc:.4f}, test_acc={test_acc:.4f}\")\n","\n","    return {\n","        \"best_val_acc\": float(best_val_acc),\n","        \"test_acc\": float(test_acc),\n","    }"],"metadata":{"id":"hoHbTd2oQ2cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 최종 실행\n","\n","results = []\n","\n","stats = train_model(\n","    num_epochs=15,      # 먼저 8 정도로 테스트\n","    lr=1e-3,\n","    weight_decay=1e-4,\n","    batch_size=8,\n","    log_interval=20,  # 20 스텝마다 batch 로그; 귀찮으면 None\n",")\n","results.append(stats)\n","\n","results_df = pd.DataFrame(results)\n","print(\"\\n===== 최종 결과 =====\")\n","print(results_df)"],"metadata":{"id":"bWxthCoSQ8W_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764279708882,"user_tz":-540,"elapsed":4077653,"user":{"displayName":"노준혁","userId":"17692108505654888249"}},"outputId":"822b9f58-93fb-4c6c-a696-5c40a69690be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Training 시작 =====\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n","  warnings.warn(warn_msg)\n"]},{"output_type":"stream","name":"stdout","text":["    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 01] train_loss=2.5644, train_acc=0.2621 | val_loss=3.1080, val_acc=0.3091\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 02] train_loss=1.7028, train_acc=0.4928 | val_loss=2.1305, val_acc=0.4667\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 03] train_loss=1.5375, train_acc=0.5163 | val_loss=1.2908, val_acc=0.6364\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 04] train_loss=1.3097, train_acc=0.5815 | val_loss=1.7380, val_acc=0.5212\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 05] train_loss=1.2106, train_acc=0.6375 | val_loss=1.2721, val_acc=0.6545\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 06] train_loss=1.0822, train_acc=0.6871 | val_loss=1.0670, val_acc=0.6606\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 07] train_loss=0.9702, train_acc=0.7027 | val_loss=0.8365, val_acc=0.7091\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 08] train_loss=0.8889, train_acc=0.7132 | val_loss=1.1457, val_acc=0.6424\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 09] train_loss=0.9173, train_acc=0.7027 | val_loss=0.9690, val_acc=0.7273\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 10] train_loss=0.7485, train_acc=0.7549 | val_loss=0.7671, val_acc=0.7576\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 11] train_loss=0.7236, train_acc=0.7718 | val_loss=0.6979, val_acc=0.7576\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 12] train_loss=0.6559, train_acc=0.7784 | val_loss=0.6995, val_acc=0.7697\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 13] train_loss=0.5551, train_acc=0.8227 | val_loss=0.8279, val_acc=0.7939\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 14] train_loss=0.6016, train_acc=0.8227 | val_loss=0.9545, val_acc=0.7394\n","    batch 0/96 ...\n","    batch 20/96 ...\n","    batch 40/96 ...\n","    batch 60/96 ...\n","    batch 80/96 ...\n","[Epoch 15] train_loss=0.5501, train_acc=0.8331 | val_loss=0.7446, val_acc=0.7879\n","[Final] best_val_acc=0.7939, test_acc=0.7333\n","\n","===== 최종 결과 =====\n","   best_val_acc  test_acc\n","0      0.793939  0.733333\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"W5UaAM3MQ9Ro"},"execution_count":null,"outputs":[]}]}