{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_BB1W0R8_PJ",
        "outputId": "661dcb33-271d-4d73-9407-cf4dfaf4d370"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1D"
      ],
      "metadata": {
        "id": "2DjDPEX78zw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# =========================\n",
        "# 0) 경로 설정\n",
        "# =========================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "DEMO_NPZ  = \"/content/drive/MyDrive/cv-medislr/data/samples/1D/demo_test20_HANDS_T16_seed42.npz\"\n",
        "\n",
        "ARTIFACT_DIR = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/model_weights/1D\"\n",
        "GRU_W    = os.path.join(ARTIFACT_DIR, \"gru_best.pt\")\n",
        "GRU_NORM = os.path.join(ARTIFACT_DIR, \"gru_norm.npz\")\n",
        "GRU_META = os.path.join(ARTIFACT_DIR, \"gru_meta.json\")\n",
        "\n",
        "TCN_W    = os.path.join(ARTIFACT_DIR, \"tcn_best.pt\")\n",
        "TCN_NORM = os.path.join(ARTIFACT_DIR, \"tcn_norm.npz\")\n",
        "TCN_META = os.path.join(ARTIFACT_DIR, \"tcn_meta.json\")\n",
        "\n",
        "assert os.path.exists(DEMO_NPZ), f\"❌ demo npz not found: {DEMO_NPZ}\"\n",
        "\n",
        "for p in [GRU_W, GRU_NORM, GRU_META, TCN_W, TCN_NORM, TCN_META]:\n",
        "    assert os.path.exists(p), f\"❌ artifact not found: {p}\"\n",
        "\n",
        "# =========================\n",
        "# 1) 모델 정의 (학습 코드와 동일)\n",
        "# =========================\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=256, num_layers=2, bidirectional=True, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "        self.attn_fc = nn.Linear(out_dim, 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(out_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # (B,T,D)\n",
        "        out, _ = self.gru(x)                          # (B,T,512)\n",
        "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        w = torch.softmax(self.attn_fc(out), dim=1)   # (B,T,1)\n",
        "        w = torch.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        feat = (w * out).sum(dim=1)                   # (B,512)\n",
        "        return self.head(feat)                        # (B,C)\n",
        "\n",
        "class TemporalConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "        padding = ((kernel_size - 1) * dilation) // 2\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
        "\n",
        "    def forward(self, x):  # (B,C,T)\n",
        "        out = self.dropout(self.relu(self.bn1(self.conv1(x))))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return self.relu(out + x)\n",
        "\n",
        "class AttnPool1d(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(in_channels, 1)\n",
        "\n",
        "    def forward(self, x):  # (B,C,T)\n",
        "        x_perm = x.transpose(1, 2)                    # (B,T,C)\n",
        "        scores = self.attn(x_perm).squeeze(-1)        # (B,T)\n",
        "        weights = torch.softmax(scores, dim=-1)       # (B,T)\n",
        "        pooled = torch.bmm(weights.unsqueeze(1), x_perm)  # (B,1,C)\n",
        "        return pooled.squeeze(1)                      # (B,C)\n",
        "\n",
        "class TCNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_channels=256):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_channels)\n",
        "        self.tcn = nn.Sequential(\n",
        "            TemporalConvBlock(hidden_channels, hidden_channels, kernel_size=3, dilation=1),\n",
        "            TemporalConvBlock(hidden_channels, hidden_channels, kernel_size=3, dilation=2),\n",
        "            TemporalConvBlock(hidden_channels, hidden_channels, kernel_size=3, dilation=4),\n",
        "        )\n",
        "        self.pool = AttnPool1d(hidden_channels)\n",
        "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):  # (B,T,D)\n",
        "        x = self.input_proj(x)       # (B,T,C)\n",
        "        x = x.transpose(1, 2)        # (B,C,T)\n",
        "        x = self.tcn(x)              # (B,C,T)\n",
        "        x = self.pool(x)             # (B,C)\n",
        "        return self.fc(x)            # (B,num_classes)\n",
        "\n",
        "# =========================\n",
        "# 2) 유틸: norm/meta/weights 로드 + 예측\n",
        "# =========================\n",
        "def load_meta(meta_path: str):\n",
        "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        meta = json.load(f)\n",
        "    label2idx = meta[\"label2idx\"]\n",
        "    idx2label = {int(v): k for k, v in label2idx.items()}\n",
        "    return meta, idx2label\n",
        "\n",
        "def load_norm(norm_path: str):\n",
        "    pack = np.load(norm_path)\n",
        "    mean = pack[\"mean\"].astype(np.float32)\n",
        "    std  = pack[\"std\"].astype(np.float32)\n",
        "    return mean, std\n",
        "\n",
        "def load_weights(model: nn.Module, w_path: str):\n",
        "    state = torch.load(w_path, map_location=DEVICE)   # state_dict only\n",
        "    model.load_state_dict(state)\n",
        "    model.to(DEVICE).eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_demo(model_name: str, model: nn.Module, X_demo: np.ndarray, y_demo: np.ndarray, idx2label: dict, mean: np.ndarray, std: np.ndarray):\n",
        "    # normalize\n",
        "    Xn = (X_demo - mean[None, None, :]) / std[None, None, :]\n",
        "    Xn = np.nan_to_num(Xn, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
        "\n",
        "    x = torch.from_numpy(Xn).to(DEVICE)  # (B,T,D)\n",
        "    logits = model(x)\n",
        "    pred = torch.argmax(logits, dim=1).cpu().numpy().astype(np.int64)\n",
        "\n",
        "    correct = int((pred == y_demo).sum())\n",
        "    total = int(len(y_demo))\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"[MODEL: {model_name}]  correct={correct}/{total}  acc={acc:.3f}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for i in range(total):\n",
        "        gt_idx = int(y_demo[i])\n",
        "        pr_idx = int(pred[i])\n",
        "        gt = idx2label.get(gt_idx, str(gt_idx))\n",
        "        pr = idx2label.get(pr_idx, str(pr_idx))\n",
        "        mark = \"✅\" if gt_idx == pr_idx else \"❌\"\n",
        "        print(f\"{i:02d} {mark}  GT: {gt:<25} | PRED: {pr}\")\n",
        "\n",
        "# =========================\n",
        "# 3) demo 데이터 로드\n",
        "# =========================\n",
        "demo = np.load(DEMO_NPZ)\n",
        "X_demo = demo[\"X_demo\"].astype(np.float32)  # (20,16,D)\n",
        "y_demo = demo[\"y_demo\"].astype(np.int64)    # (20,)\n",
        "\n",
        "# =========================\n",
        "# 4) GRU 데모\n",
        "# =========================\n",
        "gru_meta, gru_idx2label = load_meta(GRU_META)\n",
        "gru_mean, gru_std = load_norm(GRU_NORM)\n",
        "\n",
        "gru_input_dim = int(gru_meta[\"input_dim\"])\n",
        "gru_num_classes = int(gru_meta[\"num_classes\"])\n",
        "gru_cfg = gru_meta.get(\"model_cfg\", {\"hidden_dim\":256, \"num_layers\":2, \"bidirectional\":True, \"dropout\":0.2})\n",
        "\n",
        "assert X_demo.shape[-1] == gru_input_dim, f\"❌ GRU input_dim mismatch: demo D={X_demo.shape[-1]} vs meta input_dim={gru_input_dim}\"\n",
        "\n",
        "gru_model = GRUClassifier(input_dim=gru_input_dim, num_classes=gru_num_classes, **gru_cfg)\n",
        "load_weights(gru_model, GRU_W)\n",
        "run_demo(\"GRU\", gru_model, X_demo, y_demo, gru_idx2label, gru_mean, gru_std)\n",
        "\n",
        "# =========================\n",
        "# 5) TCN 데모\n",
        "# =========================\n",
        "tcn_meta, tcn_idx2label = load_meta(TCN_META)\n",
        "tcn_mean, tcn_std = load_norm(TCN_NORM)\n",
        "\n",
        "tcn_input_dim = int(tcn_meta[\"input_dim\"])\n",
        "tcn_num_classes = int(tcn_meta[\"num_classes\"])\n",
        "tcn_cfg = tcn_meta.get(\"model_cfg\", {\"hidden_channels\":256})\n",
        "\n",
        "assert X_demo.shape[-1] == tcn_input_dim, f\"❌ TCN input_dim mismatch: demo D={X_demo.shape[-1]} vs meta input_dim={tcn_input_dim}\"\n",
        "\n",
        "tcn_model = TCNClassifier(input_dim=tcn_input_dim, num_classes=tcn_num_classes, **tcn_cfg)\n",
        "load_weights(tcn_model, TCN_W)\n",
        "run_demo(\"TCN\", tcn_model, X_demo, y_demo, tcn_idx2label, tcn_mean, tcn_std)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Brj1ZaOiBgXF",
        "outputId": "9826c17e-614e-449c-f846-c2263f409e1a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "======================================================================\n",
            "[MODEL: GRU]  correct=16/20  acc=0.800\n",
            "======================================================================\n",
            "00 ✅  GT: WORD0046_설사            | PRED: WORD0046_설사\n",
            "01 ❌  GT: WORD0039_변비            | PRED: WORD0041_보건소\n",
            "02 ✅  GT: WORD0046_설사            | PRED: WORD0046_설사\n",
            "03 ❌  GT: WORD0033_당뇨병         | PRED: WORD0042_불면증\n",
            "04 ✅  GT: WORD1115_건강           | PRED: WORD1115_건강\n",
            "05 ❌  GT: WORD0689_통증           | PRED: WORD0029_검사\n",
            "06 ✅  GT: WORD0885_치료제           | PRED: WORD0885_치료제\n",
            "07 ✅  GT: WORD0046_설사            | PRED: WORD0046_설사\n",
            "08 ✅  GT: WORD1129_검사            | PRED: WORD1129_검사\n",
            "09 ✅  GT: WORD0029_검사            | PRED: WORD0029_검사\n",
            "10 ✅  GT: WORD0065_치료법          | PRED: WORD0065_치료법\n",
            "11 ❌  GT: WORD0885_치료제           | PRED: WORD0064_치료\n",
            "12 ✅  GT: WORD0064_치료             | PRED: WORD0064_치료\n",
            "13 ✅  GT: WORD0040_병명           | PRED: WORD0040_병명\n",
            "14 ✅  GT: WORD1496_병원           | PRED: WORD1496_병원\n",
            "15 ✅  GT: WORD0037_감기            | PRED: WORD0037_감기\n",
            "16 ✅  GT: WORD0885_치료제           | PRED: WORD0885_치료제\n",
            "17 ✅  GT: WORD0065_치료법          | PRED: WORD0065_치료법\n",
            "18 ✅  GT: WORD0039_변비            | PRED: WORD0039_변비\n",
            "19 ✅  GT: WORD0885_치료제           | PRED: WORD0885_치료제\n",
            "\n",
            "======================================================================\n",
            "[MODEL: TCN]  correct=19/20  acc=0.950\n",
            "======================================================================\n",
            "00 ✅  GT: WORD0046_설사            | PRED: WORD0046_설사\n",
            "01 ✅  GT: WORD0039_변비            | PRED: WORD0039_변비\n",
            "02 ✅  GT: WORD0046_설사            | PRED: WORD0046_설사\n",
            "03 ✅  GT: WORD0033_당뇨병         | PRED: WORD0033_당뇨병\n",
            "04 ✅  GT: WORD1115_건강           | PRED: WORD1115_건강\n",
            "05 ✅  GT: WORD0689_통증           | PRED: WORD0689_통증\n",
            "06 ✅  GT: WORD0885_치료제           | PRED: WORD0885_치료제\n",
            "07 ✅  GT: WORD0046_설사            | PRED: WORD0046_설사\n",
            "08 ✅  GT: WORD1129_검사            | PRED: WORD1129_검사\n",
            "09 ❌  GT: WORD0029_검사            | PRED: WORD0037_감기\n",
            "10 ✅  GT: WORD0065_치료법          | PRED: WORD0065_치료법\n",
            "11 ✅  GT: WORD0885_치료제           | PRED: WORD0885_치료제\n",
            "12 ✅  GT: WORD0064_치료             | PRED: WORD0064_치료\n",
            "13 ✅  GT: WORD0040_병명           | PRED: WORD0040_병명\n",
            "14 ✅  GT: WORD1496_병원           | PRED: WORD1496_병원\n",
            "15 ✅  GT: WORD0037_감기            | PRED: WORD0037_감기\n",
            "16 ✅  GT: WORD0885_치료제           | PRED: WORD0885_치료제\n",
            "17 ✅  GT: WORD0065_치료법          | PRED: WORD0065_치료법\n",
            "18 ✅  GT: WORD0039_변비            | PRED: WORD0039_변비\n",
            "19 ✅  GT: WORD0885_치료제           | PRED: WORD0885_치료제\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2D Sequence"
      ],
      "metadata": {
        "id": "KKMJkCVN9Sg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "\n",
        "# =========================================================\n",
        "# 0) 경로 설정 (사용자가 준 경로 그대로)\n",
        "# =========================================================\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "GRU_W = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/model_weights/2D Sequence/GRU_best.pt\"\n",
        "TCN_W = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/model_weights/2D Sequence/TCN_best.pt\"\n",
        "\n",
        "GRU_DATA_DIR = \"/content/drive/MyDrive/cv-medislr/data/samples/2D Sequence/GRU\"\n",
        "TCN_DATA_DIR = \"/content/drive/MyDrive/cv-medislr/data/samples/2D Sequence/TCN\"\n",
        "\n",
        "assert os.path.exists(GRU_W), f\"❌ GRU weight not found: {GRU_W}\"\n",
        "assert os.path.exists(TCN_W), f\"❌ TCN weight not found: {TCN_W}\"\n",
        "assert os.path.isdir(GRU_DATA_DIR), f\"❌ GRU data dir not found: {GRU_DATA_DIR}\"\n",
        "assert os.path.isdir(TCN_DATA_DIR), f\"❌ TCN data dir not found: {TCN_DATA_DIR}\"\n",
        "\n",
        "# =========================================================\n",
        "# 1) 모델 정의 (학습 코드와 동일한 구조)\n",
        "#   - Input: (B, T, 3, H, W)\n",
        "#   - Frame encoder: MobileNetV3 Small -> 256D\n",
        "#   - GRU / TCN + Attention -> Head\n",
        "# =========================================================\n",
        "class FrameEncoderMobileNetV3(nn.Module):\n",
        "    def __init__(self, out_dim=256, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = models.MobileNet_V3_Small_Weights.DEFAULT if pretrained else None\n",
        "        backbone = models.mobilenet_v3_small(weights=weights)\n",
        "        self.features = backbone.features\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        in_feat = backbone.classifier[0].in_features\n",
        "        self.proj = nn.Linear(in_feat, out_dim)\n",
        "\n",
        "    def forward(self, x):  # x: (B,3,H,W)\n",
        "        f = self.features(x)\n",
        "        f = self.gap(f).flatten(1)\n",
        "        return self.proj(f)  # (B,out_dim)\n",
        "\n",
        "class SeqCNN_MobileNet_GRU_Attn(nn.Module):\n",
        "    def __init__(self, num_classes, frame_out_dim=256, hidden_dim=192, num_layers=2, bidirectional=True, dropout=0.2, pretrained_backbone=True):\n",
        "        super().__init__()\n",
        "        self.encoder = FrameEncoderMobileNetV3(out_dim=frame_out_dim, pretrained=pretrained_backbone)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=frame_out_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "        self.attn_fc = nn.Linear(out_dim, 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(out_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: (B,T,3,H,W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B*T, C, H, W)\n",
        "        feat = self.encoder(x).view(B, T, -1)  # (B,T,256)\n",
        "\n",
        "        out, _ = self.gru(feat)                # (B,T,H*)\n",
        "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        w = torch.softmax(self.attn_fc(out), dim=1)  # (B,T,1)\n",
        "        w = torch.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        feat_seq = (w * out).sum(dim=1)        # (B,H*)\n",
        "        return self.head(feat_seq)             # (B,num_classes)\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "        padding = (kernel_size - 1) * dilation // 2  # 길이 유지\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
        "        self.final_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):  # (B,C,T)\n",
        "        out = self.relu1(self.bn1(self.conv1(x)))\n",
        "        out = self.dropout(self.bn2(self.conv2(out)))\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.final_relu(out + res)\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels=256, num_layers=3, kernel_size=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_ch = input_channels\n",
        "        for i in range(num_layers):\n",
        "            layers.append(TemporalBlock(in_ch, hidden_channels, kernel_size=kernel_size, dilation=2**i, dropout=dropout))\n",
        "            in_ch = hidden_channels\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self.out_channels = hidden_channels\n",
        "\n",
        "    def forward(self, x):  # (B,C,T)\n",
        "        return self.network(x)\n",
        "\n",
        "class SeqCNN_MobileNet_TCN_Attn(nn.Module):\n",
        "    def __init__(self, num_classes, frame_out_dim=256, tcn_hidden=256, tcn_layers=3, dropout=0.2, pretrained_backbone=True):\n",
        "        super().__init__()\n",
        "        self.encoder = FrameEncoderMobileNetV3(out_dim=frame_out_dim, pretrained=pretrained_backbone)\n",
        "        self.tcn = TemporalConvNet(input_channels=frame_out_dim, hidden_channels=tcn_hidden, num_layers=tcn_layers, kernel_size=3, dropout=dropout)\n",
        "        out_dim = self.tcn.out_channels\n",
        "        self.attn_fc = nn.Linear(out_dim, 1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(out_dim),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(out_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: (B,T,3,H,W)\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B*T, C, H, W)\n",
        "        feat = self.encoder(x).view(B, T, -1)    # (B,T,256)\n",
        "\n",
        "        out = self.tcn(feat.permute(0,2,1))      # (B,H,T)\n",
        "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        out_seq = out.permute(0,2,1)             # (B,T,H)\n",
        "        w = torch.softmax(self.attn_fc(out_seq), dim=1)  # (B,T,1)\n",
        "        w = torch.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        feat_seq = (w * out_seq).sum(dim=1)      # (B,H)\n",
        "        return self.head(feat_seq)               # (B,num_classes)\n",
        "\n",
        "# =========================================================\n",
        "# 2) 유틸: state_dict에서 num_classes 자동 추론\n",
        "# =========================================================\n",
        "def infer_num_classes_from_state(state: dict) -> int:\n",
        "    # 보통 head의 마지막 Linear가 head.2.weight로 잡힘 (Sequential: LN, Dropout, Linear)\n",
        "    for k in [\"head.2.weight\", \"head.2.bias\"]:\n",
        "        if k in state:\n",
        "            return int(state[\"head.2.weight\"].shape[0])\n",
        "    # fallback: weight 중에서 shape[0]가 \"클래스 수\"일 가능성이 큰 항목 찾기\n",
        "    cand = []\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor) and v.ndim == 2 and v.shape[0] < 10000:\n",
        "            cand.append((k, int(v.shape[0]), int(v.shape[1])))\n",
        "    # 그중 가장 \"클래스처럼 보이는\" 작은 out_dim을 우선\n",
        "    cand.sort(key=lambda x: x[1])\n",
        "    if not cand:\n",
        "        raise ValueError(\"❌ Could not infer num_classes from state_dict.\")\n",
        "    return cand[0][1]\n",
        "\n",
        "def make_idx2label(num_classes: int):\n",
        "    # 라벨 이름 매핑 파일이 없을 때를 대비한 기본 매핑\n",
        "    return {i: f\"class_{i:04d}\" for i in range(num_classes)}\n",
        "\n",
        "# =========================================================\n",
        "# 3) Dataset: pt 파일(dict)에서 x,y 읽기\n",
        "#   - pt는 이미 Normalize까지 끝난 텐서라고 가정 (전처리 코드 기준)\n",
        "# =========================================================\n",
        "class PTSeqDataset(Dataset):\n",
        "    def __init__(self, pt_dir: str):\n",
        "        self.pt_paths = sorted(glob.glob(os.path.join(pt_dir, \"*.pt\")))\n",
        "        assert len(self.pt_paths) > 0, f\"❌ No pt files found in: {pt_dir}\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pt_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pack = torch.load(self.pt_paths[idx], map_location=\"cpu\")  # {\"x\":(T,3,H,W), \"y\":int, ...}\n",
        "        x = pack[\"x\"].float()      # (T,3,H,W)\n",
        "        y = int(pack[\"y\"])\n",
        "        base_id = pack.get(\"base_id\", os.path.basename(self.pt_paths[idx]))\n",
        "        return x, y, base_id\n",
        "\n",
        "def collate_fn(batch):\n",
        "    xs, ys, ids = zip(*batch)\n",
        "    x = torch.stack(xs, dim=0)            # (B,T,3,H,W)\n",
        "    y = torch.tensor(ys, dtype=torch.long)\n",
        "    return x, y, ids\n",
        "\n",
        "# =========================================================\n",
        "# 4) 데모 실행 (정확도 + 샘플별 출력)\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def run_demo(model_name: str, model: nn.Module, loader: DataLoader, idx2label: dict, max_print: int = 30):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "\n",
        "    all_rows = []\n",
        "    for xb, yb, ids in loader:\n",
        "        xb = xb.to(DEVICE, non_blocking=True)\n",
        "        yb = yb.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        logits = model(xb)\n",
        "        pred = logits.argmax(dim=1)\n",
        "\n",
        "        total += int(yb.numel())\n",
        "        correct += int((pred == yb).sum().item())\n",
        "\n",
        "        pred_cpu = pred.cpu().tolist()\n",
        "        y_cpu = yb.cpu().tolist()\n",
        "\n",
        "        for i in range(len(y_cpu)):\n",
        "            all_rows.append((ids[i], y_cpu[i], pred_cpu[i]))\n",
        "\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"[MODEL: {model_name}]  correct={correct}/{total}  acc={acc:.4f}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    n_show = min(len(all_rows), max_print)\n",
        "    for i in range(n_show):\n",
        "        base_id, gt_i, pr_i = all_rows[i]\n",
        "        gt = idx2label.get(int(gt_i), str(gt_i))\n",
        "        pr = idx2label.get(int(pr_i), str(pr_i))\n",
        "        mark = \"✅\" if int(gt_i) == int(pr_i) else \"❌\"\n",
        "        print(f\"{i:02d} {mark}  ID: {base_id} | GT: {gt:<12} | PRED: {pr}\")\n",
        "\n",
        "    if len(all_rows) > n_show:\n",
        "        print(f\"... (printed {n_show}/{len(all_rows)})\")\n",
        "\n",
        "# =========================================================\n",
        "# 5) GRU 데모\n",
        "# =========================================================\n",
        "gru_state = torch.load(GRU_W, map_location=\"cpu\")  # 학습 코드에서 state_dict만 저장했으므로 dict가 바로 state_dict\n",
        "gru_num_classes = infer_num_classes_from_state(gru_state)\n",
        "gru_idx2label = make_idx2label(gru_num_classes)\n",
        "\n",
        "gru_model = SeqCNN_MobileNet_GRU_Attn(\n",
        "    num_classes=gru_num_classes,\n",
        "    frame_out_dim=256,\n",
        "    hidden_dim=192,\n",
        "    num_layers=2,\n",
        "    bidirectional=True,\n",
        "    dropout=0.2,\n",
        "    pretrained_backbone=True,   # 데모에서는 backbone weight가 포함돼 있을 가능성이 높음 (state_dict로 덮임)\n",
        ").to(DEVICE)\n",
        "\n",
        "gru_model.load_state_dict(gru_state, strict=True)\n",
        "gru_model.eval()\n",
        "\n",
        "gru_ds = PTSeqDataset(GRU_DATA_DIR)\n",
        "gru_dl = DataLoader(gru_ds, batch_size=8, shuffle=False, num_workers=0, pin_memory=(DEVICE.type==\"cuda\"), collate_fn=collate_fn)\n",
        "run_demo(\"2D-Sequence GRU\", gru_model, gru_dl, gru_idx2label, max_print=30)\n",
        "\n",
        "# =========================================================\n",
        "# 6) TCN 데모\n",
        "# =========================================================\n",
        "tcn_state = torch.load(TCN_W, map_location=\"cpu\")\n",
        "tcn_num_classes = infer_num_classes_from_state(tcn_state)\n",
        "tcn_idx2label = make_idx2label(tcn_num_classes)\n",
        "\n",
        "tcn_model = SeqCNN_MobileNet_TCN_Attn(\n",
        "    num_classes=tcn_num_classes,\n",
        "    frame_out_dim=256,\n",
        "    tcn_hidden=256,\n",
        "    tcn_layers=3,\n",
        "    dropout=0.2,\n",
        "    pretrained_backbone=True,\n",
        ").to(DEVICE)\n",
        "\n",
        "tcn_model.load_state_dict(tcn_state, strict=True)\n",
        "tcn_model.eval()\n",
        "\n",
        "tcn_ds = PTSeqDataset(TCN_DATA_DIR)\n",
        "tcn_dl = DataLoader(tcn_ds, batch_size=8, shuffle=False, num_workers=0, pin_memory=(DEVICE.type==\"cuda\"), collate_fn=collate_fn)\n",
        "run_demo(\"2D-Sequence TCN\", tcn_model, tcn_dl, tcn_idx2label, max_print=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AwKDAGAtuZk",
        "outputId": "a66b991d-5e92-4264-96bb-9fbd4912cc83"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.83M/9.83M [00:00<00:00, 82.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "[MODEL: 2D-Sequence GRU]  correct=19/20  acc=0.9500\n",
            "================================================================================\n",
            "00 ✅  ID: WORD0042_불면증_NIA_SL_WORD0042_REAL04_R | GT: class_0007   | PRED: class_0007\n",
            "01 ✅  ID: WORD0689_통증_NIA_SL_WORD0689_REAL06_F | GT: class_0016   | PRED: class_0016\n",
            "02 ✅  ID: WORD1115_건강_NIA_SL_WORD1115_REAL15_F | GT: class_0018   | PRED: class_0018\n",
            "03 ✅  ID: WORD1496_병원_NIA_SL_WORD1496_REAL08_U | GT: class_0021   | PRED: class_0021\n",
            "04 ✅  ID: WORD0065_치료법_NIA_SL_WORD0065_REAL02_R | GT: class_0011   | PRED: class_0011\n",
            "05 ✅  ID: WORD0041_보건소_NIA_SL_WORD0041_REAL05_U | GT: class_0006   | PRED: class_0006\n",
            "06 ✅  ID: WORD0036_면역_NIA_SL_WORD0036_REAL01_D | GT: class_0002   | PRED: class_0002\n",
            "07 ✅  ID: WORD0042_불면증_NIA_SL_WORD0042_REAL02_L | GT: class_0007   | PRED: class_0007\n",
            "08 ❌  ID: WORD0033_당뇨병_NIA_SL_WORD0033_REAL04_L | GT: class_0001   | PRED: class_0000\n",
            "09 ✅  ID: WORD0400_정밀검사_NIA_SL_WORD0400_REAL07_L | GT: class_0014   | PRED: class_0014\n",
            "10 ✅  ID: WORD0046_설사_NIA_SL_WORD0046_REAL03_R | GT: class_0008   | PRED: class_0008\n",
            "11 ✅  ID: WORD0042_불면증_NIA_SL_WORD0042_REAL06_U | GT: class_0007   | PRED: class_0007\n",
            "12 ✅  ID: WORD1129_검사_NIA_SL_WORD1129_REAL15_F | GT: class_0019   | PRED: class_0019\n",
            "13 ✅  ID: WORD0029_검사_NIA_SL_WORD0029_REAL08_F | GT: class_0000   | PRED: class_0000\n",
            "14 ✅  ID: WORD0040_병명_NIA_SL_WORD0040_REAL07_F | GT: class_0005   | PRED: class_0005\n",
            "15 ✅  ID: WORD0029_검사_NIA_SL_WORD0029_REAL04_U | GT: class_0000   | PRED: class_0000\n",
            "16 ✅  ID: WORD0029_검사_NIA_SL_WORD0029_REAL05_L | GT: class_0000   | PRED: class_0000\n",
            "17 ✅  ID: WORD1496_병원_NIA_SL_WORD1496_REAL15_L | GT: class_0021   | PRED: class_0021\n",
            "18 ✅  ID: WORD0187_간호사_NIA_SL_WORD0187_REAL08_U | GT: class_0013   | PRED: class_0013\n",
            "19 ✅  ID: WORD0037_감기_NIA_SL_WORD0037_REAL08_R | GT: class_0003   | PRED: class_0003\n",
            "\n",
            "================================================================================\n",
            "[MODEL: 2D-Sequence TCN]  correct=8/20  acc=0.4000\n",
            "================================================================================\n",
            "00 ❌  ID: WORD0187_간호사_NIA_SL_WORD0187_REAL08_F | GT: class_0013   | PRED: class_0018\n",
            "01 ❌  ID: WORD0065_치료법_NIA_SL_WORD0065_REAL02_R | GT: class_0011   | PRED: class_0005\n",
            "02 ❌  ID: WORD0400_정밀검사_NIA_SL_WORD0400_REAL16_L | GT: class_0014   | PRED: class_0016\n",
            "03 ❌  ID: WORD0041_보건소_NIA_SL_WORD0041_REAL05_U | GT: class_0006   | PRED: class_0018\n",
            "04 ✅  ID: WORD1496_병원_NIA_SL_WORD1496_REAL16_F | GT: class_0021   | PRED: class_0021\n",
            "05 ✅  ID: WORD0400_정밀검사_NIA_SL_WORD0400_REAL07_F | GT: class_0014   | PRED: class_0014\n",
            "06 ❌  ID: WORD0400_정밀검사_NIA_SL_WORD0400_REAL16_R | GT: class_0014   | PRED: class_0016\n",
            "07 ❌  ID: WORD0033_당뇨병_NIA_SL_WORD0033_REAL04_L | GT: class_0001   | PRED: class_0005\n",
            "08 ✅  ID: WORD0065_치료법_NIA_SL_WORD0065_REAL15_L | GT: class_0011   | PRED: class_0011\n",
            "09 ❌  ID: WORD0042_불면증_NIA_SL_WORD0042_REAL05_R | GT: class_0007   | PRED: class_0016\n",
            "10 ✅  ID: WORD0689_통증_NIA_SL_WORD0689_REAL15_R | GT: class_0016   | PRED: class_0016\n",
            "11 ❌  ID: WORD0042_불면증_NIA_SL_WORD0042_REAL07_R | GT: class_0007   | PRED: class_0016\n",
            "12 ✅  ID: WORD0689_통증_NIA_SL_WORD0689_REAL04_L | GT: class_0016   | PRED: class_0016\n",
            "13 ❌  ID: WORD1158_피곤하다_NIA_SL_WORD1158_REAL05_R | GT: class_0020   | PRED: class_0004\n",
            "14 ✅  ID: WORD1115_건강_NIA_SL_WORD1115_REAL06_R | GT: class_0018   | PRED: class_0018\n",
            "15 ✅  ID: WORD0062_진단서_NIA_SL_WORD0062_REAL06_L | GT: class_0009   | PRED: class_0009\n",
            "16 ❌  ID: WORD0029_검사_NIA_SL_WORD0029_REAL08_F | GT: class_0000   | PRED: class_0005\n",
            "17 ✅  ID: WORD0163_의사_NIA_SL_WORD0163_REAL16_U | GT: class_0012   | PRED: class_0012\n",
            "18 ❌  ID: WORD0029_검사_NIA_SL_WORD0029_REAL04_U | GT: class_0000   | PRED: class_0005\n",
            "19 ❌  ID: WORD1158_피곤하다_NIA_SL_WORD1158_REAL15_R | GT: class_0020   | PRED: class_0021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2D Only"
      ],
      "metadata": {
        "id": "d9pskmps9Y2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
        "\n",
        "# =========================\n",
        "# 0) 경로 설정\n",
        "# =========================\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/cv-medislr/data/samples/2d_only/skeleton_tsn\"\n",
        "CKPT_PT  = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/model_weights/2d_only/mobilenet_tsn_hands_best.pt\"\n",
        "\n",
        "SAMPLE_META = os.path.join(DATA_DIR, \"tsn_sample_meta.csv\")  # 있으면 이걸 우선 사용\n",
        "\n",
        "assert os.path.isdir(DATA_DIR), f\"❌ DATA_DIR not found: {DATA_DIR}\"\n",
        "assert os.path.exists(CKPT_PT),  f\"❌ CKPT not found: {CKPT_PT}\"\n",
        "\n",
        "# =========================\n",
        "# 1) 모델 정의 (학습 코드와 동일)\n",
        "# =========================\n",
        "class MobileNetTSN(nn.Module):\n",
        "    def __init__(self, num_classes=22, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        backbone = mobilenet_v2(weights=weights)\n",
        "\n",
        "        self.features = backbone.features\n",
        "        self.last_channel = backbone.last_channel  # 1280\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(self.last_channel, num_classes)\n",
        "\n",
        "        # ImageNet 정규화용 mean/std (0~1 입력 기준)\n",
        "        self.register_buffer(\n",
        "            \"img_mean\",\n",
        "            torch.tensor([0.485, 0.456, 0.406]).view(1, 1, 3, 1, 1)\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"img_std\",\n",
        "            torch.tensor([0.229, 0.224, 0.225]).view(1, 1, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, C, H, W)  여기서 보통 C=1 (grayscale)\n",
        "        \"\"\"\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # 1채널 → 3채널 replicate\n",
        "        if C == 1:\n",
        "            x = x.repeat(1, 1, 3, 1, 1)   # (B,T,3,H,W)\n",
        "\n",
        "        # ImageNet 정규화\n",
        "        x = (x - self.img_mean) / self.img_std\n",
        "\n",
        "        # (B*T, 3, H, W)\n",
        "        x = x.view(B * T, 3, H, W)\n",
        "\n",
        "        feat = self.features(x)          # (B*T, 1280, h, w)\n",
        "        feat = self.pool(feat)           # (B*T, 1280, 1, 1)\n",
        "        feat = feat.view(B, T, self.last_channel)  # (B, T, 1280)\n",
        "\n",
        "        # TSN: 시간 평균\n",
        "        feat = feat.mean(dim=1)          # (B, 1280)\n",
        "\n",
        "        feat = self.dropout(feat)\n",
        "        logits = self.fc(feat)           # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "# =========================\n",
        "# 2) Dataset: pt 텐서 로드\n",
        "#    - sample_meta.csv 있으면 그걸 쓰고,\n",
        "#    - 없으면 폴더의 pt 파일들을 직접 읽어서 y는 -1 처리\n",
        "# =========================\n",
        "class SkeletonTSNDemoDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, meta_csv: str | None = None):\n",
        "        self.items = []\n",
        "\n",
        "        if meta_csv is not None and os.path.exists(meta_csv):\n",
        "            df = pd.read_csv(meta_csv)\n",
        "            # label_idx 컬럼이 있을 수도 있고, 없을 수도 있어서 안전 처리\n",
        "            label_col = \"label_idx\" if \"label_idx\" in df.columns else (\"label\" if \"label\" in df.columns else None)\n",
        "\n",
        "            for _, row in df.iterrows():\n",
        "                p = row[\"tensor_path\"] if \"tensor_path\" in df.columns else row[\"tensor_path\".strip()]\n",
        "                y = int(row[label_col]) if label_col is not None else -1\n",
        "                base_id = row[\"base_id\"] if \"base_id\" in df.columns else os.path.basename(p)\n",
        "                self.items.append((p, y, base_id))\n",
        "        else:\n",
        "            pts = sorted(glob.glob(os.path.join(data_dir, \"*.pt\")))\n",
        "            assert len(pts) > 0, f\"❌ no .pt files in {data_dir}\"\n",
        "            for p in pts:\n",
        "                self.items.append((p, -1, os.path.basename(p)))\n",
        "\n",
        "        assert len(self.items) > 0, \"❌ empty dataset\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, y, base_id = self.items[idx]\n",
        "        x = torch.load(path, map_location=\"cpu\")   # 여기서는 seq_tensor만 저장했었음: (T,C,H,W)\n",
        "        if isinstance(x, dict) and \"x\" in x:\n",
        "            # 혹시 dict로 저장된 경우까지 대비\n",
        "            x = x[\"x\"]\n",
        "            if y == -1 and \"y\" in x:\n",
        "                y = int(x[\"y\"])\n",
        "        x = x.float()  # (T,C,H,W)\n",
        "        return x, int(y), base_id\n",
        "\n",
        "def collate_fn(batch):\n",
        "    xs, ys, ids = zip(*batch)\n",
        "    xb = torch.stack(xs, dim=0)  # (B,T,C,H,W)\n",
        "    yb = torch.tensor(ys, dtype=torch.long)\n",
        "    return xb, yb, ids\n",
        "\n",
        "# =========================\n",
        "# 3) 체크포인트 로드\n",
        "# =========================\n",
        "ckpt = torch.load(CKPT_PT, map_location=\"cpu\")\n",
        "state_dict = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
        "num_classes = int(ckpt.get(\"num_classes\", 22)) if isinstance(ckpt, dict) else 22\n",
        "\n",
        "model = MobileNetTSN(num_classes=num_classes, pretrained=False).to(DEVICE)\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "model.eval()\n",
        "\n",
        "idx2label = {i: f\"class_{i:04d}\" for i in range(num_classes)}  # 라벨명 파일 없으니 기본\n",
        "\n",
        "# =========================\n",
        "# 4) 데모 실행\n",
        "# =========================\n",
        "ds = SkeletonTSNDemoDataset(DATA_DIR, SAMPLE_META if os.path.exists(SAMPLE_META) else None)\n",
        "dl = DataLoader(ds, batch_size=4, shuffle=False, num_workers=0,\n",
        "                pin_memory=(DEVICE.type==\"cuda\"), collate_fn=collate_fn)\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_demo():\n",
        "    total, correct = 0, 0\n",
        "    rows = []\n",
        "\n",
        "    for xb, yb, ids in dl:\n",
        "        xb = xb.to(DEVICE, non_blocking=True)   # (B,T,C,H,W)\n",
        "        yb = yb.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        logits = model(xb)\n",
        "        pred = logits.argmax(dim=1)\n",
        "\n",
        "        # y가 -1이면(정답 없음) acc는 계산 불가 → 출력만\n",
        "        if (yb >= 0).all():\n",
        "            total += int(yb.numel())\n",
        "            correct += int((pred == yb).sum().item())\n",
        "\n",
        "        pred_cpu = pred.cpu().tolist()\n",
        "        y_cpu = yb.cpu().tolist()\n",
        "\n",
        "        for i in range(len(ids)):\n",
        "            rows.append((ids[i], y_cpu[i], pred_cpu[i]))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    if total > 0:\n",
        "        print(f\"[MODEL: MobileNetTSN] correct={correct}/{total}  acc={correct/total:.4f}\")\n",
        "    else:\n",
        "        print(\"[MODEL: MobileNetTSN] (no GT labels found) showing predictions only\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for i, (base_id, gt, pr) in enumerate(rows):\n",
        "        gt_s = idx2label.get(gt, str(gt)) if gt >= 0 else \"N/A\"\n",
        "        pr_s = idx2label.get(pr, str(pr))\n",
        "        mark = \"✅\" if (gt >= 0 and gt == pr) else (\"❌\" if gt >= 0 else \"•\")\n",
        "        print(f\"{i:02d} {mark}  ID: {base_id} | GT: {gt_s:<12} | PRED: {pr_s}\")\n",
        "\n",
        "run_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL-rzxWGuG10",
        "outputId": "fe6ec0a9-e989-4156-fe89-3d42eba35abf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "================================================================================\n",
            "[MODEL: MobileNetTSN] correct=15/20  acc=0.7500\n",
            "================================================================================\n",
            "00 ✅  ID: seq_00044.pt | GT: class_0008   | PRED: class_0008\n",
            "01 ✅  ID: seq_00568.pt | GT: class_0004   | PRED: class_0004\n",
            "02 ✅  ID: seq_00056.pt | GT: class_0011   | PRED: class_0011\n",
            "03 ❌  ID: seq_00636.pt | GT: class_0017   | PRED: class_0010\n",
            "04 ❌  ID: seq_00486.pt | GT: class_0009   | PRED: class_0001\n",
            "05 ✅  ID: seq_00096.pt | GT: class_0019   | PRED: class_0019\n",
            "06 ✅  ID: seq_00761.pt | GT: class_0020   | PRED: class_0020\n",
            "07 ✅  ID: seq_00051.pt | GT: class_0010   | PRED: class_0010\n",
            "08 ✅  ID: seq_00107.pt | GT: class_0000   | PRED: class_0000\n",
            "09 ❌  ID: seq_00666.pt | GT: class_0001   | PRED: class_0004\n",
            "10 ✅  ID: seq_00631.pt | GT: class_0016   | PRED: class_0016\n",
            "11 ❌  ID: seq_00270.pt | GT: class_0010   | PRED: class_0001\n",
            "12 ✅  ID: seq_00545.pt | GT: class_0021   | PRED: class_0021\n",
            "13 ✅  ID: seq_00849.pt | GT: class_0016   | PRED: class_0016\n",
            "14 ✅  ID: seq_01014.pt | GT: class_0005   | PRED: class_0005\n",
            "15 ✅  ID: seq_00873.pt | GT: class_0021   | PRED: class_0021\n",
            "16 ❌  ID: seq_00566.pt | GT: class_0003   | PRED: class_0000\n",
            "17 ✅  ID: seq_00516.pt | GT: class_0015   | PRED: class_0015\n",
            "18 ✅  ID: seq_00449.pt | GT: class_0002   | PRED: class_0002\n",
            "19 ✅  ID: seq_00717.pt | GT: class_0012   | PRED: class_0012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "# =========================\n",
        "# 0) 경로\n",
        "# =========================\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "DEMO_DIR = \"/content/drive/MyDrive/cv-medislr/data/samples/2d_only/skeleton_tiling/demo_test20_TILING_seed42\"\n",
        "MODEL_PT = \"/content/drive/MyDrive/cv-medislr/data/preprocessed/model_weights/2d_only/2d_tiling_resnet18_best.pt\"\n",
        "\n",
        "NUM_CLASSES = 22   # 🔥 학습 당시 클래스 수 (절대 demo에서 추정 X)\n",
        "\n",
        "# =========================\n",
        "# 1) demo pt 목록\n",
        "# =========================\n",
        "pt_files = sorted(glob.glob(os.path.join(DEMO_DIR, \"*.pt\")))\n",
        "assert len(pt_files) > 0, f\"❌ demo pt not found in: {DEMO_DIR}\"\n",
        "\n",
        "print(\"sample file:\", pt_files[0])\n",
        "\n",
        "# =========================\n",
        "# 2) 모델 정의 (학습과 동일)\n",
        "# =========================\n",
        "class TilingResNet18(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        base = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        base.fc = nn.Linear(base.fc.in_features, num_classes)\n",
        "        self.backbone = base\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "model = TilingResNet18(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# =========================\n",
        "# 3) checkpoint 로드\n",
        "# =========================\n",
        "state_dict = torch.load(MODEL_PT, map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# =========================\n",
        "# 4) DEMO 실행\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\"*78)\n",
        "print(\"[DEMO] TilingResNet18 on skeleton_tiling demo pts\")\n",
        "print(\"=\"*78)\n",
        "\n",
        "correct = 0\n",
        "with torch.inference_mode():\n",
        "    for i, p in enumerate(pt_files):\n",
        "        d = torch.load(p, map_location=\"cpu\")\n",
        "\n",
        "        x = d[\"x\"]                 # (3,224,224)\n",
        "        y = int(d[\"y\"])\n",
        "        meta = d.get(\"meta\", {})\n",
        "\n",
        "        x = x.unsqueeze(0).float().to(DEVICE)   # (1,3,224,224)\n",
        "\n",
        "        logits = model(x)\n",
        "        pred = int(torch.argmax(logits, dim=1))\n",
        "\n",
        "        ok = (pred == y)\n",
        "        correct += int(ok)\n",
        "\n",
        "        sid = meta.get(\"seq_id\", os.path.basename(p))\n",
        "        print(f\"{i:02d} {'✅' if ok else '❌'}  GT={y:02d} | PRED={pred:02d} | {sid}\")\n",
        "\n",
        "acc = correct / len(pt_files)\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(f\"RESULT: correct={correct}/{len(pt_files)}  acc={acc:.4f}\")\n",
        "print(\"-\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmnMro92xt2F",
        "outputId": "6e0cf282-3454-4346-e862-ba97bace264b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "sample file: /content/drive/MyDrive/cv-medislr/data/samples/2d_only/skeleton_tiling/demo_test20_TILING_seed42/sample00.pt\n",
            "\n",
            "==============================================================================\n",
            "[DEMO] TilingResNet18 on skeleton_tiling demo pts\n",
            "==============================================================================\n",
            "00 ❌  GT=00 | PRED=12 | 7/WORD0029_검사/F\n",
            "01 ❌  GT=00 | PRED=12 | 7/WORD0029_검사/L\n",
            "02 ✅  GT=19 | PRED=19 | 10/WORD1129_검사/R\n",
            "03 ✅  GT=05 | PRED=05 | 9/WORD0040_병명/U\n",
            "04 ✅  GT=04 | PRED=04 | 8/WORD0039_변비/D\n",
            "05 ✅  GT=05 | PRED=05 | 7/WORD0040_병명/L\n",
            "06 ✅  GT=05 | PRED=05 | 10/WORD0040_병명/L\n",
            "07 ✅  GT=17 | PRED=17 | 1/WORD0885_치료제/R\n",
            "08 ✅  GT=05 | PRED=05 | 6/WORD0040_병명/F\n",
            "09 ✅  GT=05 | PRED=05 | 5/WORD0040_병명/F\n",
            "10 ✅  GT=01 | PRED=01 | 9/WORD0033_당뇨병/R\n",
            "11 ✅  GT=03 | PRED=03 | 9/WORD0037_감기/F\n",
            "12 ✅  GT=20 | PRED=20 | 4/WORD1158_피곤하다/R\n",
            "13 ✅  GT=17 | PRED=17 | 7/WORD0885_치료제/U\n",
            "14 ✅  GT=18 | PRED=18 | 1/WORD1115_건강/D\n",
            "15 ✅  GT=01 | PRED=01 | 7/WORD0033_당뇨병/D\n",
            "16 ✅  GT=14 | PRED=14 | 7/WORD0400_정밀검사/F\n",
            "17 ✅  GT=09 | PRED=09 | 4/WORD0062_진단서/L\n",
            "18 ✅  GT=15 | PRED=15 | 6/WORD0572_환자실/F\n",
            "19 ✅  GT=03 | PRED=03 | 5/WORD0037_감기/R\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "RESULT: correct=18/20  acc=0.9000\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}